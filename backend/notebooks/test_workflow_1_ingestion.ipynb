{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow 1: ArXiv Ingestion Pipeline - Local Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment configured for host machine:\n",
      "   - Database: postgresql://researchmind:password@localhost:5433/researchmind\n",
      "   - Redis: redis://localhost:6379/0\n",
      "   - Qdrant: localhost:6333\n",
      "   - Neo4j: bolt://localhost:7687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/arxiv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root.parent))\n",
    "\n",
    "from notebook_setup import *\n",
    "\n",
    "\n",
    "from src.services.arxiv.client import ArxivClient\n",
    "from src.services.arxiv.metadata_extractor import MetadataExtractor\n",
    "from src.services.pdf_parser.factory import make_pdf_parser_service\n",
    "from src.services.pdf_parser.docling_utils import (\n",
    "    serialize_docling_document, deserialize_docling_document,\n",
    "    extract_full_text, get_document_metadata\n",
    ")\n",
    "from src.database import get_sync_session\n",
    "from src.models.paper import Paper\n",
    "from src.config import get_settings\n",
    "\n",
    "\n",
    "settings = get_settings()\n",
    "workflow_data = {}\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fetch ArXiv Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-11-02 23:40:26,535 ] [researchmind] | Module: client |Function: _make_request | Line: 50 - INFO - Fetching arXiv data: http://export.arxiv.org/api/query?search_query=cat%3Acs.AI&start=3100&max_results=3&sortBy=submittedDate&sortOrder=ascending\n",
      "[ 2025-11-02 23:40:26,896 ] [researchmind] | Module: client |Function: search_papers | Line: 178 - INFO - Found 3 papers for query: cat:cs.AI\n",
      "✓ Fetched 3 papers from cs.AI\n",
      "\n",
      "Total: 3 papers\n",
      "Sample: Fuzzy Inference Systems Optimization...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'arxiv_id': '1110.3385v1',\n",
       "  'title': 'Fuzzy Inference Systems Optimization',\n",
       "  'abstract': 'This paper compares various optimization methods for fuzzy inference system optimization. The optimization methods compared are genetic algorithm, particle swarm optimization and simulated annealing. When these techniques were implemented it was observed that the performance of each technique within the fuzzy inference system classification was context dependent.',\n",
       "  'authors': ['Pretesh Patel', 'Tshilidzi Marwala'],\n",
       "  'categories': ['cs.AI'],\n",
       "  'primary_category': 'cs.AI',\n",
       "  'published': '2011-10-15T05:39:34Z',\n",
       "  'updated': '2011-10-15T05:39:34Z',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/1110.3385v1',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1110.3385v1',\n",
       "  'doi': '',\n",
       "  'journal_ref': ''},\n",
       " {'arxiv_id': '1110.3672v1',\n",
       "  'title': 'Reasoning about Actions with Temporal Answer Sets',\n",
       "  'abstract': 'In this paper we combine Answer Set Programming (ASP) with Dynamic Linear Time Temporal Logic (DLTL) to define a temporal logic programming language for reasoning about complex actions and infinite computations. DLTL extends propositional temporal logic of linear time with regular programs of propositional dynamic logic, which are used for indexing temporal modalities. The action language allows general DLTL formulas to be included in domain descriptions to constrain the space of possible extensions. We introduce a notion of Temporal Answer Set for domain descriptions, based on the usual notion of Answer Set. Also, we provide a translation of domain descriptions into standard ASP and we use Bounded Model Checking techniques for the verification of DLTL constraints.',\n",
       "  'authors': ['Laura Giordano', 'Alberto Martelli', 'Daniele Theseider Dupré'],\n",
       "  'categories': ['cs.AI', 'cs.LO', 'F.4.1; I.2.3; I.2.4'],\n",
       "  'primary_category': 'cs.AI',\n",
       "  'published': '2011-10-17T14:12:07Z',\n",
       "  'updated': '2011-10-17T14:12:07Z',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/1110.3672v1',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1110.3672v1',\n",
       "  'doi': '',\n",
       "  'journal_ref': ''},\n",
       " {'arxiv_id': '1110.3888v2',\n",
       "  'title': 'Handling controversial arguments by matrix',\n",
       "  'abstract': 'We introduce matrix and its block to the Dung\\'s theory of argumentation framework. It is showed that each argumentation framework has a matrix representation, and the indirect attack relation and indirect defence relation can be characterized by computing the matrix. This provide a powerful mathematics way to determine the \"controversial arguments\" in an argumentation framework. Also, we introduce several kinds of blocks based on the matrix, and various prudent semantics of argumentation frameworks can all be determined by computing and comparing the matrices and their blocks which we have defined. In contrast with traditional method of directed graph, the matrix method has an excellent advantage: computability(even can be realized on computer easily). So, there is an intensive perspective to import the theory of matrix to the research of argumentation frameworks and its related areas.',\n",
       "  'authors': ['Xu Yuming'],\n",
       "  'categories': ['cs.AI'],\n",
       "  'primary_category': 'cs.AI',\n",
       "  'published': '2011-10-18T07:01:28Z',\n",
       "  'updated': '2011-10-20T05:58:23Z',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/1110.3888v2',\n",
       "  'arxiv_url': 'http://arxiv.org/abs/1110.3888v2',\n",
       "  'doi': '',\n",
       "  'journal_ref': ''}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATEGORIES = settings.arxiv_categories[:2]\n",
    "MAX_RESULTS = 3\n",
    "\n",
    "client = ArxivClient()\n",
    "papers = []\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    try:\n",
    "        category_papers = await client.search_papers(\n",
    "            query=f\"cat:{category}\",\n",
    "            max_results=MAX_RESULTS,\n",
    "            start=3100,\n",
    "            sort_by=\"submittedDate\",\n",
    "            sort_order=\"ascending\"\n",
    "\n",
    "        )\n",
    "        papers.extend(category_papers)\n",
    "        print(f\"✓ Fetched {len(category_papers)} papers from {category}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error fetching {category}: {e}\")\n",
    "\n",
    "workflow_data['fetch_papers'] = papers\n",
    "print(f\"\\nTotal: {len(papers)} papers\")\n",
    "if papers:\n",
    "    print(f\"Sample: {papers[0].get('title', 'N/A')[:80]}...\")\n",
    "\n",
    "papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Parse PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] 1110.3385v1... [ 2025-11-02 23:40:33,219 ] [researchmind] | Module: client |Function: download_pdf | Line: 272 - INFO - Downloading PDF from: http://arxiv.org/pdf/1110.3385v1\n",
      "[ 2025-11-02 23:40:34,016 ] [researchmind] | Module: client |Function: download_pdf | Line: 296 - INFO - Downloaded PDF (0.3MB) to: data/papers/1110.3385v1.pdf\n",
      "[ 2025-11-02 23:40:34,022 ] [docling.datamodel.document] | Module: document |Function: _guess_format | Line: 328 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "[ 2025-11-02 23:40:34,065 ] [docling.document_converter] | Module: document_converter |Function: _convert | Line: 318 - INFO - Going to convert document batch...\n",
      "[ 2025-11-02 23:40:34,066 ] [docling.document_converter] | Module: document_converter |Function: _get_pipeline | Line: 363 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 4b58af54f11ef1b41b300238c4ddafc5\n",
      "[ 2025-11-02 23:40:34,186 ] [docling.models.factories.base_factory] | Module: base_factory |Function: load_from_plugins | Line: 104 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "[ 2025-11-02 23:40:34,186 ] [docling.models.factories.base_factory] | Module: base_factory |Function: load_from_plugins | Line: 112 - INFO - Loading plugin 'docling_defaults'\n",
      "[ 2025-11-02 23:40:34,188 ] [docling.models.factories] | Module: __init__ |Function: get_picture_description_factory | Line: 26 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "[ 2025-11-02 23:40:34,193 ] [docling.models.factories.base_factory] | Module: base_factory |Function: load_from_plugins | Line: 104 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "[ 2025-11-02 23:40:34,194 ] [docling.models.factories.base_factory] | Module: base_factory |Function: load_from_plugins | Line: 112 - INFO - Loading plugin 'docling_defaults'\n",
      "[ 2025-11-02 23:40:34,196 ] [docling.models.factories] | Module: __init__ |Function: get_ocr_factory | Line: 16 - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "[ 2025-11-02 23:40:34,203 ] [docling.utils.accelerator_utils] | Module: accelerator_utils |Function: decide_device | Line: 82 - INFO - Accelerator device: 'mps'\n",
      "[ 2025-11-02 23:40:36,421 ] [docling.utils.accelerator_utils] | Module: accelerator_utils |Function: decide_device | Line: 82 - INFO - Accelerator device: 'mps'\n",
      "[ 2025-11-02 23:40:36,939 ] [docling.pipeline.base_pipeline] | Module: base_pipeline |Function: execute | Line: 65 - INFO - Processing document 1110.3385v1.pdf\n",
      "[ 2025-11-02 23:40:47,609 ] [docling.document_converter] | Module: document_converter |Function: _convert | Line: 342 - INFO - Finished converting document 1110.3385v1.pdf in 13.59 sec.\n",
      "[ 2025-11-02 23:40:47,610 ] [src.services.pdf_parser.docling] | Module: docling |Function: parse_pdf | Line: 116 - INFO - Parsed 1110.3385v1.pdf\n",
      "[ 2025-11-02 23:40:47,611 ] [src.services.pdf_parser.parser] | Module: parser |Function: parse_pdf | Line: 33 - INFO - Parsed 1110.3385v1.pdf\n",
      "✓ (166 elements)\n",
      "[2/3] 1110.3672v1... [ 2025-11-02 23:40:47,616 ] [researchmind] | Module: client |Function: download_pdf | Line: 272 - INFO - Downloading PDF from: http://arxiv.org/pdf/1110.3672v1\n",
      "[ 2025-11-02 23:40:47,884 ] [researchmind] | Module: client |Function: download_pdf | Line: 296 - INFO - Downloaded PDF (0.4MB) to: data/papers/1110.3672v1.pdf\n",
      "[ 2025-11-02 23:40:47,886 ] [docling.datamodel.document] | Module: document |Function: _guess_format | Line: 328 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "[ 2025-11-02 23:40:47,888 ] [docling.document_converter] | Module: document_converter |Function: _convert | Line: 318 - INFO - Going to convert document batch...\n",
      "[ 2025-11-02 23:40:47,888 ] [docling.pipeline.base_pipeline] | Module: base_pipeline |Function: execute | Line: 65 - INFO - Processing document 1110.3672v1.pdf\n",
      "[ 2025-11-02 23:40:53,349 ] [docling.document_converter] | Module: document_converter |Function: _convert | Line: 342 - INFO - Finished converting document 1110.3672v1.pdf in 5.46 sec.\n",
      "[ 2025-11-02 23:40:53,350 ] [src.services.pdf_parser.docling] | Module: docling |Function: parse_pdf | Line: 116 - INFO - Parsed 1110.3672v1.pdf\n",
      "[ 2025-11-02 23:40:53,355 ] [src.services.pdf_parser.parser] | Module: parser |Function: parse_pdf | Line: 33 - INFO - Parsed 1110.3672v1.pdf\n",
      "✓ (617 elements)\n",
      "[3/3] 1110.3888v2... [ 2025-11-02 23:40:53,359 ] [researchmind] | Module: client |Function: download_pdf | Line: 272 - INFO - Downloading PDF from: http://arxiv.org/pdf/1110.3888v2\n",
      "[ 2025-11-02 23:40:53,599 ] [researchmind] | Module: client |Function: download_pdf | Line: 296 - INFO - Downloaded PDF (0.2MB) to: data/papers/1110.3888v2.pdf\n",
      "[ 2025-11-02 23:40:53,601 ] [docling.datamodel.document] | Module: document |Function: _guess_format | Line: 328 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "[ 2025-11-02 23:40:53,602 ] [docling.document_converter] | Module: document_converter |Function: _convert | Line: 318 - INFO - Going to convert document batch...\n",
      "[ 2025-11-02 23:40:53,603 ] [docling.pipeline.base_pipeline] | Module: base_pipeline |Function: execute | Line: 65 - INFO - Processing document 1110.3888v2.pdf\n",
      "[ 2025-11-02 23:40:56,367 ] [docling.document_converter] | Module: document_converter |Function: _convert | Line: 342 - INFO - Finished converting document 1110.3888v2.pdf in 2.77 sec.\n",
      "[ 2025-11-02 23:40:56,368 ] [src.services.pdf_parser.docling] | Module: docling |Function: parse_pdf | Line: 116 - INFO - Parsed 1110.3888v2.pdf\n",
      "[ 2025-11-02 23:40:56,375 ] [src.services.pdf_parser.parser] | Module: parser |Function: parse_pdf | Line: 33 - INFO - Parsed 1110.3888v2.pdf\n",
      "✓ (446 elements)\n",
      "\n",
      "✓ Parsed 3/3 papers\n"
     ]
    }
   ],
   "source": [
    "papers = workflow_data.get('fetch_papers', [])\n",
    "download_dir = Path(settings.papers_storage_path)\n",
    "download_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "arxiv_client = ArxivClient()\n",
    "pdf_parser = make_pdf_parser_service()\n",
    "parsed_papers = []\n",
    "\n",
    "for i, paper in enumerate(papers, 1):\n",
    "    try:\n",
    "        arxiv_id = paper.get('arxiv_id')\n",
    "        pdf_url = paper.get('pdf_url')\n",
    "        \n",
    "        if not arxiv_id or not pdf_url:\n",
    "            parsed_papers.append(paper)\n",
    "            continue\n",
    "        \n",
    "        print(f\"[{i}/{len(papers)}] {arxiv_id}... \", end=\"\")\n",
    "        \n",
    "        pdf_path = await arxiv_client.download_pdf(\n",
    "            pdf_url=pdf_url,\n",
    "            download_path=download_dir / f\"{arxiv_id.replace('/', '_')}.pdf\",\n",
    "            max_file_size_mb=settings.pdf_parser_max_file_size_mb\n",
    "        )\n",
    "        \n",
    "        if not pdf_path or not pdf_path.exists():\n",
    "            print(\"download failed\")\n",
    "            parsed_papers.append(paper)\n",
    "            continue\n",
    "        \n",
    "        docling_doc = await pdf_parser.parse_pdf(pdf_path)\n",
    "        \n",
    "        if docling_doc:\n",
    "            paper_with_content = {**paper}\n",
    "            paper_with_content['docling_document'] = serialize_docling_document(docling_doc)\n",
    "            paper_with_content['docling_document_raw'] = docling_doc\n",
    "            paper_with_content['_temp_full_text'] = extract_full_text(docling_doc)\n",
    "            paper_with_content['is_processed'] = True\n",
    "            \n",
    "            doc_meta = get_document_metadata(docling_doc)\n",
    "            print(f\"✓ ({doc_meta.get('text_count', 0)} elements)\")\n",
    "            parsed_papers.append(paper_with_content)\n",
    "        else:\n",
    "            print(\"parse failed\")\n",
    "            parsed_papers.append(paper)\n",
    "        \n",
    "        pdf_path.unlink(missing_ok=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ {e}\")\n",
    "        parsed_papers.append(paper)\n",
    "\n",
    "workflow_data['parse_pdfs'] = parsed_papers\n",
    "print(f\"\\n✓ Parsed {sum(1 for p in parsed_papers if p.get('docling_document'))}/{len(parsed_papers)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] 1110.3385v1... [ 2025-11-02 23:40:56,390 ] [researchmind] | Module: metadata_extractor |Function: extract_metadata | Line: 236 - INFO - Extracting metadata for paper: 1110.3385v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-11-02 23:41:01,992 ] [httpx] | Module: _client |Function: _send_single_request | Line: 1025 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[ 2025-11-02 23:41:01,998 ] [researchmind] | Module: metadata_extractor |Function: extract_metadata | Line: 263 - INFO - Extracted metadata: 0 metrics, 2 authors\n",
      "✓ (words: 8475)\n",
      "[2/3] 1110.3672v1... [ 2025-11-02 23:41:02,015 ] [researchmind] | Module: metadata_extractor |Function: extract_metadata | Line: 236 - INFO - Extracting metadata for paper: 1110.3672v1\n",
      "[ 2025-11-02 23:41:14,634 ] [httpx] | Module: _client |Function: _send_single_request | Line: 1025 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[ 2025-11-02 23:41:14,637 ] [researchmind] | Module: metadata_extractor |Function: extract_metadata | Line: 263 - INFO - Extracted metadata: 0 metrics, 3 authors\n",
      "✓ (words: 20386)\n",
      "[3/3] 1110.3888v2... [ 2025-11-02 23:41:14,648 ] [researchmind] | Module: metadata_extractor |Function: extract_metadata | Line: 236 - INFO - Extracting metadata for paper: 1110.3888v2\n",
      "[ 2025-11-02 23:41:22,878 ] [httpx] | Module: _client |Function: _send_single_request | Line: 1025 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[ 2025-11-02 23:41:22,880 ] [researchmind] | Module: metadata_extractor |Function: extract_metadata | Line: 263 - INFO - Extracted metadata: 0 metrics, 1 authors\n",
      "✓ (words: 10525)\n",
      "\n",
      "✓ Metadata extraction complete\n"
     ]
    }
   ],
   "source": [
    "papers = workflow_data.get('parse_pdfs', [])\n",
    "extractor = MetadataExtractor()\n",
    "\n",
    "for i, paper in enumerate(papers, 1):\n",
    "    try:\n",
    "        arxiv_id = paper.get('arxiv_id', 'unknown')\n",
    "        print(f\"[{i}/{len(papers)}] {arxiv_id}... \", end=\"\")\n",
    "        \n",
    "        if paper.get('docling_document'):\n",
    "            doc = deserialize_docling_document(paper['docling_document'])\n",
    "            \n",
    "            if not paper.get('_temp_full_text'):\n",
    "                paper['_temp_full_text'] = extract_full_text(doc)\n",
    "            \n",
    "            paper['content'] = paper['_temp_full_text']\n",
    "        \n",
    "        metadata = await extractor.extract_metadata(paper)\n",
    "        paper.update(metadata)\n",
    "        \n",
    "        paper.pop('_temp_full_text', None)\n",
    "        paper.pop('content', None)\n",
    "        \n",
    "        print(f\"✓ (words: {paper.get('word_count', 'N/A')})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ {e}\")\n",
    "\n",
    "workflow_data['extract_metadata'] = papers\n",
    "print(f\"\\n✓ Metadata extraction complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Persist to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] + 1110.3385v1 (new)\n",
      "[2] + 1110.3672v1 (new)\n",
      "[3] + 1110.3888v2 (new)\n",
      "\n",
      "✓ Complete: 3 new, 0 updated/skipped\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def normalize_paper(data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Normalize paper data for database.\"\"\"\n",
    "    def to_dt(s):\n",
    "        if not s: return None\n",
    "        try:\n",
    "            if isinstance(s, str) and s.endswith(\"Z\"):\n",
    "                s = s.replace(\"Z\", \"+00:00\")\n",
    "            return datetime.fromisoformat(s) if isinstance(s, str) else s\n",
    "        except: return None\n",
    "    \n",
    "    def sanitize(text):\n",
    "        if not text or not isinstance(text, str): return text\n",
    "        return ''.join(c for c in text if ord(c) >= 32 or c in '\\n\\t\\r').strip() or None\n",
    "\n",
    "    return {\n",
    "        k: v for k, v in {\n",
    "            \"arxiv_id\": data.get(\"arxiv_id\"),\n",
    "            \"arxiv_url\": sanitize(data.get(\"arxiv_url\")) or \"\",\n",
    "            \"pdf_url\": sanitize(data.get(\"pdf_url\")) or \"\",\n",
    "            \"title\": sanitize(data.get(\"title\")) or \"Untitled\",\n",
    "            \"abstract\": sanitize(data.get(\"abstract\")) or \"\",\n",
    "            \"authors\": data.get(\"authors\") or [],\n",
    "            \"published_date\": to_dt(data.get(\"published\")) or datetime.now(),\n",
    "            \"updated_date\": to_dt(data.get(\"updated\")),\n",
    "            \"primary_category\": sanitize(data.get(\"primary_category\")) or \"unknown\",\n",
    "            \"categories\": data.get(\"categories\") or [],\n",
    "            \"is_processed\": data.get(\"is_processed\", False),\n",
    "            \"is_embedded\": data.get(\"is_embedded\", False),\n",
    "            \"docling_document\": data.get(\"docling_document\"),\n",
    "            \"word_count\": data.get(\"word_count\"),\n",
    "            \"metrics\": data.get(\"metrics\"),\n",
    "        }.items() if v is not None\n",
    "    }\n",
    "\n",
    "papers = workflow_data.get('extract_metadata', [])\n",
    "persisted, skipped = 0, 0\n",
    "\n",
    "with get_sync_session() as session:\n",
    "    for i, raw in enumerate(papers, 1):\n",
    "        try:\n",
    "            data = normalize_paper(raw)\n",
    "            arxiv_id = data.get(\"arxiv_id\")\n",
    "            \n",
    "            if not arxiv_id:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            existing = session.query(Paper).filter_by(arxiv_id=arxiv_id).first()\n",
    "            \n",
    "            if existing:\n",
    "                print(f\"[{i}] → {arxiv_id} (updating)\")\n",
    "                for key, value in data.items():\n",
    "                    if hasattr(existing, key):\n",
    "                        setattr(existing, key, value)\n",
    "                skipped += 1\n",
    "            else:\n",
    "                print(f\"[{i}] + {arxiv_id} (new)\")\n",
    "                filtered = {k: v for k, v in data.items() if hasattr(Paper, k) and v is not None}\n",
    "                paper = Paper(**filtered)\n",
    "                session.add(paper)\n",
    "                persisted += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"[{i}] ✗ {raw.get('arxiv_id', 'unknown')}: {e}\")\n",
    "            session.rollback()\n",
    "            skipped += 1\n",
    "    \n",
    "    if persisted > 0:\n",
    "        session.commit()\n",
    "\n",
    "workflow_data['persist_db'] = {\"persisted\": persisted, \"skipped\": skipped}\n",
    "print(f\"\\n✓ Complete: {persisted} new, {skipped} updated/skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test addign referncesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-11-03 22:26:31,885 ] [httpx] | Module: _client |Function: _send_single_request | Line: 1740 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/arXiv:1110.3385?fields=paperId%2CcitationCount%2CreferenceCount%2CinfluentialCitationCount%2Ctitle%2CexternalIds \"HTTP/1.1 200 OK\"\n",
      "[ 2025-11-03 22:26:32,211 ] [httpx] | Module: _client |Function: _send_single_request | Line: 1740 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/9023e74478f7299fcc4780aa21595dc5d9b76919/references?fields=citedPaper.title%2CcitedPaper.authors%2CcitedPaper.year%2CcitedPaper.externalIds%2CcitedPaper.paperId&limit=500&offset=0 \"HTTP/1.1 200 OK\"\n",
      "[ 2025-11-03 22:26:32,261 ] [httpx] | Module: _client |Function: _send_single_request | Line: 1740 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/9023e74478f7299fcc4780aa21595dc5d9b76919/citations?fields=citingPaper.title%2CcitingPaper.authors%2CcitingPaper.year%2CcitingPaper.externalIds%2CcitingPaper.paperId%2CisInfluential&limit=500&offset=0 \"HTTP/1.1 200 OK\"\n",
      "[ 2025-11-03 22:26:32,262 ] [researchmind] | Module: citation_extractor |Function: get_citations_and_references | Line: 151 - INFO - ✅ Extracted 35 refs, 1 citations for 1110.3385\n",
      "- 1110.3385v1 | source=semantic_scholar | citations=1 | references=35 | s2_id=9023e74478f7299fcc4780aa21595dc5d9b76919\n",
      "[ 2025-11-03 22:26:32,758 ] [httpx] | Module: _client |Function: _send_single_request | Line: 1740 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/arXiv:1110.3672?fields=paperId%2CcitationCount%2CreferenceCount%2CinfluentialCitationCount%2Ctitle%2CexternalIds \"HTTP/1.1 404 Not Found\"\n",
      "[ 2025-11-03 22:26:32,759 ] [researchmind] | Module: citation_extractor |Function: get_citations_and_references | Line: 165 - ERROR - Failed to extract citations for 1110.3672: Client error '404 Not Found' for url 'https://api.semanticscholar.org/graph/v1/paper/arXiv:1110.3672?fields=paperId%2CcitationCount%2CreferenceCount%2CinfluentialCitationCount%2Ctitle%2CexternalIds'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mk/Documents/Work/Projects/arxiv-ai-explorer/backend/src/services/arxiv/citation_extractor.py\", line 140, in get_citations_and_references\n",
      "    core = await self._fetch_paper_core(arxiv_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mk/Documents/Work/Projects/arxiv-ai-explorer/backend/src/services/arxiv/citation_extractor.py\", line 58, in _fetch_paper_core\n",
      "    return await self._s2_get(url, {\"fields\": fields})\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/mk/Documents/Work/Projects/arxiv-ai-explorer/backend/src/services/arxiv/citation_extractor.py\", line 49, in _s2_get\n",
      "    r.raise_for_status()\n",
      "  File \"/opt/miniconda3/envs/arxiv/lib/python3.11/site-packages/httpx/_models.py\", line 829, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://api.semanticscholar.org/graph/v1/paper/arXiv:1110.3672?fields=paperId%2CcitationCount%2CreferenceCount%2CinfluentialCitationCount%2Ctitle%2CexternalIds'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n",
      "- 1110.3672v1 | source=none | citations=0 | references=0 | s2_id=None\n",
      "[ 2025-11-03 22:26:33,276 ] [httpx] | Module: _client |Function: _send_single_request | Line: 1740 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/arXiv:1110.3888?fields=paperId%2CcitationCount%2CreferenceCount%2CinfluentialCitationCount%2Ctitle%2CexternalIds \"HTTP/1.1 200 OK\"\n",
      "[ 2025-11-03 22:26:33,798 ] [httpx] | Module: _client |Function: _send_single_request | Line: 1740 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/af16e5a64c9cdfd4e726f624e59c1677da1cbea5/citations?fields=citingPaper.title%2CcitingPaper.authors%2CcitingPaper.year%2CcitingPaper.externalIds%2CcitingPaper.paperId%2CisInfluential&limit=500&offset=0 \"HTTP/1.1 200 OK\"\n",
      "[ 2025-11-03 22:26:33,865 ] [httpx] | Module: _client |Function: _send_single_request | Line: 1740 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/af16e5a64c9cdfd4e726f624e59c1677da1cbea5/references?fields=citedPaper.title%2CcitedPaper.authors%2CcitedPaper.year%2CcitedPaper.externalIds%2CcitedPaper.paperId&limit=500&offset=0 \"HTTP/1.1 200 OK\"\n",
      "[ 2025-11-03 22:26:33,866 ] [researchmind] | Module: citation_extractor |Function: get_citations_and_references | Line: 151 - INFO - ✅ Extracted 21 refs, 0 citations for 1110.3888\n",
      "- 1110.3888v2 | source=semantic_scholar | citations=0 | references=21 | s2_id=af16e5a64c9cdfd4e726f624e59c1677da1cbea5\n",
      "\n",
      "Sample result keys: dict_keys(['source', 'arxiv_id', 's2_paper_id', 'reference_count', 'citation_count', 'influential_citation_count', 'references', 'cited_by'])\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from src.services.arxiv.citation_extractor import CitationExtractor\n",
    "\n",
    "test_ids = [\"1110.3385v1\", \"1110.3672v1\", \"1110.3888v2\"]\n",
    "\n",
    "async def run_extraction(ids):\n",
    "    extractor = CitationExtractor()\n",
    "    try:\n",
    "        results = []\n",
    "        for aid in ids:\n",
    "            res = await extractor.get_citations_and_references(aid)\n",
    "            results.append(res)\n",
    "            print(f\"- {aid} | source={res.get('source')} | \"\n",
    "                  f\"citations={res.get('citation_count')} | \"\n",
    "                  f\"references={res.get('reference_count')} | \"\n",
    "                  f\"s2_id={res.get('s2_paper_id')}\")\n",
    "        return results\n",
    "    finally:\n",
    "        await extractor.close()\n",
    "\n",
    "results = await run_extraction(test_ids)\n",
    "print(\"\\nSample result keys:\", results[0].keys() if results else [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORKFLOW 1 SUMMARY\n",
      "============================================================\n",
      "Fetched:  10 papers\n",
      "Parsed:   10 papers\n",
      "Persisted: 0 new\n",
      "Updated:   10 existing\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"WORKFLOW 1 SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Fetched:  {len(workflow_data.get('fetch_papers', []))} papers\")\n",
    "print(f\"Parsed:   {sum(1 for p in workflow_data.get('parse_pdfs', []) if p.get('docling_document'))} papers\")\n",
    "print(f\"Persisted: {workflow_data.get('persist_db', {}).get('persisted', 0)} new\")\n",
    "print(f\"Updated:   {workflow_data.get('persist_db', {}).get('skipped', 0)} existing\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
