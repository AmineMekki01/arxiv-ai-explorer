{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test arXiv PDF Parser Integration\n",
    "\n",
    "This notebook tests the arXiv client, PDF parsing via Docling, metadata extraction and chunking workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: d:\\Projects\\arxiv-ai-explorer\\backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"./..\")\n",
    "print(\"CWD:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amine\\anaconda3\\envs\\arxiv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "from src.services.arxiv.client import ArxivClient\n",
    "from src.services.pdf_parser.factory import make_pdf_parser_service\n",
    "from src.services.arxiv.metadata_extractor import MetadataExtractor\n",
    "from src.config import get_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Search for recent papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using category: cs.AI\n",
      "[ 2025-09-21 14:28:04,048 ] [researchmind] | Module: client |Function: _make_request | Line: 50 - INFO - Fetching arXiv data: http://export.arxiv.org/api/query?search_query=cat%3Acs.AI&start=0&max_results=3&sortBy=submittedDate&sortOrder=descending\n",
      "[ 2025-09-21 14:28:04,210 ] [researchmind] | Module: client |Function: search_papers | Line: 178 - INFO - Found 3 papers for query: cat:cs.AI\n",
      "Found 3 papers\n",
      "\n",
      "1. Generalizable Geometric Image Caption Synthesis...\n",
      "   arXiv ID: 2509.15217v1\n",
      "   PDF URL: http://arxiv.org/pdf/2509.15217v1\n",
      "\n",
      "2. Explicit Context-Driven Neural Acoustic Modeling for High-Fidelity RIR Generatio...\n",
      "   arXiv ID: 2509.15210v1\n",
      "   PDF URL: http://arxiv.org/pdf/2509.15210v1\n",
      "\n",
      "3. FlowRL: Matching Reward Distributions for LLM Reasoning...\n",
      "   arXiv ID: 2509.15207v1\n",
      "   PDF URL: http://arxiv.org/pdf/2509.15207v1\n"
     ]
    }
   ],
   "source": [
    "settings = get_settings()\n",
    "client = ArxivClient()\n",
    "category = (settings.arxiv_categories[0] if settings.arxiv_categories else 'cs.AI')\n",
    "print(f'Using category: {category}')\n",
    "\n",
    "papers = await client.search_papers(\n",
    "    query=f'cat:{category}',\n",
    "    max_results=3,\n",
    "    sort_by='submittedDate',\n",
    "    sort_order='descending',\n",
    ")\n",
    "print(f'Found {len(papers)} papers')\n",
    "for i, paper in enumerate(papers):\n",
    "    print(f\"\\n{i+1}. {paper['title'][:80]}...\")\n",
    "    print(f\"   arXiv ID: {paper['arxiv_id']}\")\n",
    "    print(f\"   PDF URL: {paper['pdf_url']}\")\n",
    "\n",
    "test_paper = papers[0] if papers else None\n",
    "assert test_paper, 'No papers found for the selected category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arxiv_id': '2509.15217v1',\n",
       " 'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       " 'abstract': 'Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\\\\%\\\\text{-}4.8\\\\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\\\\%\\\\text{-}3.9\\\\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.',\n",
       " 'authors': ['Yue Xin',\n",
       "  'Wenyuan Wang',\n",
       "  'Rui Pan',\n",
       "  'Ruida Wang',\n",
       "  'Howard Meng',\n",
       "  'Renjie Pi',\n",
       "  'Shizhe Diao',\n",
       "  'Tong Zhang'],\n",
       " 'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       " 'primary_category': 'cs.AI',\n",
       " 'published': '2025-09-18T17:59:11Z',\n",
       " 'updated': '2025-09-18T17:59:11Z',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2509.15217v1',\n",
       " 'arxiv_url': 'http://arxiv.org/abs/2509.15217v1',\n",
       " 'doi': '',\n",
       " 'journal_ref': ''}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Download PDF from arXiv URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with paper: Generalizable Geometric Image Caption Synthesis\n",
      "arXiv ID: 2509.15217v1\n",
      "[ 2025-09-21 14:28:04,397 ] [researchmind] | Module: client |Function: download_pdf | Line: 272 - INFO - Downloading PDF from: https://arxiv.org/pdf/2509.15207\n",
      "[ 2025-09-21 14:28:04,650 ] [researchmind] | Module: client |Function: download_pdf | Line: 296 - INFO - Downloaded PDF (0.9MB) to: data\\test_downloads\\2509.15217v1.pdf\n",
      "Downloaded PDF to: data\\test_downloads\\2509.15217v1.pdf\n",
      "File size: 0.88 MB\n",
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Testing with paper: {test_paper['title']}\")\n",
    "print(f\"arXiv ID: {test_paper['arxiv_id']}\")\n",
    "\n",
    "download_dir = Path('./data/test_downloads')\n",
    "download_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_path = await client.download_pdf(\n",
    "    pdf_url=\"https://arxiv.org/pdf/2509.15207\",\n",
    "    download_path=download_dir / f\"{test_paper['arxiv_id'].replace('/', '_')}.pdf\",\n",
    "    max_file_size_mb=150\n",
    ")\n",
    "print(f'Downloaded PDF to: {pdf_path}')\n",
    "print(f'File size: {pdf_path.stat().st_size / (1024*1024):.2f} MB')\n",
    "print(f'File exists: {pdf_path.exists()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Parse downloaded PDF with Docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-21 14:28:04,686 ] [docling.datamodel.document] | Module: document |Function: _guess_format | Line: 328 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "[ 2025-09-21 14:28:04,897 ] [docling.document_converter] | Module: document_converter |Function: _convert | Line: 318 - INFO - Going to convert document batch...\n",
      "[ 2025-09-21 14:28:04,898 ] [docling.document_converter] | Module: document_converter |Function: _get_pipeline | Line: 363 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 60c8066c482b9239b869b997da3fb1da\n",
      "[ 2025-09-21 14:28:05,879 ] [docling.models.factories.base_factory] | Module: base_factory |Function: load_from_plugins | Line: 112 - INFO - Loading plugin 'docling_defaults'\n",
      "[ 2025-09-21 14:28:05,884 ] [docling.models.factories.base_factory] | Module: base_factory |Function: load_from_plugins | Line: 104 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "[ 2025-09-21 14:28:05,885 ] [docling.models.factories] | Module: __init__ |Function: get_picture_description_factory | Line: 26 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "[ 2025-09-21 14:28:05,943 ] [docling.models.factories.base_factory] | Module: base_factory |Function: load_from_plugins | Line: 112 - INFO - Loading plugin 'docling_defaults'\n",
      "[ 2025-09-21 14:28:05,952 ] [docling.models.factories.base_factory] | Module: base_factory |Function: load_from_plugins | Line: 104 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "[ 2025-09-21 14:28:05,953 ] [docling.models.factories] | Module: __init__ |Function: get_ocr_factory | Line: 16 - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "[ 2025-09-21 14:28:05,968 ] [docling.utils.accelerator_utils] | Module: accelerator_utils |Function: decide_device | Line: 82 - INFO - Accelerator device: 'cpu'\n",
      "[ 2025-09-21 14:28:07,174 ] [docling.utils.accelerator_utils] | Module: accelerator_utils |Function: decide_device | Line: 82 - INFO - Accelerator device: 'cpu'\n",
      "[ 2025-09-21 14:28:07,913 ] [docling.pipeline.base_pipeline] | Module: base_pipeline |Function: execute | Line: 65 - INFO - Processing document 2509.15217v1.pdf\n",
      "[ 2025-09-21 14:28:44,922 ] [docling.document_converter] | Module: document_converter |Function: _convert | Line: 342 - INFO - Finished converting document 2509.15217v1.pdf in 40.23 sec.\n",
      "[ 2025-09-21 14:28:44,956 ] [docling_core.types.doc.document] | Module: document |Function: export_to_markdown | Line: 4532 - WARNING - Parameter `strict_text` has been deprecated and will be ignored.\n",
      "[ 2025-09-21 14:28:44,960 ] [src.services.pdf_parser.parser] | Module: parser |Function: parse_pdf | Line: 34 - INFO - Parsed 2509.15217v1.pdf\n"
     ]
    }
   ],
   "source": [
    "pdf_parser = make_pdf_parser_service()\n",
    "parsed_content = await pdf_parser.parse_pdf(pdf_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if parsed_content:\n",
    "    paper_with_content = {**paper}\n",
    "    paper_with_content['content'] = parsed_content.raw_text\n",
    "    paper_with_content['sections'] = [\n",
    "        {'title': section.title, 'content': section.content}\n",
    "        for section in parsed_content.sections\n",
    "    ]\n",
    "    paper_with_content['is_processed'] = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arxiv_id': '2509.15207v1',\n",
       " 'title': 'FlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       " 'abstract': 'We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\\\\%$ over GRPO and $5.1\\\\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.',\n",
       " 'authors': ['Xuekai Zhu',\n",
       "  'Daixuan Cheng',\n",
       "  'Dinghuai Zhang',\n",
       "  'Hengli Li',\n",
       "  'Kaiyan Zhang',\n",
       "  'Che Jiang',\n",
       "  'Youbang Sun',\n",
       "  'Ermo Hua',\n",
       "  'Yuxin Zuo',\n",
       "  'Xingtai Lv',\n",
       "  'Qizheng Zhang',\n",
       "  'Lin Chen',\n",
       "  'Fanghao Shao',\n",
       "  'Bo Xue',\n",
       "  'Yunchong Song',\n",
       "  'Zhenjie Yang',\n",
       "  'Ganqu Cui',\n",
       "  'Ning Ding',\n",
       "  'Jianfeng Gao',\n",
       "  'Xiaodong Liu',\n",
       "  'Bowen Zhou',\n",
       "  'Hongyuan Mei',\n",
       "  'Zhouhan Lin'],\n",
       " 'categories': ['cs.LG', 'cs.AI', 'cs.CL'],\n",
       " 'primary_category': 'cs.LG',\n",
       " 'published': '2025-09-18T17:56:36Z',\n",
       " 'updated': '2025-09-18T17:56:36Z',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2509.15207v1',\n",
       " 'arxiv_url': 'http://arxiv.org/abs/2509.15207v1',\n",
       " 'doi': '',\n",
       " 'journal_ref': '',\n",
       " 'content': \"## FlowRL: Matching Reward Distributions for LLM Reasoning\\n\\nXuekai Zhu 1 , Daixuan Cheng 6 , Dinghuai Zhang 3 , Hengli Li 5 , Kaiyan Zhang 4 , Che Jiang 4 , Youbang Sun 4 , Ermo Hua 4 , Yuxin Zuo 4 , Xingtai Lv 4 , Qizheng Zhang 7 , Lin Chen 1 , Fanghao Shao 1 , Bo Xue 1 , Yunchong Song 1 , Zhenjie Yang 1 , Ganqu Cui 2 , Ning Ding 4 , 2 , Jianfeng Gao 3 , Xiaodong Liu 3 , Bowen Zhou 4 , 2 ‡ , Hongyuan Mei 8 ‡ , Zhouhan Lin 1 , 2 ‡\\n\\n1 Shanghai Jiao Tong University 2 Shanghai AI Laboratory 3 Microsoft Research 4 Tsinghua University 5 Peking University 6 Renmin University of China 7 Stanford University 8 Toyota Technological Institute at Chicago\\n\\n/envelope hongyuanmei@gmail.com /envelope xuekaizhu0@gmail.com /github FlowRL ‡ Corresponding Authors.\\n\\nAbstract | We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods ( e.g. , PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10 . 0% over GRPO and 5 . 1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.\\n\\nFigure 1 | Top : Comparison between distribution-matching and reward-maximizing approaches. FlowRL (left) learns to match the full reward distribution, maintaining diversity across multiple modes with low KL divergence. In contrast, reward-maximizing methods like GRPO (right) concentrate on a single high-reward peak, leading to mode collapse and higher KL divergence. Bottom : Performance comparison. FlowRL consistently outperforms GRPO across math and code domains.\\n\\n## 1. Introduction\\n\\nReinforcement learning (RL) plays a crucial role in the post-training of large language models (LLMs) [Zhang et al., 2025b]. A series of powerful reasoning models [Guo et al., 2025, Kavukcuoglu, 2025, Rastogi et al., 2025] have employed large-scale reinforcement learning to achieve strong performance on highly challenging benchmarks [He et al., 2024]. The evolution of RL algorithms for LLM reasoning has progressed through several key stages: REINFORCE [Sutton et al., 1999a] provides a solid baseline that is easy to implement and efficient in simple settings; PPO [Schulman et al., 2017] improves upon REINFORCE with better stability and efficiency in complex settings; GRPO [Shao et al., 2024] simplifies PPO training by eliminating value functions and relying on group comparisons, though at the cost of requiring more rollouts per update. However, all these methods share a fundamental limitation in their reward-maximizing objective.\\n\\nReward-maximizing RL methods tend to overfit to the dominant mode of the reward distribution [Gao et al., 2023, Pan et al., 2022, Skalse et al., 2022, Zelikman et al., 2022]. This often results in limited diversity among generated reasoning paths and reduces generalization to less frequent yet valid logical outcomes [Hu et al., 2023]. As illustrated in Figure 1, GRPO neglects other meaningful modes. These drawbacks become especially pronounced in complex long chain-of-thought (CoT; Wei et al., 2022) reasoning, where capturing a diverse distribution of plausible solutions is essential for effective generalization [Liu et al., 2025a]. Recent approaches adjust the clip ratio [Yu et al., 2025b], augment the advantage function with an entropy-based term [Cheng et al., 2025], or selectively promote high-entropy tokens [Wang et al., 2025], thereby dynamically adapting the training data distribution and implicitly increasing diversity during training. This raises a fundamental question: How can we promote diverse exploration to prevent convergence to dominant solution patterns in RL training?\\n\\nIn this paper, we propose FlowRL , a policy optimization algorithm that aligns the policy model with the full reward distribution, encouraging mode coverage. FlowRL achieves more efficient exploration by fundamentally shifting from reward maximization to reward distribution matching, thereby addressing the inherent mode-collapse limitations of previous RL approaches. As illustrated in Figure 1, the core idea of FlowRL is to introduce a learnable partition function that normalizes scalar rewards into a target distribution, and to minimize the reverse KL divergence between the policy and this reward-induced distribution. We develop this KL objective based on the trajectory balance formulation from GFlowNets [Bengio et al., 2023b], providing a gradient equivalence proof that bridges generative modeling and policy optimization. To address the challenges of long CoT training, we introduce two key technical solutions: length normalization to tackle gradient explosion issues that occur with variable-length CoT reasoning, and importance sampling to correct for the distribution mismatch between generated rollouts and the current policy.\\n\\nWe compare FlowRL with mainstream RL algorithms including REINFORCE++, PPO, and GRPO across math and code domains, using both base and distilled LLMs (7B, 32B). In math domain, FlowRL outperforms GRPO and PPO by 10 . 0% and 5 . 1%, respectively, demonstrating consistent improvements across six challenging math benchmarks. Furthermore, FlowRL surpasses both PPO and GRPO on three challenging coding benchmarks, highlighting its strong generalization capabilities in code reasoning tasks. To understand what drives these performance gains, we analyze the diversity of generated reasoning paths. This diversity analysis confirms that FlowRL generates substantially more diverse rollouts than baseline methods, validating our approach's effectiveness in exploring multiple solution strategies.\\n\\nContributions. We summarize the key contributions of this work as follows:\\n\\n- We propose FlowRL, a policy optimization algorithm that shifts from reward maximization to\\n\\n- reward distribution matching via flow balance, encouraging diverse reasoning path exploration while addressing the inherent mode-collapse limitations of existing RL methods.\\n- FlowRL outperforms GRPO and PPO by 10.0% and 5.1% respectively across math benchmarks and demonstrates strong generalization on code reasoning tasks, with diversity analysis confirming substantially more diverse solution exploration.\\n- We introduce length normalization and importance sampling to enable effective training on variablelength CoT reasoning, addressing gradient explosion and sampling mismatch issues.\\n\\n## 2. Preliminaries\\n\\nReinforcement Learning for Reasoning. Weformulate reasoning as a conditional generation problem, where the policy model receives a question x ∈ X and generates an answer y ∈ Y . The objective is to learn a policy 𝜋𝜃 ( y | x ) that produces high-quality answers under task-specific reward signals 𝑟 . To better illustrate the policy optimization procedure, we provide a detailed formulation of GRPO below. For each question x , GRPO samples a group of answers { y 1 , y 2 , . . . , y 𝐺 } from old policy 𝜋𝜃𝑜𝑙𝑑 and updates the model by maximizing the following objective:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝜖 and 𝜆 are hyper-parameters. Here, 𝐴𝑖 denotes the advantage, computed by normalizing the group reward values { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } as 𝐴𝑖 = 𝑟 𝑖 -mean ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) std ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) . Compared to GRPO, REINFORCE applies the policy gradient directly, without advantage normalization, clipping, or KL regularization. PPO uses a critic model to estimate the advantage and employs importance sampling to stabilize policy updates.\\n\\nGFlowNets. Generative Flow Networks [Bengio et al., 2023a] are a probabilistic framework for training stochastic policies to sample discrete, compositional objects ( e.g. , graphs, sequences) in proportion to a given reward. As shown in Figure 2, the core principle of GFlowNets is to balance the forward and backward probability flows at each state, inspired by flow matching [Bengio et al., 2021]. The initial flow is estimated by 𝑍𝜙 ( 𝑠 0 ) at the initial state 𝑠 0 . The output flow is equal to the outcome reward 𝑟 ( 𝑠 𝑛 ) conditioned at the final state 𝑠 𝑛 . Following Lee et al. [2024], we use a 3-layer MLP to parameterize 𝑍𝜙 . This flow-balancing mechanism facilitates the discovery of diverse,\\n\\nFigure 2 | GFlowNets [Bengio et al., 2023a], a flow-balance perspective on reinforcement learning. The initial flow 𝑍𝜙 ( 𝑠 0 ) injects probability mass into the environment, which is transported through intermediate states by the policy 𝜋𝜃 and accumulated at terminal states in proportion to the scalar rewards.\\n\\nhigh-reward solutions by ensuring proper exploration of the solution space. See Appendix C for detailed GFlowNets background.\\n\\n## 3. Methodology\\n\\nIn this section, we first formulate distribution matching in reinforcement learning through reverse KL divergence and establish its connection to trajectory balance from GFlowNets. To address the challenges of gradient explosion and sampling mismatch encountered during long CoT training, we further incorporate length normalization and importance sampling. Using this enhanced framework, we derive a flow-balanced objective, termed FlowRL .\\n\\n## 3.1. From Reward Maximization to Distribution Matching\\n\\nAs illustrated in Figure 1, recent powerful large reasoning models typically employ reward-maximizing RL algorithms, such as PPO or GRPO. However, these methods tend to optimize toward the dominant reward mode, frequently resulting in mode collapse and the neglect of other plausible, high-quality reasoning paths. To address this fundamental limitation, we propose optimizing the policy by aligning its output distribution to a target reward distribution. A simple yet effective way to achieve this is to minimize the reverse KL divergence 1 between the policy and this target. However, in long CoT reasoning tasks, the available supervision in RL is a scalar reward, rather than a full distribution. Moreover, enumerating or sampling all valid trajectories to recover the true reward distribution is computationally intractable.\\n\\nInspired by energy-based modeling [Du and Mordatch, 2019, Hinton et al., 1995], we introduce a learnable partition function 𝑍𝜙 ( x ) to normalize scalar rewards into a valid target distribution. This allows us to minimize the reverse KL divergence between the policy and the reward-weighted distribution, formalized as:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝑟 ( x , y ) is the reward function, 𝛽 is a hyperparameter, 𝑍𝜙 ( x ) is the learned partition function, and the resulting target distribution is defined as ˜ 𝜋 ( y | x ) = exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) . This objective encourages the policy to sample diverse, high-reward trajectories in proportion to their rewards, rather than collapsing to dominant modes as in standard reward maximization.\\n\\nWhile the KL-based formulation provides a principled target distribution, we derive a more practical, RL-style objective that facilitates efficient policy optimization.\\n\\nProposition 1. In terms of expected gradients, minimizing the KL objective in Eq. 2 is equivalent to minimizing the trajectory balance loss used in GFlowNet [Bartoldson et al., 2025, Lee et al., 2024, Malkin et al., 2022, 2023]:\\n\\n<!-- formula-not-decoded -->\\n\\nRemark 2 ( Trajectory balance as a practical surrogate for KL minimization ) . Given the equivalence established in Proposition 1, the KL-based distribution matching objective can be reformulated as the trajectory balance loss. This reformulation provides a practical optimization approach by using a stable squared loss form rather than direct KL optimization, and by treating 𝑍𝜙 ( x ) as a learnable parameter rather than requiring explicit computation of the intractable partition function. The trajectory balance objective thus serves as a tractable surrogate for reward-guided KL minimization that can be directly integrated into existing RL frameworks.\\n\\n1 We use reverse KL since we can only sample from the policy model, not the target reward distribution.\\n\\n## 3.2. FlowRL\\n\\nAs established in Proposition 1, the target reward distribution can be approximated by optimizing the trajectory balance objective. However, applying this objective directly to long CoT reasoning introduces two key challenges:\\n\\nProblem I: Exploding gradients from long trajectories. Trajectory balance is a sequence-level objective, and applying it to long CoT reasoning with up to 8K tokens leads to exploding gradients and unstable updates. This issue is not observed in prior GFlowNets works, which typically operate on short trajectories in small discrete spaces. Specifically, the log-probability term log 𝜋𝜃 ( y | x ) decomposes into a token-wise sum, ˝ 𝑡 log 𝜋𝜃 ( y 𝑡 | y &lt;𝑡 , x ) , causing the gradient norm to potentially scale with sequence length.\\n\\nProblem II: Sampling mismatch. Mainstream RL algorithms such as PPO and GRPO commonly perform micro-batch updates and reuse trajectories collected from an old policy 𝜋𝜃 old , enabling data-efficient training. In contrast, the KL-based trajectory balance objective assumes fully onpolicy sampling, where responses are drawn from the current policy. This mismatch poses practical limitations when integrating trajectory balance into existing RL pipelines.\\n\\nThese limitations motivate our reformulation that retains the benefits of distribution matching while addressing key practical challenges. To enable this reformulation, we first redefine the reward function following established practices in GFlowNets literature [Bartoldson et al., 2025, Lee et al., 2024, Yu et al., 2025a] by incorporating a reference model as a prior constraint on the reward distribution. Specifically , we modify the original exp ( 𝛽𝑟 ( x , y )) to include the reference model:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝑟 ( x , y ) denotes the outcome reward commonly used in reinforcement learning and 𝜋 ref is the initial pre-trained model. We follow Guo et al. [2025] to use outcome-based reward signals, and apply group normalization to 𝑟 ( x , y ) as ˆ 𝑟 𝑖 = ( 𝑟 𝑖 -mean ( r ))/ std ( r ) , where r = { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } denotes the set of rewards within a sampled group. By substituting the redefined reward formulation Eq. 4 into Eq. 3, we derive the following objective 2 :\\n\\n<!-- formula-not-decoded -->\\n\\nRemark 3 ( Reward shaping via length normalization ) . Trajectory balance treats both the initial flow and the outcome reward as sequence-level quantities. In contrast, standard policy optimization methods such as PPO or GRPO assign rewards at the token level and compute gradients at each step. However, for trajectories of varying lengths ( e.g. , CoT responses), this mismatch can cause the log-probability term log 𝜋𝜃 ( y | x ) = ˝ | y | 𝑡 = 1 log 𝜋𝜃 ( 𝑦 𝑡 | 𝑦 &lt;𝑡 , x ) to scale with sequence length. To address this, we apply a form of reward shaping by normalizing log-probabilities with respect to sequence length. Specifically, we rescale the term as 1 | y | log 𝜋𝜃 ( y | x ) , balancing the contributions of long and short sequences and stabilizing the learning signal.\\n\\nRemark 4 ( Importance sampling for data-efficient training ) . To mitigate sampling mismatch, we employ importance sampling inspired by PPO to stabilize policy updates with off-policy data. We re-weight stale trajectories using the importance ratio 𝑤 = 𝜋𝜃 ( y | x )/ 𝜋 old ( y | x ) , which serves as a coefficient in the surrogate loss. Since our objective focuses on optimizing trajectory balance rather than expected return, we detach the gradient from the current policy to prevent excessive policy drift: 𝑤 = detach [ 𝜋𝜃 ( y | x )]/ 𝜋 old ( y | x ) . For additional stability, we incorporate PPO-style clipping to detach\\n\\n.\\n\\n<!-- formula-not-decoded -->\\n\\n2 The substitution replaces 𝛽𝑟 ( x , y ) in trajectory balance objective Eq. 3 with 𝛽𝑟 ( x , y ) + log 𝜋 ref ( y | x ) to incorporate the reference model constraint.\\n\\nIncorporating these improvements into Eq. 5, we arrive at the following FlowRL objective:\\n\\nFlowRL\\n\\n<!-- formula-not-decoded -->\\n\\nwhere the clipped importance weight 𝑤 and normalized reward ˆ 𝑟 ( x , y ) are defined as:\\n\\n<!-- formula-not-decoded -->\\n\\nWe use this objective to update the policy parameters 𝜃 during training, and refer to this strategy as FlowRL . Implementation details and theoretical analysis are provided in § 5 and § B, respectively.\\n\\n## 4. Related Work\\n\\n## 4.1. Reinforcement Learning for Reasoning\\n\\nReinforcement learning has emerged as a powerful approach for large language models post-training on reasoning tasks [Guo et al., 2025, Lightman et al., 2023b, Schulman et al., 2017, Shao et al., 2024, Sutton et al., 1999b]. Most approaches employ reward-maximizing RL to optimize expected cumulative returns. Entropy regularization [Ahmed et al., 2019, Cheng et al., 2025, Haarnoja et al., 2018] is a classical technique for mitigating mode collapse by promoting diversity in the policy's output distribution, and has also been shown to enhance reasoning capabilities in various settings [Chao et al., 2024, Eysenbach and Levine, 2021]. However, for long CoT reasoning, the extended trajectory length (e.g., 8k-16k tokens) makes it difficult for the regularization signal to effectively influence reward-maximizing learning. Recent work [Cheng et al., 2025, Cui et al., 2025, Dong et al., 2025, Wang et al., 2025] has discovered that training with more diverse or high-entropy training data can further enhance training effectiveness. Compared to traditional entropy regularization, the above methods explicitly increase the proportion of low-probability (i.e., high-entropy) tokens in the training data. In our work, we address the mode-collapse problem by fundamentally shifting from reward maximization to reward distribution matching in our RL formulation.\\n\\n## 4.2. GFlowNets\\n\\nGFlowNets [Bengio et al., 2023a] represent a class of diversity-driven algorithms designed to balance probability flows across states. They have rich connections to probabilistic modeling methods [Ma et al., Malkin et al., 2023, Zhang et al., 2022a,b, 2024a, Zimmermann et al., 2022], and control methods [Pan et al., 2023b,c,d, Tiapkin et al., 2024, Zhang et al., 2024b]. This advantage has enabled GFlowNets to achieve successful applications in multiple downstream tasks, such as molecular drug discovery [Jain et al., 2022, 2023a,b, Kim et al., 2023, 2024, Liu et al., 2022, Pan et al., 2023a, Shen et al., 2023], phylogenetic inference [Zhou et al., 2024], and combinatorial optimization [Zhang et al., 2023a,b]. For generative AI, GFlowNets provide a powerful approach to align pretrained models in scenarios such as image generation [Yun et al., 2025, Zhang et al., 2025a] and language model fine-tuning [Hu et al., 2024, Lee et al., 2024, Yu et al., 2025a]. Another line of work primarily focuses on the theoretical aspects of GFlowNets. Recent theoretical studies have interpreted GFlowNets as solving a maximum entropy reinforcement learning problem within a modified Markov Decision Process (MDP) [Deleu et al., 2024, Mohammadpour et al., 2024, Tiapkin et al., 2024]. These theoretical contributions have\\n\\ninspired us to enhance reinforcement learning from a more foundational standpoint using GFlowNets principles. A comprehensive overview of GFlowNets theory can be found in Appendix C.\\n\\n## 4.3. Flow-Matching Policies\\n\\nFlow matching simplifies diffusion-based approaches by learning vector fields that transport samples from prior to target distributions [Lipman et al., 2023]. Recent work has explored flow matching for policy optimization. McAllister et al. [2025] reformulates policy optimization using advantageweighted ratios from conditional flow matching loss, enabling flow-based policy training without expensive likelihood computations. Pfrommer et al. [2025] explored reward-weighted flow matching for improving policies beyond demonstration performance. Park et al. [2025] uses a separate one-step policy to avoid unstable backpropagation through time when training flow policies with RL. Zhang et al. [2025a] proposed a combined loss function integrating PPO and GFlowNets to optimize diffusion model alignment. However, these approaches focus on continuous control, image generation, or vision-action models, rather than addressing mode-collapse limitations in reward-maximizing RL. Inspired by flow matching principles, our work improves upon RL training to enhance training stability while promoting diverse solution exploration.\\n\\n## 5. Experiment Settings\\n\\nBackbone Models. There are two learnable modules in Eq. 6: the policy model 𝜋𝜃 and the partition function 𝑍𝜙 . For the policy model 𝜋𝜃 , we use Qwen-2.5-7B/32B [Team, 2024] for math tasks and DeepSeek-R1-Distill-Qwen-7B [DeepSeek-AI, 2025] for code tasks, respectively. For partition function 𝑍𝜙 , following Lee et al. [2024], we use a randomly initialized 3-layer MLP with hidden dimensions matching those of the base model. The reference model 𝜋 ref is the corresponding fixed pretrained model. All training scripts are based on the veRL [Sheng et al., 2024]. For the reward function, following Lee et al. [2024], we set the hyperparameter 𝛽 = 15.\\n\\nBaselines. We compare our method against three representative reward-maximization RL baselines: REINFORCE++ (R++; Hu et al., 2025, Sutton et al., 1999b), PPO [Schulman et al., 2017], and GRPO [Shao et al., 2024]. All baselines follow the official veRL recipes, with consistent training configurations. For fair comparison, all methods use the same learning rate, batch size, and training steps, and are evaluated at convergence using identical step counts.\\n\\nTraining Configuration. We experiment on both math and code domains. For the math domain, we use the training set collected from DAPO [Yu et al., 2025b]. For the code domain, we follow the setup of DeepCoder [Luo et al., 2025], using their training set. For 7B model training, we use a single node equipped with 8 NVIDIA H800 GPUs (80GB memory each). For 32B model training, we scale to 4 nodes with 32 GPUs to accommodate the larger memory requirements. All experiments use max_prompt_length = 2048 and max_response_length = 8192 across both model sizes. We use a batch size of 512 for math reasoning tasks and 64 for code reasoning tasks. We set the learning rate to 1e-6 and enable dynamic batch sizing in veRL for efficient training. For GRPO and FlowRL, we configure rollout_n = 8, meaning each prompt generates 8 response rollouts as the group size.\\n\\nEvaluation Configuration. For the math domain, we evaluate on six challenging benchmarks: AIME 2024/2025 [MAA, 2025], AMC 2023 [MAA, 2023], MATH-500 [Lightman et al., 2023a], Minerva [Lewkowycz et al., 2022], and Olympiad [He et al., 2024]. For the code domain, we evaluate on LiveCodeBench [Jain et al., 2024], CodeForces [Penedo et al., 2025], and HumanEval+ [Chen et al., 2021]. For all evaluation datasets, we perform 16 rollouts and report the average accuracy, denoted as Avg@16. We further report rating and percentile for Codeforces. During generation, we\\n\\nTable 1 | Results on math benchmarks. We report Avg@16 accuracy with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL outperforms all baselines across both 7B and 32B model scales.\\n\\n|                                       | AIME24                                | AIME25                                | AMC23                                 | MATH500                               | Minerva                               | Olympiad                              | Avg                                   |\\n|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|\\n| Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K |\\n| Backbone                              | 4.6                                   | 2.1                                   | 28.6                                  | 52.5                                  | 27.0                                  | 21.4                                  | 22.7                                  |\\n| R++                                   | 14 . 8 + 10 . 2                       | 9 . 2 + 7 . 1                         | 52 . 7 + 24 . 1                       | 44 . 4 - 8 . 1                        | 17 . 4 - 9 . 6                        | 24 . 5 + 3 . 1                        | 27.1                                  |\\n| PPO                                   | 26 . 9 + 22 . 3                       | 20 . 4 + 18 . 3                       | 76 . 4 + 47 . 8                       | 69 . 2 + 16 . 7                       | 28 . 8 + 1 . 8                        | 37 . 9 + 16 . 5                       | 43.3                                  |\\n| GRPO                                  | 23 . 1 + 18 . 5                       | 14 . 6 + 12 . 5                       | 76 . 9 + 48 . 3                       | 61 . 6 + 9 . 1                        | 19 . 0 - 8 . 0                        | 34 . 9 + 13 . 5                       | 38.3                                  |\\n| FlowRL                                | 24 . 0 + 19 . 4                       | 21 . 9 + 19 . 8                       | 73 . 8 + 45 . 2                       | 80 . 8 + 28 . 3                       | 38 . 2 + 11 . 2                       | 51 . 8 + 30 . 4                       | 48.4                                  |\\n| Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  |\\n| Backbone                              | 4.4                                   | 2.1                                   | 30.8                                  | 54.5                                  | 22.4                                  | 24.0                                  | 23.0                                  |\\n| R++                                   | 11 . 0 + 6 . 6                        | 5 . 4 + 3 . 3                         | 66 . 7 + 35 . 9                       | 54 . 3 - 0 . 2                        | 24 . 4 + 2 . 0                        | 27 . 3 + 3 . 3                        | 31.5                                  |\\n| PPO                                   | 9 . 4 + 5 . 0                         | 7 . 3 + 5 . 2                         | 63 . 4 + 32 . 6                       | 58 . 0 + 3 . 5                        | 26 . 5 + 4 . 1                        | 27 . 3 + 3 . 3                        | 32.0                                  |\\n| GRPO                                  | 13 . 5 + 9 . 1                        | 9 . 8 + 7 . 7                         | 64 . 5 + 33 . 7                       | 57 . 1 + 2 . 6                        | 23 . 1 + 0 . 7                        | 26 . 9 + 2 . 9                        | 32.5                                  |\\n| FlowRL                                | 15 . 4 + 11 . 0                       | 10 . 8 + 8 . 7                        | 54 . 5 + 23 . 7                       | 67 . 0 + 12 . 5                       | 31 . 4 + 9 . 0                        | 34 . 6 + 10 . 6                       | 35.6                                  |\\n\\nTable 2 | Results on code benchmarks. We report metrics with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL achieves the strongest performance across all three benchmarks, demonstrating its effectiveness in code reasoning tasks.\\n\\n| Models                           | LiveCodeBench                    | LiveCodeBench                    | CodeForces                       | CodeForces      | HumanEval+     |\\n|----------------------------------|----------------------------------|----------------------------------|----------------------------------|-----------------|----------------|\\n|                                  | Avg@16                           | Pass@16                          | Rating                           | Percentile      | Avg@16         |\\n| DeepSeek-R1-Distill-Qwen-7B, Max | DeepSeek-R1-Distill-Qwen-7B, Max | DeepSeek-R1-Distill-Qwen-7B, Max | DeepSeek-R1-Distill-Qwen-7B, Max | Response        | Len=8K         |\\n| Backbone                         | 30.7                             | 49.5                             | 886.7                            | 19.4            | 80.9           |\\n| R++                              | 30 . 5 - 0 . 2                   | 52 . 7 + 3 . 2                   | 1208 . 0 + 321 . 3               | 56 . 8 + 37 . 4 | 76 . 6 - 4 . 3 |\\n| PPO                              | 35 . 1 + 4 . 4                   | 54 . 5 + 5 . 0                   | 1403 . 1 + 516 . 4               | 73 . 7 + 54 . 3 | 82 . 3 + 1 . 4 |\\n| GRPO                             | 32 . 8 + 2 . 1                   | 52 . 3 + 2 . 8                   | 1313 . 8 + 427 . 1               | 67 . 1 + 47 . 7 | 80 . 1 - 0 . 8 |\\n| FlowRL                           | 37 . 4 + 6 . 7                   | 56 . 3 + 6 . 8                   | 1549 . 5 + 662 . 8               | 83 . 3 + 63 . 9 | 83 . 3 + 2 . 4 |\\n\\nuse sampling parameters of temperature =0.6 and top_p =0.95 for all evaluations. The response length for evaluation is set to 8,192, consistent with the training configuration.\\n\\n## 6. Results\\n\\n## 6.1. Main Results\\n\\nOur experimental results, summarized in Table 1 and Table 2, demonstrate that FlowRL consistently outperforms all reward-maximization baselines across both math and code reasoning domains. Table 1 reports results on math reasoning benchmarks using both 7B and 32B base models, while Table 2 presents the corresponding results on code reasoning tasks. On math reasoning tasks, FlowRL achieves the highest average accuracy of 35.6% with the 7B model and 48.4% with the 32B model, surpassing PPO by 5.1% and GRPO by 10.1% on the 32B model. FlowRL shows strong improvements on challenging benchmarks like MATH-500 and Olympiad problems, demonstrating consistent gains\\n\\nTable 3 | Ablation study on FlowRL with Qwen2.5-7B as the base model. Avg@16 accuracy is reported across six math reasoning benchmarks. IS denotes importance sampling.\\n\\n| Method               |   AIME 2024 |   AIME 2025 |   AMC 2023 |   MATH-500 |   Minerva |   Olympiad |   Avg |\\n|----------------------|-------------|-------------|------------|------------|-----------|------------|-------|\\n| FlowRL               |       15.41 |       10.83 |      54.53 |      66.96 |     31.41 |      34.61 | 35.63 |\\n| w/o IS               |        6.25 |        7.91 |      41.4  |      56.97 |     22.19 |      25.52 | 26.71 |\\n| Zhang et al. [2025a] |       10.41 |        6.66 |      53.75 |      66.5  |     30.97 |      33.72 | 33.67 |\\n\\nacross diverse mathematical domains. On code generation tasks, FlowRL achieves compelling improvements with the highest Avg@16 score of 37.43% on LiveCodeBench, a Codeforces rating of 1549.47 with 83.3% percentile ranking, and 83.28% accuracy on HumanEval+, outperforming all baselines across the board. These consistent performance gains across both domains and model scales provide strong empirical evidence that FlowRL's flow-balanced optimization successfully enhances generalization. This improvement comes from promoting diverse solution exploration compared to previous reward-maximizing RL approaches.\\n\\n## 6.2. Ablation Studies\\n\\nWe conduct ablation studies on importance sampling and the 𝛽 hyperparameter. For importance sampling, we compared the performance with and without it, and implemented a combined loss approach proposed by Zhang et al. [2025a] that simultaneously optimizes both GFlowNets and PPO objectives. This combined loss focuses on optimizing diffusion models, and we adapt it to long CoT reasoning tasks for comparison. Table 3 demonstrates that importance sampling substantially improves FlowRL performance across all math reasoning benchmarks. Compared to Zhang et al. [2025a], using importance sampling as a trajectory-level ratio is more suitable than the combined loss of GFlowNets and PPO. The performance drop without importance sampling (from 35.63% to 26.71%) highlights the critical role of correcting for distribution mismatch between rollout generation and policy training. For the hyperparam-\\n\\nFigure 3 | Ablation study on the 𝛽 in FlowRL. 𝛽 = 15 (highlighted in blue) achieves the best performance.\\n\\neter 𝛽 , we conduct a series of parameter ablation studies, and Figure 3 shows that 𝛽 = 15 achieves optimal performance, with detailed results shown in Table 7.\\n\\n## 7. Analysis\\n\\n## 7.1. Diversity Analysis\\n\\nTo assess solution diversity , we follow the approach of Yu et al. [2025a] and employ GPT-4o-mini [OpenAI, 2024] to evaluate all responses generated by each method on AIME 24/25. The evaluation prompt is shown in Appendix C. As shown in Figure 4, FlowRL achieves higher diversity scores compared to baseline methods. This demonstrates that FlowRL improves sample diversity compared to baselines, which tend to exhibit repetitive solution patterns. This diversity evaluation reveals\\n\\nTable 4 | Case study comparing GRPO and FlowRL rollouts on an AIME problem. GRPO exhibits repetitive patterns (AM-GM × 3, identity loops × 2), while FlowRL follows a more diverse solution path.\\n\\nsignificant differences in exploration patterns across methods. This nearly doubling of diversity score compared to the strongest baseline (PPO) indicates that FlowRL generates qualitatively different solution approaches rather than minor variations of the same strategy. The diversity analysis provides empirical validation of our core hypothesis that flow-balanced optimization promotes mode coverage in complex reasoning tasks.\\n\\n## 7.2. Case Study\\n\\nTable 4 illustrates the behavioral differences between GRPO and FlowRL on a representative AIME problem. GRPO exhibits repetitive patterns, applying AMGM three times and getting stuck in identity loops, failing to solve the problem. FlowRL explores more diverse actions: it sets 𝑎 = 𝑏 , derives a cubic equation, finds the rational root, and reaches the correct answer. This shows that FlowRL successfully avoids the repetitive exploration patterns. The contrast reveals fundamental differences in exploration strategies: GRPO's reward-maximizing approach leads to exploitation of familiar techniques (AM-GM inequality) without exploring alternatives, eventually reaching contradictory conclusions like 𝑎 = 𝑏 = 𝑐 . In contrast, FlowRL's distribution-matching enables strategic decisions such as the symmetry assumption 𝑎 = 𝑏 , which\\n\\nFigure 4 | GPT-judged diversity scores on rollouts of AIME 24/25 problems. FlowRL generates more diverse solutions than R++, GRPO, and PPO.\\n\\ntransforms the problem into a tractable cubic equation 𝑎 3 -27 𝑎 + 46 = 0, allowing systematic solution through rational root testing and polynomial factorization.\\n\\n## 8. Conclusion\\n\\nIn this work, we introduce FlowRL, which transforms scalar rewards into normalized target distributions using a learnable partition function and minimizes the reverse KL divergence between the policy and target distribution. We demonstrate that this approach is theoretically equivalent to trajectory balance objectives from GFlowNets and implicitly maximizes both reward and entropy, thereby promoting diverse reasoning trajectories. To further address gradient explosion and sampling mismatch issues in long CoT reasoning, we incorporate importance sampling and length normalization. Through experiments on math and code reasoning benchmarks, FlowRL achieves consistent improvements across all tasks compared to GRPO and PPO. Our diversity analysis and case studies confirm that FlowRL generates more varied solution approaches while avoiding repetitive patterns.\\n\\n## Acknowledgments\\n\\nWe are grateful to Mingqian Feng and Yuetai Li for their valuable discussions and feedback, which helped improve the quality of this work.\\n\\n## References\\n\\n- Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of entropy on policy optimization. In International conference on machine learning , pages 151-160. PMLR, 2019.\\n- Brian R Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, and Bhavya Kailkhura. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable llm post-training. arXiv preprint arXiv:2503.18929 , 2025.\\n- Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS) , 2021.\\n- Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research , 24(210):1-55, 2023a. URL http: //jmlr.org/papers/v24/22-0364.html .\\n- Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. The Journal of Machine Learning Research , 24(1):10006-10060, 2023b.\\n- Chen-Hao Chao, Chien Feng, Wei-Fang Sun, Cheng-Kuang Lee, Simon See, and Chun-Yi Lee. Maximum entropy reinforcement learning via energy-based normalizing flow. arXiv preprint arXiv:2405.13629 , 2024.\\n- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\\n\\n- Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.\\n- Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758 , 2025.\\n- Miruna Cretu, Charles Harris, Ilia Igashov, Arne Schneuing, Marwin Segler, Bruno Correia, Julien Roy, Emmanuel Bengio, and Pietro Liò. Synflownet: Design of diverse and novel molecules with synthesis constraints. arXiv preprint arXiv:2405.01155 , 2024.\\n- Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617 , 2025.\\n- DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 .\\n- Tristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, and Yoshua Bengio. Discrete probabilistic inference as control in multi-path environments. arXiv preprint arXiv:2402.10309 , 2024.\\n- Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849 , 2025.\\n- Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in neural information processing systems , 32, 2019.\\n- Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257 , 2021.\\n- Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning , pages 10835-10866. PMLR, 2023.\\n- Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\\n- Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning , pages 1861-1870. Pmlr, 2018.\\n- Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 , 2024.\\n- Haoran He, Can Chang, Huazhe Xu, and Ling Pan. Looking backward: Retrospective backward synthesis for goal-conditioned GFlownets. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?id=fNMKqyvuZT .\\n- Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and R M Neal. The 'wake-sleep' algorithm for unsupervised neural networks. Science , 268 5214:1158-61, 1995.\\n- Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. arXiv preprint arXiv:2310.04363 , 2023.\\n\\n- Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/f orum?id=Ouj6p4ca60 .\\n- Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv. org/abs/2501 , 3262:32-33, 2025.\\n- Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena Simine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. International Conference on Machine Learning (ICML) , 2022.\\n- Moksh Jain, Tristan Deleu, Jason S. Hartford, Cheng-Hao Liu, Alex Hernández-García, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. ArXiv , abs/2302.00615, 2023a. URL https: //api.semanticscholar.org/CorpusID:256459319 .\\n- Moksh Jain, Sharath Chandra Raparthy, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective GFlowNets. International Conference on Machine Learning (ICML) , 2023b.\\n- Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.\\n- Koray Kavukcuoglu. Gemini 2.5: Our most intelligent AI model, 2025. URL https://blog.goo gle/technology/google-deepmind/gemini-model-thinking-updates-march-2025/ . Google Blog (The Keyword), Published Mar. 25, 2025.\\n- Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search gflownets. ArXiv , abs/2310.02710, 2023.\\n- Minsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan, Woochang Kim, Jinkyoo Park, Emmanuel Bengio, and Yoshua Bengio. Learning to scale logits for temperature-conditional gflownets, 2024.\\n- Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et al. Learning diverse attacks on large language models for robust red-teaming and safety tuning. arXiv preprint arXiv:2405.18540 , 2024.\\n- Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 3843-3857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/18abb eef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf .\\n- Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050 , 2023a.\\n- Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. In The Twelfth International Conference on Learning Representations , 2023b.\\n\\n- Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t .\\n- Dianbo Liu, Moksh Jain, Bonaventure F. P. Dossou, Qianli Shen, Salem Lahlou, Anirudh Goyal, Nikolay Malkin, Chris C. Emezue, Dinghuai Zhang, Nadhir Hassen, Xu Ji, Kenji Kawaguchi, and Yoshua Bengio. Gflowout: Dropout with generative flow networks. In International Conference on Machine Learning , 2022.\\n- Mingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507 , 2025a.\\n- Zhen Liu, Tim Z Xiao, , Weiyang Liu, Yoshua Bengio, and Dinghuai Zhang. Efficient diversity-preserving diffusion alignment via gradient-informed gflownets. In ICLR , 2025b.\\n- Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, Ion Stoica, and Tianjun Zhang. Deepcoder: A fully open-source 14b coder at o3-mini level, 2025. Notion Blog.\\n- Jiangyan Ma, Emmanuel Bengio, Yoshua Bengio, and Dinghuai Zhang. Baking symmetry into gflownets.\\n- MAA. American mathematics competitions - amc. https://maa.org/ , 2023.\\n- MAA. American invitational mathematics examination - aime. https://maa.org/ , 2025.\\n- Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning , pages 23467-23483. PMLR, 2023.\\n- Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. Advances in Neural Information Processing Systems , 35: 5955-5967, 2022.\\n- Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. International Conference on Learning Representations (ICLR) , 2023.\\n- David McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053 , 2025.\\n- Sobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropy gflownets with soft q-learning. In International Conference on Artificial Intelligence and Statistics , pages 2593-2601. PMLR, 2024.\\n- OpenAI. Gpt-4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-effic ient-intelligence/ , 2024. Accessed: 2024.\\n- Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544 , 2022.\\n- Ling Pan, Moksh Jain, Kanika Madan, and Yoshua Bengio. Pre-training and fine-tuning generative flow networks, 2023a.\\n\\n- Ling Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. Better training of GFlowNets with local credit and incomplete trajectories. International Conference on Machine Learning (ICML) , 2023b.\\n- Ling Pan, Dinghuai Zhang, Aaron Courville, Longbo Huang, and Yoshua Bengio. Generative augmented flow networks. International Conference on Learning Representations (ICLR) , 2023c.\\n- Ling Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic generative flow networks. Uncertainty in Artificial Intelligence (UAI) , 2023d.\\n- Seohong Park, Qiyang Li, and Sergey Levine. Flow q-learning. In Forty-second International Conference on Machine Learning , 2025. URL https://openreview.net/forum?id=KVf2SFL1pi .\\n- Guilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/open-r1/codeforces , 2025.\\n- Samuel Pfrommer, Yixiao Huang, and Somayeh Sojoudi. Reinforcement learning for flow-matching policies. arXiv preprint arXiv:2507.15073 , 2025.\\n- Abhinav Rastogi, Albert Q Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910 , 2025.\\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\\n- Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\\n- Max W. Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet training. ArXiv , abs/2305.07170, 2023. URL https://api.semanticscholar.org/CorpusID:258676487 .\\n- Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 , 2024.\\n- Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460-9471, 2022.\\n- Richard S Sutton, Andrew G Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience , 11(1):126-134, 1999a.\\n- Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. Müller, editors, Advances in Neural Information Processing Systems , volume 12. MIT Press, 1999b. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0b ed98e80ade0a5c43b0f-Paper.pdf .\\n- Qwen Team. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.g ithub.io/blog/qwen2.5/ .\\n\\n- Daniil Tiapkin, Nikita Morozov, Alexey Naumov, and Dmitry P Vetrov. Generative flow networks as entropy-regularized rl. In International Conference on Artificial Intelligence and Statistics , pages 4213-4221. PMLR, 2024.\\n- Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939 , 2025.\\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824-24837, 2022.\\n- Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Training llms for divergent reasoning with minimal examples. In Forty-second International Conference on Machine Learning , 2025a.\\n- Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 , 2025b.\\n- Taeyoung Yun, Dinghuai Zhang, Jinkyoo Park, and Ling Pan. Learning to sample effective and diverse prompts for text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 23625-23635, 2025.\\n- Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems , 35:15476-15488, 2022.\\n- David W. Zhang, Corrado Rainone, Markus F. Peschl, and Roberto Bondesan. Robust scheduling with gflownets. ArXiv , abs/2302.05446, 2023a. URL https://api.semanticscholar.org/Corp usID:256827133 .\\n- Dinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models with GFlowNets and beyond. arXiv preprint arXiv:2209.02606v2 , 2022a.\\n- Dinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML) , 2022b.\\n- Dinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C. Courville, Yoshua Bengio, and Ling Pan. Let the flows tell: Solving graph combinatorial optimization problems with gflownets. ArXiv , abs/2305.17010, 2023b.\\n- Dinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization, 2024a.\\n- Dinghuai Zhang, Ling Pan, Ricky T. Q. Chen, Aaron Courville, and Yoshua Bengio. Distributional gflownets with quantile flows, 2024b.\\n- Dinghuai Zhang, Yizhe Zhang, Jiatao Gu, Ruixiang ZHANG, Joshua M. Susskind, Navdeep Jaitly, and Shuangfei Zhai. Improving GFlownets for text-to-image diffusion alignment. Transactions on Machine Learning Research , 2025a. ISSN 2835-8856. URL https://openreview.net/forum ?id=XDbY3qhM42 .\\n\\n- Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. A survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827 , 2025b.\\n- Mingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, and Yoshua Bengio. Phylogfn: Phylogenetic inference with generative flow networks, 2024.\\n- Heiko Zimmermann, Fredrik Lindsten, J.-W. van de Meent, and Christian Andersson Naesseth. A variational perspective on generative flow networks. ArXiv , abs/2210.07992, 2022. URL https: //api.semanticscholar.org/CorpusID:252907672 .\\n\\n## A. Proof of Proposition 1\\n\\nWe begin by analyzing the gradient of the Kullback-Leibler (KL) divergence between the policy 𝜋𝜃 ( y | x ) and the target reward distribution exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) :\\n\\n<!-- formula-not-decoded -->\\n\\nNext, consider the trajectory balance objective used in GFlowNets learning [Bartoldson et al., 2025, Bengio et al., 2023b, Lee et al., 2024], defined as:\\n\\n<!-- formula-not-decoded -->\\n\\nTaking the gradient of this objective with respect to 𝜃 yields:\\n\\n<!-- formula-not-decoded -->\\n\\nThus, minimizing the KL divergence is equivalent (up to a constant) to minimizing the trajectory balance loss, confirming Proposition 1.\\n\\n## B. Theoretical Analysis\\n\\nWe conduct an interpretation of FlowRL that clarifies the role of each component in the objective.\\n\\nProposition 5. Minimizing the KL divergence in Eq. 5 is equivalent (in terms of gradients) to jointly maximizing reward and policy entropy:\\n\\n<!-- formula-not-decoded -->\\n\\nRemark 6 ( FlowRL beyond reward maximization ) . Proposition 5 reveals that FlowRL can be interpreted as jointly maximizing expected reward and policy entropy. This shift encourages the policy to explore a broader set of high-quality solutions, enabling more diverse and generalizable behaviors on reasoning tasks. Our interpretation also aligns with prior work that views GFlowNets training as a form of maximum entropy RL [Deleu et al., 2024, Mohammadpour et al., 2024].\\n\\nThe proof of Proposition 5 is provided as below.\\n\\nRecall from Eq. 3 and Eq. 5 that the FlowRL objective is sourced from the minimization of a KL divergence:\\n\\n<!-- formula-not-decoded -->\\n\\nRearranging the terms, we obtain:\\n\\n<!-- formula-not-decoded -->\\n\\nFinally, we express the FlowRL objective in its compact form:\\n\\n<!-- formula-not-decoded -->\\n\\n\\uf8f0\\n\\n\\uf8fb\\n\\nTherefore, minimizing the FlowRL objective can be interpreted as jointly maximizing reward and entropy, while also aligning the policy with a structured prior. The reward term drives task performance, while the normalization term 𝑍𝜙 ( x ) ensures consistency with a properly normalized target distribution. This encourages the policy 𝜋𝜃 to cover the entire reward-weighted distribution rather than collapsing to a few high-reward modes. The reference policy 𝜋 ref provides inductive bias that regularizes the policy toward desirable structures, and the entropy term H( 𝜋𝜃 ) encourages diversity in sampled solutions. Together, these components promote better generalization of FlowRL.\\n\\n## C. GFlowNets\\n\\nWe follow the notation of [He et al., 2025, Madan et al., 2023] to introduce the fundamentals of GFlowNets. Let X denote the compositional objects and 𝑅 be a reward function that assigns nonnegative values to each object 𝑥 ∈ X . GFlowNets aim to learn a sequential, constructive sampling policy 𝜋 that generates objects 𝑥 with probabilities proportional to their rewards, i.e., 𝜋 ( 𝑥 ) ∝ 𝑅 ( 𝑥 ) . This process can be represented as a directed acyclic graph (DAG) G = (S , A) , where the vertices 𝑠 ∈ S are referred to as states , and the directed edges ( 𝑢 → 𝑣 ) ∈ A are called actions . The generation of an object 𝑥 ∈ X corresponds to a complete trajectory 𝜏 = ( 𝑠 0 → · · · → 𝑠 𝑛 ) ∈ T within the DAG, beginning at the initial state 𝑠 0 and ending at a terminal state 𝑠 𝑛 ∈ X . The state flow 𝐹 ( 𝑠 ) is defined as a non-negative weight assigned to each state 𝑠 ∈ S . The forward policy 𝑃𝐹 ( 𝑠 ′ | 𝑠 ) specifies the transition probability to a child state 𝑠 ′ , while the backward policy 𝑃𝐵 ( 𝑠 | 𝑠 ′ ) specifies the transition probability to a parent state 𝑠 . To this end, detailed balance objective enforces local flow consistency across every edge ( 𝑠 → 𝑠 ′ ) ∈ A :\\n\\n<!-- formula-not-decoded -->\\n\\nTo achieve this flow consistency, GFlowNets employ training objectives at different levels of granularity, including detailed balance [Bengio et al., 2023b], trajectory balance [Malkin et al., 2022], and subtrajectory balance [Madan et al., 2023]. Leveraging their diversity-seeking behavior, GFlowNets have been successfully applied across a range of domains, including molecule generation [Cretu et al., 2024], diffusion fine-tuning [Liu et al., 2025b, Zhang et al., 2025a], and amortized reasoning [Hu et al., 2024, Yu et al., 2025a]. Among various training objective in GFlowNets, trajectory balance maintains flow consistency at the trajectory level, defined as:\\n\\n<!-- formula-not-decoded -->\\n\\nFurthermore, sub-trajectory balance achieves local balance on arbitrary subpaths 𝜏𝑖 : 𝑗 = { 𝑠 𝑖 → · · ·→ 𝑠 𝑗 } , offering a more stable and less biased learning signal. We build on trajectory balance to extend our KL-based objective through a gradient-equivalence formulation (Prop. 1), and further improve it to better support long CoT reasoning in RL.\\n\\nTable 5 | Math reasoning performance (Avg@64) at temperature = 0 . 6. Relative improvements are shown as subscripts, with positive gains in green and negative changes in red. FlowRL consistently outperforms all baselines and achieves the best average score under this low-temperature setting.\\n\\n| Models                | AIME 2024             | AIME 2025             | AMC 2023              | MATH-500              | Minerva               | Olympiad              | Avg                   |\\n|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\\n| Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model |\\n| Backbone              | 4.37                  | 2.08                  | 30.78                 | 54.48                 | 22.38                 | 24.02                 | 23.02                 |\\n| R++                   | 10 . 57 + 6 . 20      | 5 . 10 + 3 . 02       | 66 . 02 + 35 . 24     | 54 . 29 - 0 . 19      | 24 . 47 + 2 . 09      | 27 . 30 + 3 . 28      | 31.29                 |\\n| PPO                   | 9 . 95 + 5 . 58       | 7 . 34 + 5 . 26       | 63 . 63 + 32 . 85     | 57 . 72 + 3 . 24      | 26 . 22 + 3 . 84      | 27 . 35 + 3 . 33      | 32.03                 |\\n| GRPO                  | 14 . 01 + 9 . 64      | 10 . 73 + 8 . 65      | 64 . 10 + 33 . 32     | 57 . 41 + 2 . 93      | 23 . 17 + 0 . 79      | 27 . 11 + 3 . 09      | 32.76                 |\\n| FlowRL                | 14 . 32 + 9 . 95      | 10 . 05 + 7 . 97      | 55 . 08 + 24 . 30     | 66 . 78 + 12 . 30     | 31 . 52 + 9 . 14      | 34 . 60 + 10 . 58     | 35.39                 |\\n\\nTable 6 | Math reasoning performance (Avg@64) at temperature = 1 . 0. Relative improvements are shown as subscripts, with positive gains in green. FlowRL maintains robust performance under higher generation randomness and continues to outperform all baselines on average.\\n\\n| Models                | AIME 2024             | AIME 2025             | AMC 2023              | MATH-500              | Minerva               | Olympiad              | Avg                   |\\n|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\\n| Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model |\\n| Backbone              | 3.39                  | 1.51                  | 23.90                 | 45.18                 | 16.98                 | 18.27                 | 18.20                 |\\n| R++                   | 10 . 63 + 7 . 24      | 4 . 63 + 3 . 12       | 66 . 99 + 43 . 09     | 54 . 36 + 9 . 18      | 23 . 89 + 6 . 91      | 26 . 65 + 8 . 38      | 31.19                 |\\n| PPO                   | 10 . 52 + 7 . 13      | 6 . 51 + 5 . 00       | 63 . 04 + 39 . 14     | 57 . 46 + 12 . 28     | 25 . 91 + 8 . 93      | 27 . 16 + 8 . 89      | 31.77                 |\\n| GRPO                  | 12 . 50 + 9 . 11      | 10 . 10 + 8 . 59      | 64 . 72 + 40 . 82     | 57 . 15 + 11 . 97     | 23 . 28 + 6 . 30      | 26 . 90 + 8 . 63      | 32.44                 |\\n| FlowRL                | 14 . 22 + 10 . 83     | 9 . 58 + 8 . 07       | 52 . 92 + 29 . 02     | 66 . 20 + 21 . 02     | 30 . 32 + 13 . 34     | 34 . 47 + 16 . 20     | 34.62                 |\\n\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\n\\n| Models   |   AIME 2024 |   AIME 2025 |   AMC 2023 |   MATH-500 |   Minerva |   Olympiad |   Avg |\\n|----------|-------------|-------------|------------|------------|-----------|------------|-------|\\n| 𝛽 = 5    |       13.54 |       10    |      56.09 |      58.91 |     20.79 |      28.72 | 31.34 |\\n| 𝛽 = 10   |       14.79 |       10.2  |      59.53 |      64.3  |     25.27 |      32.39 | 34.41 |\\n| 𝛽 = 15   |       15.41 |       10.83 |      54.53 |      66.96 |     31.41 |      34.61 | 35.63 |\\n| 𝛽 = 30   |       15    |       10.83 |      50.62 |      69.02 |     30.03 |      35.03 | 35.09 |\\n\\nTable 7 | Ablation study on the effect of the 𝛽 parameter in FlowRL. We report Avg@16 accuracy across six math reasoning benchmarks for different values of 𝛽 .\\n\\n## Diversity Evaluation Prompt\\n\\nSystem: You are evaluating the DIVERSITY of solution approaches for a mathematics competition problem. Focus on detecting even SUBTLE differences in methodology that indicate different problemsolving strategies.\\n\\n## PROBLEM:\\n\\n{problem}\\n\\n## 16 SOLUTION ATTEMPTS:\\n\\n{formatted_responses}\\n\\n## EVALUATION CRITERIA - Rate diversity from 1 to 5:\\n\\n## Score 1 - Minimal Diversity:\\n\\n- Same mathematical setup, same variable choices, same solution path\\n- 14+ responses use essentially identical approaches\\n- Only trivial differences (arithmetic, notation, wording)\\n- Indicates very low exploration/diversity in the generation process\\n\\n## Score 2 - Low Diversity:\\n\\n- 1-2 alternative approaches appear but are rare\\n- 11-13 responses use the same main approach\\n- Minor variations within the dominant method (different substitutions, orderings)\\n- Some exploration but heavily biased toward one strategy\\n\\n## Score 3 - Moderate Diversity:\\n\\n- 2-3 distinct alternative approaches present\\n- 7-10 responses use the most common approach\\n- Noticeable variation in problem setup or mathematical techniques\\n- Balanced mix showing reasonable exploration\\n\\n## Score 4 - High Diversity:\\n\\n- 3-4 distinct solution strategies well-represented\\n- 4-6 responses use the most common approach\\n- Multiple mathematical techniques and problem framings\\n- Strong evidence of diverse exploration strategies\\n\\n## Score 5 - Maximum Diversity:\\n\\n- 4+ distinctly different solution strategies\\n- No single approach dominates ( ≤ 3 responses use same method)\\n- Wide variety of mathematical techniques and creative approaches\\n\\nIMPORTANT: Focusing on the DIVERSITY of the attempted approaches. Return ONLY a number from 1 to 5.\\n\\n- Excellent exploration and generation diversity\",\n",
       " 'sections': [{'title': 'Content',\n",
       "   'content': 'arXiv:2509.15207v1  [cs.LG]  18 Sep 2025\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\n2025-09-17'},\n",
       "  {'title': 'FlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       "   'content': 'Xuekai Zhu 1 , Daixuan Cheng 6 , Dinghuai Zhang 3 , Hengli Li 5 , Kaiyan Zhang 4 , Che Jiang 4 , Youbang Sun 4 , Ermo Hua 4 , Yuxin Zuo 4 , Xingtai Lv 4 , Qizheng Zhang 7 , Lin Chen 1 , Fanghao Shao 1 , Bo Xue 1 , Yunchong Song 1 , Zhenjie Yang 1 , Ganqu Cui 2 , Ning Ding 4 , 2 , Jianfeng Gao 3 , Xiaodong Liu 3 , Bowen Zhou 4 , 2 ‡ , Hongyuan Mei 8 ‡ , Zhouhan Lin 1 , 2 ‡\\n1 Shanghai Jiao Tong University 2 Shanghai AI Laboratory 3 Microsoft Research 4 Tsinghua University 5 Peking University 6 Renmin University of China 7 Stanford University 8 Toyota Technological Institute at Chicago\\n/envelope hongyuanmei@gmail.com /envelope xuekaizhu0@gmail.com /github FlowRL ‡ Corresponding Authors.\\nAbstract | We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods ( e.g. , PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10 . 0% over GRPO and 5 . 1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.\\nFigure 1 | Top : Comparison between distribution-matching and reward-maximizing approaches. FlowRL (left) learns to match the full reward distribution, maintaining diversity across multiple modes with low KL divergence. In contrast, reward-maximizing methods like GRPO (right) concentrate on a single high-reward peak, leading to mode collapse and higher KL divergence. Bottom : Performance comparison. FlowRL consistently outperforms GRPO across math and code domains.\\nDistribution-matching: FlowRL\\nKL = 0.11\\nKL = 8.68\\nReward-maximizing\\n∶\\nR++, PPO and GRPO\\nMath Average Score\\nCodeForces Rating\\nFlowRL: Matching Reward Distributions for LLM Reasoning'},\n",
       "  {'title': '1. Introduction',\n",
       "   'content': \"Reinforcement learning (RL) plays a crucial role in the post-training of large language models (LLMs) [Zhang et al., 2025b]. A series of powerful reasoning models [Guo et al., 2025, Kavukcuoglu, 2025, Rastogi et al., 2025] have employed large-scale reinforcement learning to achieve strong performance on highly challenging benchmarks [He et al., 2024]. The evolution of RL algorithms for LLM reasoning has progressed through several key stages: REINFORCE [Sutton et al., 1999a] provides a solid baseline that is easy to implement and efficient in simple settings; PPO [Schulman et al., 2017] improves upon REINFORCE with better stability and efficiency in complex settings; GRPO [Shao et al., 2024] simplifies PPO training by eliminating value functions and relying on group comparisons, though at the cost of requiring more rollouts per update. However, all these methods share a fundamental limitation in their reward-maximizing objective.\\nReward-maximizing RL methods tend to overfit to the dominant mode of the reward distribution [Gao et al., 2023, Pan et al., 2022, Skalse et al., 2022, Zelikman et al., 2022]. This often results in limited diversity among generated reasoning paths and reduces generalization to less frequent yet valid logical outcomes [Hu et al., 2023]. As illustrated in Figure 1, GRPO neglects other meaningful modes. These drawbacks become especially pronounced in complex long chain-of-thought (CoT; Wei et al., 2022) reasoning, where capturing a diverse distribution of plausible solutions is essential for effective generalization [Liu et al., 2025a]. Recent approaches adjust the clip ratio [Yu et al., 2025b], augment the advantage function with an entropy-based term [Cheng et al., 2025], or selectively promote high-entropy tokens [Wang et al., 2025], thereby dynamically adapting the training data distribution and implicitly increasing diversity during training. This raises a fundamental question: How can we promote diverse exploration to prevent convergence to dominant solution patterns in RL training?\\nIn this paper, we propose FlowRL , a policy optimization algorithm that aligns the policy model with the full reward distribution, encouraging mode coverage. FlowRL achieves more efficient exploration by fundamentally shifting from reward maximization to reward distribution matching, thereby addressing the inherent mode-collapse limitations of previous RL approaches. As illustrated in Figure 1, the core idea of FlowRL is to introduce a learnable partition function that normalizes scalar rewards into a target distribution, and to minimize the reverse KL divergence between the policy and this reward-induced distribution. We develop this KL objective based on the trajectory balance formulation from GFlowNets [Bengio et al., 2023b], providing a gradient equivalence proof that bridges generative modeling and policy optimization. To address the challenges of long CoT training, we introduce two key technical solutions: length normalization to tackle gradient explosion issues that occur with variable-length CoT reasoning, and importance sampling to correct for the distribution mismatch between generated rollouts and the current policy.\\nWe compare FlowRL with mainstream RL algorithms including REINFORCE++, PPO, and GRPO across math and code domains, using both base and distilled LLMs (7B, 32B). In math domain, FlowRL outperforms GRPO and PPO by 10 . 0% and 5 . 1%, respectively, demonstrating consistent improvements across six challenging math benchmarks. Furthermore, FlowRL surpasses both PPO and GRPO on three challenging coding benchmarks, highlighting its strong generalization capabilities in code reasoning tasks. To understand what drives these performance gains, we analyze the diversity of generated reasoning paths. This diversity analysis confirms that FlowRL generates substantially more diverse rollouts than baseline methods, validating our approach's effectiveness in exploring multiple solution strategies.\\nContributions. We summarize the key contributions of this work as follows:\\nWe propose FlowRL, a policy optimization algorithm that shifts from reward maximization to\\n2\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nreward distribution matching via flow balance, encouraging diverse reasoning path exploration while addressing the inherent mode-collapse limitations of existing RL methods.\\nFlowRL outperforms GRPO and PPO by 10.0% and 5.1% respectively across math benchmarks and demonstrates strong generalization on code reasoning tasks, with diversity analysis confirming substantially more diverse solution exploration.\\nWe introduce length normalization and importance sampling to enable effective training on variablelength CoT reasoning, addressing gradient explosion and sampling mismatch issues.\"},\n",
       "  {'title': '2. Preliminaries',\n",
       "   'content': 'Reinforcement Learning for Reasoning. Weformulate reasoning as a conditional generation problem, where the policy model receives a question x ∈ X and generates an answer y ∈ Y . The objective is to learn a policy 𝜋𝜃 ( y | x ) that produces high-quality answers under task-specific reward signals 𝑟 . To better illustrate the policy optimization procedure, we provide a detailed formulation of GRPO below. For each question x , GRPO samples a group of answers { y 1 , y 2 , . . . , y 𝐺 } from old policy 𝜋𝜃𝑜𝑙𝑑 and updates the model by maximizing the following objective:\\nwhere 𝜖 and 𝜆 are hyper-parameters. Here, 𝐴𝑖 denotes the advantage, computed by normalizing the group reward values { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } as 𝐴𝑖 = 𝑟 𝑖 -mean ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) std ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) . Compared to GRPO, REINFORCE applies the policy gradient directly, without advantage normalization, clipping, or KL regularization. PPO uses a critic model to estimate the advantage and employs importance sampling to stabilize policy updates.\\nGFlowNets. Generative Flow Networks [Bengio et al., 2023a] are a probabilistic framework for training stochastic policies to sample discrete, compositional objects ( e.g. , graphs, sequences) in proportion to a given reward. As shown in Figure 2, the core principle of GFlowNets is to balance the forward and backward probability flows at each state, inspired by flow matching [Bengio et al., 2021]. The initial flow is estimated by 𝑍𝜙 ( 𝑠 0 ) at the initial state 𝑠 0 . The output flow is equal to the outcome reward 𝑟 ( 𝑠 𝑛 ) conditioned at the final state 𝑠 𝑛 . Following Lee et al. [2024], we use a 3-layer MLP to parameterize 𝑍𝜙 . This flow-balancing mechanism facilitates the discovery of diverse,\\nFigure 2 | GFlowNets [Bengio et al., 2023a], a flow-balance perspective on reinforcement learning. The initial flow 𝑍𝜙 ( 𝑠 0 ) injects probability mass into the environment, which is transported through intermediate states by the policy 𝜋𝜃 and accumulated at terminal states in proportion to the scalar rewards.\\n𝑠\\n!\\n𝑠\\n\"\\n𝑠\\n#\\n𝑠\\n$\\n𝑠\\n%\\n𝑠\\n&\\n𝑠\\n\\'\\n𝑠\\n(\\n𝑠\\n)\\n𝑠\\n*\\n𝑠\\n\"!\\n𝑠\\n+\\n𝑠\\n+\\n𝑆\\n!\\n𝑠\\n+\\n𝑠\\n+\\n𝑠\\n+\\nIn Flow\\nZ\\n\"\\n𝑠\\n#\\nOut Flow r\\n(𝜏)\\nhigh-reward solutions by ensuring proper exploration of the solution space. See Appendix C for detailed GFlowNets background.\\n3\\nFlowRL: Matching Reward Distributions for LLM Reasoning'},\n",
       "  {'title': '3. Methodology',\n",
       "   'content': 'In this section, we first formulate distribution matching in reinforcement learning through reverse KL divergence and establish its connection to trajectory balance from GFlowNets. To address the challenges of gradient explosion and sampling mismatch encountered during long CoT training, we further incorporate length normalization and importance sampling. Using this enhanced framework, we derive a flow-balanced objective, termed FlowRL .'},\n",
       "  {'title': '3.1. From Reward Maximization to Distribution Matching',\n",
       "   'content': 'As illustrated in Figure 1, recent powerful large reasoning models typically employ reward-maximizing RL algorithms, such as PPO or GRPO. However, these methods tend to optimize toward the dominant reward mode, frequently resulting in mode collapse and the neglect of other plausible, high-quality reasoning paths. To address this fundamental limitation, we propose optimizing the policy by aligning its output distribution to a target reward distribution. A simple yet effective way to achieve this is to minimize the reverse KL divergence 1 between the policy and this target. However, in long CoT reasoning tasks, the available supervision in RL is a scalar reward, rather than a full distribution. Moreover, enumerating or sampling all valid trajectories to recover the true reward distribution is computationally intractable.\\nInspired by energy-based modeling [Du and Mordatch, 2019, Hinton et al., 1995], we introduce a learnable partition function 𝑍𝜙 ( x ) to normalize scalar rewards into a valid target distribution. This allows us to minimize the reverse KL divergence between the policy and the reward-weighted distribution, formalized as:\\nwhere 𝑟 ( x , y ) is the reward function, 𝛽 is a hyperparameter, 𝑍𝜙 ( x ) is the learned partition function, and the resulting target distribution is defined as ˜ 𝜋 ( y | x ) = exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) . This objective encourages the policy to sample diverse, high-reward trajectories in proportion to their rewards, rather than collapsing to dominant modes as in standard reward maximization.\\nWhile the KL-based formulation provides a principled target distribution, we derive a more practical, RL-style objective that facilitates efficient policy optimization.\\nProposition 1. In terms of expected gradients, minimizing the KL objective in Eq. 2 is equivalent to minimizing the trajectory balance loss used in GFlowNet [Bartoldson et al., 2025, Lee et al., 2024, Malkin et al., 2022, 2023]:\\nRemark 2 ( Trajectory balance as a practical surrogate for KL minimization ) . Given the equivalence established in Proposition 1, the KL-based distribution matching objective can be reformulated as the trajectory balance loss. This reformulation provides a practical optimization approach by using a stable squared loss form rather than direct KL optimization, and by treating 𝑍𝜙 ( x ) as a learnable parameter rather than requiring explicit computation of the intractable partition function. The trajectory balance objective thus serves as a tractable surrogate for reward-guided KL minimization that can be directly integrated into existing RL frameworks.\\n1 We use reverse KL since we can only sample from the policy model, not the target reward distribution.\\n4\\nFlowRL: Matching Reward Distributions for LLM Reasoning'},\n",
       "  {'title': '3.2. FlowRL',\n",
       "   'content': 'As established in Proposition 1, the target reward distribution can be approximated by optimizing the trajectory balance objective. However, applying this objective directly to long CoT reasoning introduces two key challenges:\\nProblem I: Exploding gradients from long trajectories. Trajectory balance is a sequence-level objective, and applying it to long CoT reasoning with up to 8K tokens leads to exploding gradients and unstable updates. This issue is not observed in prior GFlowNets works, which typically operate on short trajectories in small discrete spaces. Specifically, the log-probability term log 𝜋𝜃 ( y | x ) decomposes into a token-wise sum, ˝ 𝑡 log 𝜋𝜃 ( y 𝑡 | y <𝑡 , x ) , causing the gradient norm to potentially scale with sequence length.\\nProblem II: Sampling mismatch. Mainstream RL algorithms such as PPO and GRPO commonly perform micro-batch updates and reuse trajectories collected from an old policy 𝜋𝜃 old , enabling data-efficient training. In contrast, the KL-based trajectory balance objective assumes fully onpolicy sampling, where responses are drawn from the current policy. This mismatch poses practical limitations when integrating trajectory balance into existing RL pipelines.\\nThese limitations motivate our reformulation that retains the benefits of distribution matching while addressing key practical challenges. To enable this reformulation, we first redefine the reward function following established practices in GFlowNets literature [Bartoldson et al., 2025, Lee et al., 2024, Yu et al., 2025a] by incorporating a reference model as a prior constraint on the reward distribution. Specifically , we modify the original exp ( 𝛽𝑟 ( x , y )) to include the reference model:\\nwhere 𝑟 ( x , y ) denotes the outcome reward commonly used in reinforcement learning and 𝜋 ref is the initial pre-trained model. We follow Guo et al. [2025] to use outcome-based reward signals, and apply group normalization to 𝑟 ( x , y ) as ˆ 𝑟 𝑖 = ( 𝑟 𝑖 -mean ( r ))/ std ( r ) , where r = { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } denotes the set of rewards within a sampled group. By substituting the redefined reward formulation Eq. 4 into Eq. 3, we derive the following objective 2 :\\nRemark 3 ( Reward shaping via length normalization ) . Trajectory balance treats both the initial flow and the outcome reward as sequence-level quantities. In contrast, standard policy optimization methods such as PPO or GRPO assign rewards at the token level and compute gradients at each step. However, for trajectories of varying lengths ( e.g. , CoT responses), this mismatch can cause the log-probability term log 𝜋𝜃 ( y | x ) = ˝ | y | 𝑡 = 1 log 𝜋𝜃 ( 𝑦 𝑡 | 𝑦 <𝑡 , x ) to scale with sequence length. To address this, we apply a form of reward shaping by normalizing log-probabilities with respect to sequence length. Specifically, we rescale the term as 1 | y | log 𝜋𝜃 ( y | x ) , balancing the contributions of long and short sequences and stabilizing the learning signal.\\nRemark 4 ( Importance sampling for data-efficient training ) . To mitigate sampling mismatch, we employ importance sampling inspired by PPO to stabilize policy updates with off-policy data. We re-weight stale trajectories using the importance ratio 𝑤 = 𝜋𝜃 ( y | x )/ 𝜋 old ( y | x ) , which serves as a coefficient in the surrogate loss. Since our objective focuses on optimizing trajectory balance rather than expected return, we detach the gradient from the current policy to prevent excessive policy drift: 𝑤 = detach [ 𝜋𝜃 ( y | x )]/ 𝜋 old ( y | x ) . For additional stability, we incorporate PPO-style clipping to detach\\n.\\n2 The substitution replaces 𝛽𝑟 ( x , y ) in trajectory balance objective Eq. 3 with 𝛽𝑟 ( x , y ) + log 𝜋 ref ( y | x ) to incorporate the reference model constraint.\\n5\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nIncorporating these improvements into Eq. 5, we arrive at the following FlowRL objective:\\nFlowRL\\nwhere the clipped importance weight 𝑤 and normalized reward ˆ 𝑟 ( x , y ) are defined as:\\nWe use this objective to update the policy parameters 𝜃 during training, and refer to this strategy as FlowRL . Implementation details and theoretical analysis are provided in § 5 and § B, respectively.'},\n",
       "  {'title': '4.1. Reinforcement Learning for Reasoning',\n",
       "   'content': \"Reinforcement learning has emerged as a powerful approach for large language models post-training on reasoning tasks [Guo et al., 2025, Lightman et al., 2023b, Schulman et al., 2017, Shao et al., 2024, Sutton et al., 1999b]. Most approaches employ reward-maximizing RL to optimize expected cumulative returns. Entropy regularization [Ahmed et al., 2019, Cheng et al., 2025, Haarnoja et al., 2018] is a classical technique for mitigating mode collapse by promoting diversity in the policy's output distribution, and has also been shown to enhance reasoning capabilities in various settings [Chao et al., 2024, Eysenbach and Levine, 2021]. However, for long CoT reasoning, the extended trajectory length (e.g., 8k-16k tokens) makes it difficult for the regularization signal to effectively influence reward-maximizing learning. Recent work [Cheng et al., 2025, Cui et al., 2025, Dong et al., 2025, Wang et al., 2025] has discovered that training with more diverse or high-entropy training data can further enhance training effectiveness. Compared to traditional entropy regularization, the above methods explicitly increase the proportion of low-probability (i.e., high-entropy) tokens in the training data. In our work, we address the mode-collapse problem by fundamentally shifting from reward maximization to reward distribution matching in our RL formulation.\"},\n",
       "  {'title': '4.2. GFlowNets',\n",
       "   'content': 'GFlowNets [Bengio et al., 2023a] represent a class of diversity-driven algorithms designed to balance probability flows across states. They have rich connections to probabilistic modeling methods [Ma et al., Malkin et al., 2023, Zhang et al., 2022a,b, 2024a, Zimmermann et al., 2022], and control methods [Pan et al., 2023b,c,d, Tiapkin et al., 2024, Zhang et al., 2024b]. This advantage has enabled GFlowNets to achieve successful applications in multiple downstream tasks, such as molecular drug discovery [Jain et al., 2022, 2023a,b, Kim et al., 2023, 2024, Liu et al., 2022, Pan et al., 2023a, Shen et al., 2023], phylogenetic inference [Zhou et al., 2024], and combinatorial optimization [Zhang et al., 2023a,b]. For generative AI, GFlowNets provide a powerful approach to align pretrained models in scenarios such as image generation [Yun et al., 2025, Zhang et al., 2025a] and language model fine-tuning [Hu et al., 2024, Lee et al., 2024, Yu et al., 2025a]. Another line of work primarily focuses on the theoretical aspects of GFlowNets. Recent theoretical studies have interpreted GFlowNets as solving a maximum entropy reinforcement learning problem within a modified Markov Decision Process (MDP) [Deleu et al., 2024, Mohammadpour et al., 2024, Tiapkin et al., 2024]. These theoretical contributions have\\n6\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\ninspired us to enhance reinforcement learning from a more foundational standpoint using GFlowNets principles. A comprehensive overview of GFlowNets theory can be found in Appendix C.'},\n",
       "  {'title': '4.3. Flow-Matching Policies',\n",
       "   'content': 'Flow matching simplifies diffusion-based approaches by learning vector fields that transport samples from prior to target distributions [Lipman et al., 2023]. Recent work has explored flow matching for policy optimization. McAllister et al. [2025] reformulates policy optimization using advantageweighted ratios from conditional flow matching loss, enabling flow-based policy training without expensive likelihood computations. Pfrommer et al. [2025] explored reward-weighted flow matching for improving policies beyond demonstration performance. Park et al. [2025] uses a separate one-step policy to avoid unstable backpropagation through time when training flow policies with RL. Zhang et al. [2025a] proposed a combined loss function integrating PPO and GFlowNets to optimize diffusion model alignment. However, these approaches focus on continuous control, image generation, or vision-action models, rather than addressing mode-collapse limitations in reward-maximizing RL. Inspired by flow matching principles, our work improves upon RL training to enhance training stability while promoting diverse solution exploration.'},\n",
       "  {'title': '5. Experiment Settings',\n",
       "   'content': 'Backbone Models. There are two learnable modules in Eq. 6: the policy model 𝜋𝜃 and the partition function 𝑍𝜙 . For the policy model 𝜋𝜃 , we use Qwen-2.5-7B/32B [Team, 2024] for math tasks and DeepSeek-R1-Distill-Qwen-7B [DeepSeek-AI, 2025] for code tasks, respectively. For partition function 𝑍𝜙 , following Lee et al. [2024], we use a randomly initialized 3-layer MLP with hidden dimensions matching those of the base model. The reference model 𝜋 ref is the corresponding fixed pretrained model. All training scripts are based on the veRL [Sheng et al., 2024]. For the reward function, following Lee et al. [2024], we set the hyperparameter 𝛽 = 15.\\nBaselines. We compare our method against three representative reward-maximization RL baselines: REINFORCE++ (R++; Hu et al., 2025, Sutton et al., 1999b), PPO [Schulman et al., 2017], and GRPO [Shao et al., 2024]. All baselines follow the official veRL recipes, with consistent training configurations. For fair comparison, all methods use the same learning rate, batch size, and training steps, and are evaluated at convergence using identical step counts.\\nTraining Configuration. We experiment on both math and code domains. For the math domain, we use the training set collected from DAPO [Yu et al., 2025b]. For the code domain, we follow the setup of DeepCoder [Luo et al., 2025], using their training set. For 7B model training, we use a single node equipped with 8 NVIDIA H800 GPUs (80GB memory each). For 32B model training, we scale to 4 nodes with 32 GPUs to accommodate the larger memory requirements. All experiments use max_prompt_length = 2048 and max_response_length = 8192 across both model sizes. We use a batch size of 512 for math reasoning tasks and 64 for code reasoning tasks. We set the learning rate to 1e-6 and enable dynamic batch sizing in veRL for efficient training. For GRPO and FlowRL, we configure rollout_n = 8, meaning each prompt generates 8 response rollouts as the group size.\\nEvaluation Configuration. For the math domain, we evaluate on six challenging benchmarks: AIME 2024/2025 [MAA, 2025], AMC 2023 [MAA, 2023], MATH-500 [Lightman et al., 2023a], Minerva [Lewkowycz et al., 2022], and Olympiad [He et al., 2024]. For the code domain, we evaluate on LiveCodeBench [Jain et al., 2024], CodeForces [Penedo et al., 2025], and HumanEval+ [Chen et al., 2021]. For all evaluation datasets, we perform 16 rollouts and report the average accuracy, denoted as Avg@16. We further report rating and percentile for Codeforces. During generation, we\\n7\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 1 | Results on math benchmarks. We report Avg@16 accuracy with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL outperforms all baselines across both 7B and 32B model scales.\\nTable 2 | Results on code benchmarks. We report metrics with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL achieves the strongest performance across all three benchmarks, demonstrating its effectiveness in code reasoning tasks.\\nuse sampling parameters of temperature =0.6 and top_p =0.95 for all evaluations. The response length for evaluation is set to 8,192, consistent with the training configuration.'},\n",
       "  {'title': '6.1. Main Results',\n",
       "   'content': \"Our experimental results, summarized in Table 1 and Table 2, demonstrate that FlowRL consistently outperforms all reward-maximization baselines across both math and code reasoning domains. Table 1 reports results on math reasoning benchmarks using both 7B and 32B base models, while Table 2 presents the corresponding results on code reasoning tasks. On math reasoning tasks, FlowRL achieves the highest average accuracy of 35.6% with the 7B model and 48.4% with the 32B model, surpassing PPO by 5.1% and GRPO by 10.1% on the 32B model. FlowRL shows strong improvements on challenging benchmarks like MATH-500 and Olympiad problems, demonstrating consistent gains\\n8\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 3 | Ablation study on FlowRL with Qwen2.5-7B as the base model. Avg@16 accuracy is reported across six math reasoning benchmarks. IS denotes importance sampling.\\nacross diverse mathematical domains. On code generation tasks, FlowRL achieves compelling improvements with the highest Avg@16 score of 37.43% on LiveCodeBench, a Codeforces rating of 1549.47 with 83.3% percentile ranking, and 83.28% accuracy on HumanEval+, outperforming all baselines across the board. These consistent performance gains across both domains and model scales provide strong empirical evidence that FlowRL's flow-balanced optimization successfully enhances generalization. This improvement comes from promoting diverse solution exploration compared to previous reward-maximizing RL approaches.\"},\n",
       "  {'title': '6.2. Ablation Studies',\n",
       "   'content': 'We conduct ablation studies on importance sampling and the 𝛽 hyperparameter. For importance sampling, we compared the performance with and without it, and implemented a combined loss approach proposed by Zhang et al. [2025a] that simultaneously optimizes both GFlowNets and PPO objectives. This combined loss focuses on optimizing diffusion models, and we adapt it to long CoT reasoning tasks for comparison. Table 3 demonstrates that importance sampling substantially improves FlowRL performance across all math reasoning benchmarks. Compared to Zhang et al. [2025a], using importance sampling as a trajectory-level ratio is more suitable than the combined loss of GFlowNets and PPO. The performance drop without importance sampling (from 35.63% to 26.71%) highlights the critical role of correcting for distribution mismatch between rollout generation and policy training. For the hyperparam-\\nFigure 3 | Ablation study on the 𝛽 in FlowRL. 𝛽 = 15 (highlighted in blue) achieves the best performance.\\n=5\\n=10\\n=15\\n=30\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\nAverage Score (%)\\n31.34\\n34.41\\n35.63\\n35.09\\neter 𝛽 , we conduct a series of parameter ablation studies, and Figure 3 shows that 𝛽 = 15 achieves optimal performance, with detailed results shown in Table 7.'},\n",
       "  {'title': '7.1. Diversity Analysis',\n",
       "   'content': 'To assess solution diversity , we follow the approach of Yu et al. [2025a] and employ GPT-4o-mini [OpenAI, 2024] to evaluate all responses generated by each method on AIME 24/25. The evaluation prompt is shown in Appendix C. As shown in Figure 4, FlowRL achieves higher diversity scores compared to baseline methods. This demonstrates that FlowRL improves sample diversity compared to baselines, which tend to exhibit repetitive solution patterns. This diversity evaluation reveals\\n9\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 4 | Case study comparing GRPO and FlowRL rollouts on an AIME problem. GRPO exhibits repetitive patterns (AM-GM × 3, identity loops × 2), while FlowRL follows a more diverse solution path.'},\n",
       "  {'title': 'Question',\n",
       "   'content': \"Let B be the set of rectangular boxes with surface area 54 and volume 23. Let 𝑟 be the radius of the smallest sphere that can contain each box in B . If 𝑟 2 = 𝑝 𝑞 with gcd ( 𝑝, 𝑞 ) = 1, find 𝑝 + 𝑞 .\\nGRPO\\n'. . .\\ndenote\\n𝑎, 𝑏, 𝑐\\n. . .\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n=\\n54,\\n𝑎𝑏𝑐\\n=\\n23 ...\\n𝑑\\n=\\n√\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2 ,\\n𝑟\\n=\\n𝑑\\n/\\n2\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n=\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n+\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n. . . AM-GM\\n×\\n3 :\\nAM-GM (1)\\n. . .\\nAM-GM (2)\\n. . .\\nAM-GM (3)\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n3\\nidentity loop\\n×\\n2 :\\nloop (1)\\n. . .\\nloop (2)\\n. . .\\n𝑎\\n=\\n𝑏\\n=\\n𝑐\\n(contradiction) .. .\\nback to\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n. . .\\nno factorization . . . '\\nFlowRL\\n'. . .\\nlet\\n𝑎, 𝑏, 𝑐\\nwith\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n=\\n54,\\n𝑎𝑏𝑐\\n=\\n23\\n. . .\\n𝑑\\n=\\n√\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2 ,\\n𝑟\\n=\\n𝑑\\n/\\n2\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n⇒\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n=\\n𝑠\\n2\\n-\\n54\\n. . .\\n𝑎\\n=\\n𝑏\\n. . .\\n𝑎\\n3\\n-\\n27\\n𝑎\\n+\\n46\\n=\\n0\\n. . .\\nrational root\\n𝑎\\n=\\n2 ...\\nfactor\\n(\\n𝑎\\n-\\n2\\n)(\\n𝑎\\n2\\n+\\n2\\n𝑎\\n-\\n23\\n)\\n. . .\\nbranch\\n𝑎\\n=\\n-\\n1\\n+\\n2\\n√\\n6 ...\\nback-sub\\n𝑐\\n=\\n23\\n/\\n𝑎\\n2\\n. . .\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n=\\n657\\n16\\n. . .\\n𝑟\\n2\\n=\\n657\\n64\\n. . .\\nAnswer 721 ...'\\nsignificant differences in exploration patterns across methods. This nearly doubling of diversity score compared to the strongest baseline (PPO) indicates that FlowRL generates qualitatively different solution approaches rather than minor variations of the same strategy. The diversity analysis provides empirical validation of our core hypothesis that flow-balanced optimization promotes mode coverage in complex reasoning tasks.\"},\n",
       "  {'title': '7.2. Case Study',\n",
       "   'content': \"Table 4 illustrates the behavioral differences between GRPO and FlowRL on a representative AIME problem. GRPO exhibits repetitive patterns, applying AMGM three times and getting stuck in identity loops, failing to solve the problem. FlowRL explores more diverse actions: it sets 𝑎 = 𝑏 , derives a cubic equation, finds the rational root, and reaches the correct answer. This shows that FlowRL successfully avoids the repetitive exploration patterns. The contrast reveals fundamental differences in exploration strategies: GRPO's reward-maximizing approach leads to exploitation of familiar techniques (AM-GM inequality) without exploring alternatives, eventually reaching contradictory conclusions like 𝑎 = 𝑏 = 𝑐 . In contrast, FlowRL's distribution-matching enables strategic decisions such as the symmetry assumption 𝑎 = 𝑏 , which\\nFigure 4 | GPT-judged diversity scores on rollouts of AIME 24/25 problems. FlowRL generates more diverse solutions than R++, GRPO, and PPO.\\nR++\\nGRPO\\nPPO\\nFlowRL\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nDiversity Score\\n1.11\\n1.23\\n1.31\\n2.28\\ntransforms the problem into a tractable cubic equation 𝑎 3 -27 𝑎 + 46 = 0, allowing systematic solution through rational root testing and polynomial factorization.\\n10\\nFlowRL: Matching Reward Distributions for LLM Reasoning\"},\n",
       "  {'title': '8. Conclusion',\n",
       "   'content': 'In this work, we introduce FlowRL, which transforms scalar rewards into normalized target distributions using a learnable partition function and minimizes the reverse KL divergence between the policy and target distribution. We demonstrate that this approach is theoretically equivalent to trajectory balance objectives from GFlowNets and implicitly maximizes both reward and entropy, thereby promoting diverse reasoning trajectories. To further address gradient explosion and sampling mismatch issues in long CoT reasoning, we incorporate importance sampling and length normalization. Through experiments on math and code reasoning benchmarks, FlowRL achieves consistent improvements across all tasks compared to GRPO and PPO. Our diversity analysis and case studies confirm that FlowRL generates more varied solution approaches while avoiding repetitive patterns.'},\n",
       "  {'title': 'Acknowledgments',\n",
       "   'content': 'We are grateful to Mingqian Feng and Yuetai Li for their valuable discussions and feedback, which helped improve the quality of this work.'},\n",
       "  {'title': 'References',\n",
       "   'content': \"Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of entropy on policy optimization. In International conference on machine learning , pages 151-160. PMLR, 2019.\\nBrian R Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, and Bhavya Kailkhura. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable llm post-training. arXiv preprint arXiv:2503.18929 , 2025.\\nEmmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS) , 2021.\\nYoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research , 24(210):1-55, 2023a. URL http: //jmlr.org/papers/v24/22-0364.html .\\nYoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. The Journal of Machine Learning Research , 24(1):10006-10060, 2023b.\\nChen-Hao Chao, Chien Feng, Wei-Fang Sun, Cheng-Kuang Lee, Simon See, and Chun-Yi Lee. Maximum entropy reinforcement learning via energy-based normalizing flow. arXiv preprint arXiv:2405.13629 , 2024.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\\n11\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.\\nDaixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758 , 2025.\\nMiruna Cretu, Charles Harris, Ilia Igashov, Arne Schneuing, Marwin Segler, Bruno Correia, Julien Roy, Emmanuel Bengio, and Pietro Liò. Synflownet: Design of diverse and novel molecules with synthesis constraints. arXiv preprint arXiv:2405.01155 , 2024.\\nGanqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617 , 2025.\\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 .\\nTristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, and Yoshua Bengio. Discrete probabilistic inference as control in multi-path environments. arXiv preprint arXiv:2402.10309 , 2024.\\nGuanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849 , 2025.\\nYilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in neural information processing systems , 32, 2019.\\nBenjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257 , 2021.\\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning , pages 10835-10866. PMLR, 2023.\\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning , pages 1861-1870. Pmlr, 2018.\\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 , 2024.\\nHaoran He, Can Chang, Huazhe Xu, and Ling Pan. Looking backward: Retrospective backward synthesis for goal-conditioned GFlownets. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?id=fNMKqyvuZT .\\nGeoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and R M Neal. The 'wake-sleep' algorithm for unsupervised neural networks. Science , 268 5214:1158-61, 1995.\\nEdward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. arXiv preprint arXiv:2310.04363 , 2023.\\n12\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nEdward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/f orum?id=Ouj6p4ca60 .\\nJian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv. org/abs/2501 , 3262:32-33, 2025.\\nMoksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena Simine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. International Conference on Machine Learning (ICML) , 2022.\\nMoksh Jain, Tristan Deleu, Jason S. Hartford, Cheng-Hao Liu, Alex Hernández-García, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. ArXiv , abs/2302.00615, 2023a. URL https: //api.semanticscholar.org/CorpusID:256459319 .\\nMoksh Jain, Sharath Chandra Raparthy, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective GFlowNets. International Conference on Machine Learning (ICML) , 2023b.\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.\\nKoray Kavukcuoglu. Gemini 2.5: Our most intelligent AI model, 2025. URL https://blog.goo gle/technology/google-deepmind/gemini-model-thinking-updates-march-2025/ . Google Blog (The Keyword), Published Mar. 25, 2025.\\nMinsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search gflownets. ArXiv , abs/2310.02710, 2023.\\nMinsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan, Woochang Kim, Jinkyoo Park, Emmanuel Bengio, and Yoshua Bengio. Learning to scale logits for temperature-conditional gflownets, 2024.\\nSeanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et al. Learning diverse attacks on large language models for robust red-teaming and safety tuning. arXiv preprint arXiv:2405.18540 , 2024.\\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 3843-3857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/18abb eef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf .\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050 , 2023a.\\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. In The Twelfth International Conference on Learning Representations , 2023b.\\n13\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nYaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t .\\nDianbo Liu, Moksh Jain, Bonaventure F. P. Dossou, Qianli Shen, Salem Lahlou, Anirudh Goyal, Nikolay Malkin, Chris C. Emezue, Dinghuai Zhang, Nadhir Hassen, Xu Ji, Kenji Kawaguchi, and Yoshua Bengio. Gflowout: Dropout with generative flow networks. In International Conference on Machine Learning , 2022.\\nMingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507 , 2025a.\\nZhen Liu, Tim Z Xiao, , Weiyang Liu, Yoshua Bengio, and Dinghuai Zhang. Efficient diversity-preserving diffusion alignment via gradient-informed gflownets. In ICLR , 2025b.\\nMichael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, Ion Stoica, and Tianjun Zhang. Deepcoder: A fully open-source 14b coder at o3-mini level, 2025. Notion Blog.\\nJiangyan Ma, Emmanuel Bengio, Yoshua Bengio, and Dinghuai Zhang. Baking symmetry into gflownets.\\nMAA. American mathematics competitions - amc. https://maa.org/ , 2023.\\nMAA. American invitational mathematics examination - aime. https://maa.org/ , 2025.\\nKanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning , pages 23467-23483. PMLR, 2023.\\nNikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. Advances in Neural Information Processing Systems , 35: 5955-5967, 2022.\\nNikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. International Conference on Learning Representations (ICLR) , 2023.\\nDavid McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053 , 2025.\\nSobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropy gflownets with soft q-learning. In International Conference on Artificial Intelligence and Statistics , pages 2593-2601. PMLR, 2024.\\nOpenAI. Gpt-4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-effic ient-intelligence/ , 2024. Accessed: 2024.\\nAlexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544 , 2022.\\nLing Pan, Moksh Jain, Kanika Madan, and Yoshua Bengio. Pre-training and fine-tuning generative flow networks, 2023a.\\n14\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nLing Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. Better training of GFlowNets with local credit and incomplete trajectories. International Conference on Machine Learning (ICML) , 2023b.\\nLing Pan, Dinghuai Zhang, Aaron Courville, Longbo Huang, and Yoshua Bengio. Generative augmented flow networks. International Conference on Learning Representations (ICLR) , 2023c.\\nLing Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic generative flow networks. Uncertainty in Artificial Intelligence (UAI) , 2023d.\\nSeohong Park, Qiyang Li, and Sergey Levine. Flow q-learning. In Forty-second International Conference on Machine Learning , 2025. URL https://openreview.net/forum?id=KVf2SFL1pi .\\nGuilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/open-r1/codeforces , 2025.\\nSamuel Pfrommer, Yixiao Huang, and Somayeh Sojoudi. Reinforcement learning for flow-matching policies. arXiv preprint arXiv:2507.15073 , 2025.\\nAbhinav Rastogi, Albert Q Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910 , 2025.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\\nMax W. Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet training. ArXiv , abs/2305.07170, 2023. URL https://api.semanticscholar.org/CorpusID:258676487 .\\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 , 2024.\\nJoar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460-9471, 2022.\\nRichard S Sutton, Andrew G Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience , 11(1):126-134, 1999a.\\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. Müller, editors, Advances in Neural Information Processing Systems , volume 12. MIT Press, 1999b. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0b ed98e80ade0a5c43b0f-Paper.pdf .\\nQwen Team. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.g ithub.io/blog/qwen2.5/ .\\n15\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nDaniil Tiapkin, Nikita Morozov, Alexey Naumov, and Dmitry P Vetrov. Generative flow networks as entropy-regularized rl. In International Conference on Artificial Intelligence and Statistics , pages 4213-4221. PMLR, 2024.\\nShenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939 , 2025.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824-24837, 2022.\\nFangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Training llms for divergent reasoning with minimal examples. In Forty-second International Conference on Machine Learning , 2025a.\\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 , 2025b.\\nTaeyoung Yun, Dinghuai Zhang, Jinkyoo Park, and Ling Pan. Learning to sample effective and diverse prompts for text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 23625-23635, 2025.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems , 35:15476-15488, 2022.\\nDavid W. Zhang, Corrado Rainone, Markus F. Peschl, and Roberto Bondesan. Robust scheduling with gflownets. ArXiv , abs/2302.05446, 2023a. URL https://api.semanticscholar.org/Corp usID:256827133 .\\nDinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models with GFlowNets and beyond. arXiv preprint arXiv:2209.02606v2 , 2022a.\\nDinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML) , 2022b.\\nDinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C. Courville, Yoshua Bengio, and Ling Pan. Let the flows tell: Solving graph combinatorial optimization problems with gflownets. ArXiv , abs/2305.17010, 2023b.\\nDinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization, 2024a.\\nDinghuai Zhang, Ling Pan, Ricky T. Q. Chen, Aaron Courville, and Yoshua Bengio. Distributional gflownets with quantile flows, 2024b.\\nDinghuai Zhang, Yizhe Zhang, Jiatao Gu, Ruixiang ZHANG, Joshua M. Susskind, Navdeep Jaitly, and Shuangfei Zhai. Improving GFlownets for text-to-image diffusion alignment. Transactions on Machine Learning Research , 2025a. ISSN 2835-8856. URL https://openreview.net/forum ?id=XDbY3qhM42 .\\n16\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nKaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. A survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827 , 2025b.\\nMingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, and Yoshua Bengio. Phylogfn: Phylogenetic inference with generative flow networks, 2024.\\nHeiko Zimmermann, Fredrik Lindsten, J.-W. van de Meent, and Christian Andersson Naesseth. A variational perspective on generative flow networks. ArXiv , abs/2210.07992, 2022. URL https: //api.semanticscholar.org/CorpusID:252907672 .\\n17\\nFlowRL: Matching Reward Distributions for LLM Reasoning\"},\n",
       "  {'title': 'A. Proof of Proposition 1',\n",
       "   'content': 'We begin by analyzing the gradient of the Kullback-Leibler (KL) divergence between the policy 𝜋𝜃 ( y | x ) and the target reward distribution exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) :\\nNext, consider the trajectory balance objective used in GFlowNets learning [Bartoldson et al., 2025, Bengio et al., 2023b, Lee et al., 2024], defined as:\\nTaking the gradient of this objective with respect to 𝜃 yields:\\nThus, minimizing the KL divergence is equivalent (up to a constant) to minimizing the trajectory balance loss, confirming Proposition 1.'},\n",
       "  {'title': 'B. Theoretical Analysis',\n",
       "   'content': 'We conduct an interpretation of FlowRL that clarifies the role of each component in the objective.\\nProposition 5. Minimizing the KL divergence in Eq. 5 is equivalent (in terms of gradients) to jointly maximizing reward and policy entropy:\\nRemark 6 ( FlowRL beyond reward maximization ) . Proposition 5 reveals that FlowRL can be interpreted as jointly maximizing expected reward and policy entropy. This shift encourages the policy to explore a broader set of high-quality solutions, enabling more diverse and generalizable behaviors on reasoning tasks. Our interpretation also aligns with prior work that views GFlowNets training as a form of maximum entropy RL [Deleu et al., 2024, Mohammadpour et al., 2024].\\n18\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nThe proof of Proposition 5 is provided as below.\\nRecall from Eq. 3 and Eq. 5 that the FlowRL objective is sourced from the minimization of a KL divergence:\\nRearranging the terms, we obtain:\\nFinally, we express the FlowRL objective in its compact form:\\n\\uf8f0\\n\\uf8fb\\nTherefore, minimizing the FlowRL objective can be interpreted as jointly maximizing reward and entropy, while also aligning the policy with a structured prior. The reward term drives task performance, while the normalization term 𝑍𝜙 ( x ) ensures consistency with a properly normalized target distribution. This encourages the policy 𝜋𝜃 to cover the entire reward-weighted distribution rather than collapsing to a few high-reward modes. The reference policy 𝜋 ref provides inductive bias that regularizes the policy toward desirable structures, and the entropy term H( 𝜋𝜃 ) encourages diversity in sampled solutions. Together, these components promote better generalization of FlowRL.'},\n",
       "  {'title': 'C. GFlowNets',\n",
       "   'content': 'We follow the notation of [He et al., 2025, Madan et al., 2023] to introduce the fundamentals of GFlowNets. Let X denote the compositional objects and 𝑅 be a reward function that assigns nonnegative values to each object 𝑥 ∈ X . GFlowNets aim to learn a sequential, constructive sampling policy 𝜋 that generates objects 𝑥 with probabilities proportional to their rewards, i.e., 𝜋 ( 𝑥 ) ∝ 𝑅 ( 𝑥 ) . This process can be represented as a directed acyclic graph (DAG) G = (S , A) , where the vertices 𝑠 ∈ S are referred to as states , and the directed edges ( 𝑢 → 𝑣 ) ∈ A are called actions . The generation of an object 𝑥 ∈ X corresponds to a complete trajectory 𝜏 = ( 𝑠 0 → · · · → 𝑠 𝑛 ) ∈ T within the DAG, beginning at the initial state 𝑠 0 and ending at a terminal state 𝑠 𝑛 ∈ X . The state flow 𝐹 ( 𝑠 ) is defined as a non-negative weight assigned to each state 𝑠 ∈ S . The forward policy 𝑃𝐹 ( 𝑠 ′ | 𝑠 ) specifies the transition probability to a child state 𝑠 ′ , while the backward policy 𝑃𝐵 ( 𝑠 | 𝑠 ′ ) specifies the transition probability to a parent state 𝑠 . To this end, detailed balance objective enforces local flow consistency across every edge ( 𝑠 → 𝑠 ′ ) ∈ A :\\n19\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTo achieve this flow consistency, GFlowNets employ training objectives at different levels of granularity, including detailed balance [Bengio et al., 2023b], trajectory balance [Malkin et al., 2022], and subtrajectory balance [Madan et al., 2023]. Leveraging their diversity-seeking behavior, GFlowNets have been successfully applied across a range of domains, including molecule generation [Cretu et al., 2024], diffusion fine-tuning [Liu et al., 2025b, Zhang et al., 2025a], and amortized reasoning [Hu et al., 2024, Yu et al., 2025a]. Among various training objective in GFlowNets, trajectory balance maintains flow consistency at the trajectory level, defined as:\\nFurthermore, sub-trajectory balance achieves local balance on arbitrary subpaths 𝜏𝑖 : 𝑗 = { 𝑠 𝑖 → · · ·→ 𝑠 𝑗 } , offering a more stable and less biased learning signal. We build on trajectory balance to extend our KL-based objective through a gradient-equivalence formulation (Prop. 1), and further improve it to better support long CoT reasoning in RL.\\nTable 5 | Math reasoning performance (Avg@64) at temperature = 0 . 6. Relative improvements are shown as subscripts, with positive gains in green and negative changes in red. FlowRL consistently outperforms all baselines and achieves the best average score under this low-temperature setting.\\nTable 6 | Math reasoning performance (Avg@64) at temperature = 1 . 0. Relative improvements are shown as subscripts, with positive gains in green. FlowRL maintains robust performance under higher generation randomness and continues to outperform all baselines on average.\\n20\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 7 | Ablation study on the effect of the 𝛽 parameter in FlowRL. We report Avg@16 accuracy across six math reasoning benchmarks for different values of 𝛽 .'},\n",
       "  {'title': 'Diversity Evaluation Prompt',\n",
       "   'content': 'System: You are evaluating the DIVERSITY of solution approaches for a mathematics competition problem. Focus on detecting even SUBTLE differences in methodology that indicate different problemsolving strategies.'},\n",
       "  {'title': 'PROBLEM:', 'content': '{problem}'},\n",
       "  {'title': '16 SOLUTION ATTEMPTS:', 'content': '{formatted_responses}'},\n",
       "  {'title': 'Score 1 - Minimal Diversity:',\n",
       "   'content': 'Same mathematical setup, same variable choices, same solution path\\n14+ responses use essentially identical approaches\\nOnly trivial differences (arithmetic, notation, wording)\\nIndicates very low exploration/diversity in the generation process'},\n",
       "  {'title': 'Score 2 - Low Diversity:',\n",
       "   'content': '1-2 alternative approaches appear but are rare\\n11-13 responses use the same main approach\\nMinor variations within the dominant method (different substitutions, orderings)\\nSome exploration but heavily biased toward one strategy'},\n",
       "  {'title': 'Score 3 - Moderate Diversity:',\n",
       "   'content': '2-3 distinct alternative approaches present\\n7-10 responses use the most common approach\\nNoticeable variation in problem setup or mathematical techniques\\nBalanced mix showing reasonable exploration'},\n",
       "  {'title': 'Score 4 - High Diversity:',\n",
       "   'content': '3-4 distinct solution strategies well-represented\\n4-6 responses use the most common approach\\nMultiple mathematical techniques and problem framings\\nStrong evidence of diverse exploration strategies'},\n",
       "  {'title': 'Score 5 - Maximum Diversity:',\n",
       "   'content': '4+ distinctly different solution strategies\\nNo single approach dominates ( ≤ 3 responses use same method)\\nWide variety of mathematical techniques and creative approaches\\nIMPORTANT: Focusing on the DIVERSITY of the attempted approaches. Return ONLY a number from 1 to 5.\\nExcellent exploration and generation diversity\\n21'}],\n",
       " 'is_processed': True}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_with_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Build paper_with_content and extract metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.services.arxiv.metadata_extractor import MetadataExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-21 14:28:44,995 ] [researchmind] | Module: metadata_extractor |Function: extract_metadata | Line: 177 - INFO - Extracting metadata for paper: 2509.15217v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-21 14:28:45,031 ] [researchmind] | Module: metadata_extractor |Function: extract_metadata | Line: 251 - INFO - Extracted metadata: 1 metrics\n"
     ]
    }
   ],
   "source": [
    "paper_with_content = {**test_paper}\n",
    "paper_with_content['content'] = parsed_content.raw_text if parsed_content else ''\n",
    "paper_with_content['sections'] = [\n",
    "    {'title': s.title, 'content': s.content} for s in (parsed_content.sections if parsed_content else [])\n",
    "]\n",
    "paper_with_content['is_processed'] = True\n",
    "\n",
    "metadata_extractor = MetadataExtractor()\n",
    "enriched = await metadata_extractor.extract_metadata(paper_with_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_with_content.update(enriched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arxiv_id': '2509.15217v1',\n",
       " 'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       " 'abstract': 'Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\\\\%\\\\text{-}4.8\\\\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\\\\%\\\\text{-}3.9\\\\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.',\n",
       " 'authors': ['Yue Xin',\n",
       "  'Wenyuan Wang',\n",
       "  'Rui Pan',\n",
       "  'Ruida Wang',\n",
       "  'Howard Meng',\n",
       "  'Renjie Pi',\n",
       "  'Shizhe Diao',\n",
       "  'Tong Zhang'],\n",
       " 'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       " 'primary_category': 'cs.AI',\n",
       " 'published': '2025-09-18T17:59:11Z',\n",
       " 'updated': '2025-09-18T17:59:11Z',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2509.15217v1',\n",
       " 'arxiv_url': 'http://arxiv.org/abs/2509.15217v1',\n",
       " 'doi': '',\n",
       " 'journal_ref': '',\n",
       " 'content': \"## FlowRL: Matching Reward Distributions for LLM Reasoning\\n\\nXuekai Zhu 1 , Daixuan Cheng 6 , Dinghuai Zhang 3 , Hengli Li 5 , Kaiyan Zhang 4 , Che Jiang 4 , Youbang Sun 4 , Ermo Hua 4 , Yuxin Zuo 4 , Xingtai Lv 4 , Qizheng Zhang 7 , Lin Chen 1 , Fanghao Shao 1 , Bo Xue 1 , Yunchong Song 1 , Zhenjie Yang 1 , Ganqu Cui 2 , Ning Ding 4 , 2 , Jianfeng Gao 3 , Xiaodong Liu 3 , Bowen Zhou 4 , 2 ‡ , Hongyuan Mei 8 ‡ , Zhouhan Lin 1 , 2 ‡\\n\\n1 Shanghai Jiao Tong University 2 Shanghai AI Laboratory 3 Microsoft Research 4 Tsinghua University 5 Peking University 6 Renmin University of China 7 Stanford University 8 Toyota Technological Institute at Chicago\\n\\n/envelope hongyuanmei@gmail.com /envelope xuekaizhu0@gmail.com /github FlowRL ‡ Corresponding Authors.\\n\\nAbstract | We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods ( e.g. , PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10 . 0% over GRPO and 5 . 1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.\\n\\nFigure 1 | Top : Comparison between distribution-matching and reward-maximizing approaches. FlowRL (left) learns to match the full reward distribution, maintaining diversity across multiple modes with low KL divergence. In contrast, reward-maximizing methods like GRPO (right) concentrate on a single high-reward peak, leading to mode collapse and higher KL divergence. Bottom : Performance comparison. FlowRL consistently outperforms GRPO across math and code domains.\\n\\n## 1. Introduction\\n\\nReinforcement learning (RL) plays a crucial role in the post-training of large language models (LLMs) [Zhang et al., 2025b]. A series of powerful reasoning models [Guo et al., 2025, Kavukcuoglu, 2025, Rastogi et al., 2025] have employed large-scale reinforcement learning to achieve strong performance on highly challenging benchmarks [He et al., 2024]. The evolution of RL algorithms for LLM reasoning has progressed through several key stages: REINFORCE [Sutton et al., 1999a] provides a solid baseline that is easy to implement and efficient in simple settings; PPO [Schulman et al., 2017] improves upon REINFORCE with better stability and efficiency in complex settings; GRPO [Shao et al., 2024] simplifies PPO training by eliminating value functions and relying on group comparisons, though at the cost of requiring more rollouts per update. However, all these methods share a fundamental limitation in their reward-maximizing objective.\\n\\nReward-maximizing RL methods tend to overfit to the dominant mode of the reward distribution [Gao et al., 2023, Pan et al., 2022, Skalse et al., 2022, Zelikman et al., 2022]. This often results in limited diversity among generated reasoning paths and reduces generalization to less frequent yet valid logical outcomes [Hu et al., 2023]. As illustrated in Figure 1, GRPO neglects other meaningful modes. These drawbacks become especially pronounced in complex long chain-of-thought (CoT; Wei et al., 2022) reasoning, where capturing a diverse distribution of plausible solutions is essential for effective generalization [Liu et al., 2025a]. Recent approaches adjust the clip ratio [Yu et al., 2025b], augment the advantage function with an entropy-based term [Cheng et al., 2025], or selectively promote high-entropy tokens [Wang et al., 2025], thereby dynamically adapting the training data distribution and implicitly increasing diversity during training. This raises a fundamental question: How can we promote diverse exploration to prevent convergence to dominant solution patterns in RL training?\\n\\nIn this paper, we propose FlowRL , a policy optimization algorithm that aligns the policy model with the full reward distribution, encouraging mode coverage. FlowRL achieves more efficient exploration by fundamentally shifting from reward maximization to reward distribution matching, thereby addressing the inherent mode-collapse limitations of previous RL approaches. As illustrated in Figure 1, the core idea of FlowRL is to introduce a learnable partition function that normalizes scalar rewards into a target distribution, and to minimize the reverse KL divergence between the policy and this reward-induced distribution. We develop this KL objective based on the trajectory balance formulation from GFlowNets [Bengio et al., 2023b], providing a gradient equivalence proof that bridges generative modeling and policy optimization. To address the challenges of long CoT training, we introduce two key technical solutions: length normalization to tackle gradient explosion issues that occur with variable-length CoT reasoning, and importance sampling to correct for the distribution mismatch between generated rollouts and the current policy.\\n\\nWe compare FlowRL with mainstream RL algorithms including REINFORCE++, PPO, and GRPO across math and code domains, using both base and distilled LLMs (7B, 32B). In math domain, FlowRL outperforms GRPO and PPO by 10 . 0% and 5 . 1%, respectively, demonstrating consistent improvements across six challenging math benchmarks. Furthermore, FlowRL surpasses both PPO and GRPO on three challenging coding benchmarks, highlighting its strong generalization capabilities in code reasoning tasks. To understand what drives these performance gains, we analyze the diversity of generated reasoning paths. This diversity analysis confirms that FlowRL generates substantially more diverse rollouts than baseline methods, validating our approach's effectiveness in exploring multiple solution strategies.\\n\\nContributions. We summarize the key contributions of this work as follows:\\n\\n- We propose FlowRL, a policy optimization algorithm that shifts from reward maximization to\\n\\n- reward distribution matching via flow balance, encouraging diverse reasoning path exploration while addressing the inherent mode-collapse limitations of existing RL methods.\\n- FlowRL outperforms GRPO and PPO by 10.0% and 5.1% respectively across math benchmarks and demonstrates strong generalization on code reasoning tasks, with diversity analysis confirming substantially more diverse solution exploration.\\n- We introduce length normalization and importance sampling to enable effective training on variablelength CoT reasoning, addressing gradient explosion and sampling mismatch issues.\\n\\n## 2. Preliminaries\\n\\nReinforcement Learning for Reasoning. Weformulate reasoning as a conditional generation problem, where the policy model receives a question x ∈ X and generates an answer y ∈ Y . The objective is to learn a policy 𝜋𝜃 ( y | x ) that produces high-quality answers under task-specific reward signals 𝑟 . To better illustrate the policy optimization procedure, we provide a detailed formulation of GRPO below. For each question x , GRPO samples a group of answers { y 1 , y 2 , . . . , y 𝐺 } from old policy 𝜋𝜃𝑜𝑙𝑑 and updates the model by maximizing the following objective:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝜖 and 𝜆 are hyper-parameters. Here, 𝐴𝑖 denotes the advantage, computed by normalizing the group reward values { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } as 𝐴𝑖 = 𝑟 𝑖 -mean ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) std ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) . Compared to GRPO, REINFORCE applies the policy gradient directly, without advantage normalization, clipping, or KL regularization. PPO uses a critic model to estimate the advantage and employs importance sampling to stabilize policy updates.\\n\\nGFlowNets. Generative Flow Networks [Bengio et al., 2023a] are a probabilistic framework for training stochastic policies to sample discrete, compositional objects ( e.g. , graphs, sequences) in proportion to a given reward. As shown in Figure 2, the core principle of GFlowNets is to balance the forward and backward probability flows at each state, inspired by flow matching [Bengio et al., 2021]. The initial flow is estimated by 𝑍𝜙 ( 𝑠 0 ) at the initial state 𝑠 0 . The output flow is equal to the outcome reward 𝑟 ( 𝑠 𝑛 ) conditioned at the final state 𝑠 𝑛 . Following Lee et al. [2024], we use a 3-layer MLP to parameterize 𝑍𝜙 . This flow-balancing mechanism facilitates the discovery of diverse,\\n\\nFigure 2 | GFlowNets [Bengio et al., 2023a], a flow-balance perspective on reinforcement learning. The initial flow 𝑍𝜙 ( 𝑠 0 ) injects probability mass into the environment, which is transported through intermediate states by the policy 𝜋𝜃 and accumulated at terminal states in proportion to the scalar rewards.\\n\\nhigh-reward solutions by ensuring proper exploration of the solution space. See Appendix C for detailed GFlowNets background.\\n\\n## 3. Methodology\\n\\nIn this section, we first formulate distribution matching in reinforcement learning through reverse KL divergence and establish its connection to trajectory balance from GFlowNets. To address the challenges of gradient explosion and sampling mismatch encountered during long CoT training, we further incorporate length normalization and importance sampling. Using this enhanced framework, we derive a flow-balanced objective, termed FlowRL .\\n\\n## 3.1. From Reward Maximization to Distribution Matching\\n\\nAs illustrated in Figure 1, recent powerful large reasoning models typically employ reward-maximizing RL algorithms, such as PPO or GRPO. However, these methods tend to optimize toward the dominant reward mode, frequently resulting in mode collapse and the neglect of other plausible, high-quality reasoning paths. To address this fundamental limitation, we propose optimizing the policy by aligning its output distribution to a target reward distribution. A simple yet effective way to achieve this is to minimize the reverse KL divergence 1 between the policy and this target. However, in long CoT reasoning tasks, the available supervision in RL is a scalar reward, rather than a full distribution. Moreover, enumerating or sampling all valid trajectories to recover the true reward distribution is computationally intractable.\\n\\nInspired by energy-based modeling [Du and Mordatch, 2019, Hinton et al., 1995], we introduce a learnable partition function 𝑍𝜙 ( x ) to normalize scalar rewards into a valid target distribution. This allows us to minimize the reverse KL divergence between the policy and the reward-weighted distribution, formalized as:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝑟 ( x , y ) is the reward function, 𝛽 is a hyperparameter, 𝑍𝜙 ( x ) is the learned partition function, and the resulting target distribution is defined as ˜ 𝜋 ( y | x ) = exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) . This objective encourages the policy to sample diverse, high-reward trajectories in proportion to their rewards, rather than collapsing to dominant modes as in standard reward maximization.\\n\\nWhile the KL-based formulation provides a principled target distribution, we derive a more practical, RL-style objective that facilitates efficient policy optimization.\\n\\nProposition 1. In terms of expected gradients, minimizing the KL objective in Eq. 2 is equivalent to minimizing the trajectory balance loss used in GFlowNet [Bartoldson et al., 2025, Lee et al., 2024, Malkin et al., 2022, 2023]:\\n\\n<!-- formula-not-decoded -->\\n\\nRemark 2 ( Trajectory balance as a practical surrogate for KL minimization ) . Given the equivalence established in Proposition 1, the KL-based distribution matching objective can be reformulated as the trajectory balance loss. This reformulation provides a practical optimization approach by using a stable squared loss form rather than direct KL optimization, and by treating 𝑍𝜙 ( x ) as a learnable parameter rather than requiring explicit computation of the intractable partition function. The trajectory balance objective thus serves as a tractable surrogate for reward-guided KL minimization that can be directly integrated into existing RL frameworks.\\n\\n1 We use reverse KL since we can only sample from the policy model, not the target reward distribution.\\n\\n## 3.2. FlowRL\\n\\nAs established in Proposition 1, the target reward distribution can be approximated by optimizing the trajectory balance objective. However, applying this objective directly to long CoT reasoning introduces two key challenges:\\n\\nProblem I: Exploding gradients from long trajectories. Trajectory balance is a sequence-level objective, and applying it to long CoT reasoning with up to 8K tokens leads to exploding gradients and unstable updates. This issue is not observed in prior GFlowNets works, which typically operate on short trajectories in small discrete spaces. Specifically, the log-probability term log 𝜋𝜃 ( y | x ) decomposes into a token-wise sum, ˝ 𝑡 log 𝜋𝜃 ( y 𝑡 | y &lt;𝑡 , x ) , causing the gradient norm to potentially scale with sequence length.\\n\\nProblem II: Sampling mismatch. Mainstream RL algorithms such as PPO and GRPO commonly perform micro-batch updates and reuse trajectories collected from an old policy 𝜋𝜃 old , enabling data-efficient training. In contrast, the KL-based trajectory balance objective assumes fully onpolicy sampling, where responses are drawn from the current policy. This mismatch poses practical limitations when integrating trajectory balance into existing RL pipelines.\\n\\nThese limitations motivate our reformulation that retains the benefits of distribution matching while addressing key practical challenges. To enable this reformulation, we first redefine the reward function following established practices in GFlowNets literature [Bartoldson et al., 2025, Lee et al., 2024, Yu et al., 2025a] by incorporating a reference model as a prior constraint on the reward distribution. Specifically , we modify the original exp ( 𝛽𝑟 ( x , y )) to include the reference model:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝑟 ( x , y ) denotes the outcome reward commonly used in reinforcement learning and 𝜋 ref is the initial pre-trained model. We follow Guo et al. [2025] to use outcome-based reward signals, and apply group normalization to 𝑟 ( x , y ) as ˆ 𝑟 𝑖 = ( 𝑟 𝑖 -mean ( r ))/ std ( r ) , where r = { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } denotes the set of rewards within a sampled group. By substituting the redefined reward formulation Eq. 4 into Eq. 3, we derive the following objective 2 :\\n\\n<!-- formula-not-decoded -->\\n\\nRemark 3 ( Reward shaping via length normalization ) . Trajectory balance treats both the initial flow and the outcome reward as sequence-level quantities. In contrast, standard policy optimization methods such as PPO or GRPO assign rewards at the token level and compute gradients at each step. However, for trajectories of varying lengths ( e.g. , CoT responses), this mismatch can cause the log-probability term log 𝜋𝜃 ( y | x ) = ˝ | y | 𝑡 = 1 log 𝜋𝜃 ( 𝑦 𝑡 | 𝑦 &lt;𝑡 , x ) to scale with sequence length. To address this, we apply a form of reward shaping by normalizing log-probabilities with respect to sequence length. Specifically, we rescale the term as 1 | y | log 𝜋𝜃 ( y | x ) , balancing the contributions of long and short sequences and stabilizing the learning signal.\\n\\nRemark 4 ( Importance sampling for data-efficient training ) . To mitigate sampling mismatch, we employ importance sampling inspired by PPO to stabilize policy updates with off-policy data. We re-weight stale trajectories using the importance ratio 𝑤 = 𝜋𝜃 ( y | x )/ 𝜋 old ( y | x ) , which serves as a coefficient in the surrogate loss. Since our objective focuses on optimizing trajectory balance rather than expected return, we detach the gradient from the current policy to prevent excessive policy drift: 𝑤 = detach [ 𝜋𝜃 ( y | x )]/ 𝜋 old ( y | x ) . For additional stability, we incorporate PPO-style clipping to detach\\n\\n.\\n\\n<!-- formula-not-decoded -->\\n\\n2 The substitution replaces 𝛽𝑟 ( x , y ) in trajectory balance objective Eq. 3 with 𝛽𝑟 ( x , y ) + log 𝜋 ref ( y | x ) to incorporate the reference model constraint.\\n\\nIncorporating these improvements into Eq. 5, we arrive at the following FlowRL objective:\\n\\nFlowRL\\n\\n<!-- formula-not-decoded -->\\n\\nwhere the clipped importance weight 𝑤 and normalized reward ˆ 𝑟 ( x , y ) are defined as:\\n\\n<!-- formula-not-decoded -->\\n\\nWe use this objective to update the policy parameters 𝜃 during training, and refer to this strategy as FlowRL . Implementation details and theoretical analysis are provided in § 5 and § B, respectively.\\n\\n## 4. Related Work\\n\\n## 4.1. Reinforcement Learning for Reasoning\\n\\nReinforcement learning has emerged as a powerful approach for large language models post-training on reasoning tasks [Guo et al., 2025, Lightman et al., 2023b, Schulman et al., 2017, Shao et al., 2024, Sutton et al., 1999b]. Most approaches employ reward-maximizing RL to optimize expected cumulative returns. Entropy regularization [Ahmed et al., 2019, Cheng et al., 2025, Haarnoja et al., 2018] is a classical technique for mitigating mode collapse by promoting diversity in the policy's output distribution, and has also been shown to enhance reasoning capabilities in various settings [Chao et al., 2024, Eysenbach and Levine, 2021]. However, for long CoT reasoning, the extended trajectory length (e.g., 8k-16k tokens) makes it difficult for the regularization signal to effectively influence reward-maximizing learning. Recent work [Cheng et al., 2025, Cui et al., 2025, Dong et al., 2025, Wang et al., 2025] has discovered that training with more diverse or high-entropy training data can further enhance training effectiveness. Compared to traditional entropy regularization, the above methods explicitly increase the proportion of low-probability (i.e., high-entropy) tokens in the training data. In our work, we address the mode-collapse problem by fundamentally shifting from reward maximization to reward distribution matching in our RL formulation.\\n\\n## 4.2. GFlowNets\\n\\nGFlowNets [Bengio et al., 2023a] represent a class of diversity-driven algorithms designed to balance probability flows across states. They have rich connections to probabilistic modeling methods [Ma et al., Malkin et al., 2023, Zhang et al., 2022a,b, 2024a, Zimmermann et al., 2022], and control methods [Pan et al., 2023b,c,d, Tiapkin et al., 2024, Zhang et al., 2024b]. This advantage has enabled GFlowNets to achieve successful applications in multiple downstream tasks, such as molecular drug discovery [Jain et al., 2022, 2023a,b, Kim et al., 2023, 2024, Liu et al., 2022, Pan et al., 2023a, Shen et al., 2023], phylogenetic inference [Zhou et al., 2024], and combinatorial optimization [Zhang et al., 2023a,b]. For generative AI, GFlowNets provide a powerful approach to align pretrained models in scenarios such as image generation [Yun et al., 2025, Zhang et al., 2025a] and language model fine-tuning [Hu et al., 2024, Lee et al., 2024, Yu et al., 2025a]. Another line of work primarily focuses on the theoretical aspects of GFlowNets. Recent theoretical studies have interpreted GFlowNets as solving a maximum entropy reinforcement learning problem within a modified Markov Decision Process (MDP) [Deleu et al., 2024, Mohammadpour et al., 2024, Tiapkin et al., 2024]. These theoretical contributions have\\n\\ninspired us to enhance reinforcement learning from a more foundational standpoint using GFlowNets principles. A comprehensive overview of GFlowNets theory can be found in Appendix C.\\n\\n## 4.3. Flow-Matching Policies\\n\\nFlow matching simplifies diffusion-based approaches by learning vector fields that transport samples from prior to target distributions [Lipman et al., 2023]. Recent work has explored flow matching for policy optimization. McAllister et al. [2025] reformulates policy optimization using advantageweighted ratios from conditional flow matching loss, enabling flow-based policy training without expensive likelihood computations. Pfrommer et al. [2025] explored reward-weighted flow matching for improving policies beyond demonstration performance. Park et al. [2025] uses a separate one-step policy to avoid unstable backpropagation through time when training flow policies with RL. Zhang et al. [2025a] proposed a combined loss function integrating PPO and GFlowNets to optimize diffusion model alignment. However, these approaches focus on continuous control, image generation, or vision-action models, rather than addressing mode-collapse limitations in reward-maximizing RL. Inspired by flow matching principles, our work improves upon RL training to enhance training stability while promoting diverse solution exploration.\\n\\n## 5. Experiment Settings\\n\\nBackbone Models. There are two learnable modules in Eq. 6: the policy model 𝜋𝜃 and the partition function 𝑍𝜙 . For the policy model 𝜋𝜃 , we use Qwen-2.5-7B/32B [Team, 2024] for math tasks and DeepSeek-R1-Distill-Qwen-7B [DeepSeek-AI, 2025] for code tasks, respectively. For partition function 𝑍𝜙 , following Lee et al. [2024], we use a randomly initialized 3-layer MLP with hidden dimensions matching those of the base model. The reference model 𝜋 ref is the corresponding fixed pretrained model. All training scripts are based on the veRL [Sheng et al., 2024]. For the reward function, following Lee et al. [2024], we set the hyperparameter 𝛽 = 15.\\n\\nBaselines. We compare our method against three representative reward-maximization RL baselines: REINFORCE++ (R++; Hu et al., 2025, Sutton et al., 1999b), PPO [Schulman et al., 2017], and GRPO [Shao et al., 2024]. All baselines follow the official veRL recipes, with consistent training configurations. For fair comparison, all methods use the same learning rate, batch size, and training steps, and are evaluated at convergence using identical step counts.\\n\\nTraining Configuration. We experiment on both math and code domains. For the math domain, we use the training set collected from DAPO [Yu et al., 2025b]. For the code domain, we follow the setup of DeepCoder [Luo et al., 2025], using their training set. For 7B model training, we use a single node equipped with 8 NVIDIA H800 GPUs (80GB memory each). For 32B model training, we scale to 4 nodes with 32 GPUs to accommodate the larger memory requirements. All experiments use max_prompt_length = 2048 and max_response_length = 8192 across both model sizes. We use a batch size of 512 for math reasoning tasks and 64 for code reasoning tasks. We set the learning rate to 1e-6 and enable dynamic batch sizing in veRL for efficient training. For GRPO and FlowRL, we configure rollout_n = 8, meaning each prompt generates 8 response rollouts as the group size.\\n\\nEvaluation Configuration. For the math domain, we evaluate on six challenging benchmarks: AIME 2024/2025 [MAA, 2025], AMC 2023 [MAA, 2023], MATH-500 [Lightman et al., 2023a], Minerva [Lewkowycz et al., 2022], and Olympiad [He et al., 2024]. For the code domain, we evaluate on LiveCodeBench [Jain et al., 2024], CodeForces [Penedo et al., 2025], and HumanEval+ [Chen et al., 2021]. For all evaluation datasets, we perform 16 rollouts and report the average accuracy, denoted as Avg@16. We further report rating and percentile for Codeforces. During generation, we\\n\\nTable 1 | Results on math benchmarks. We report Avg@16 accuracy with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL outperforms all baselines across both 7B and 32B model scales.\\n\\n|                                       | AIME24                                | AIME25                                | AMC23                                 | MATH500                               | Minerva                               | Olympiad                              | Avg                                   |\\n|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|\\n| Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K |\\n| Backbone                              | 4.6                                   | 2.1                                   | 28.6                                  | 52.5                                  | 27.0                                  | 21.4                                  | 22.7                                  |\\n| R++                                   | 14 . 8 + 10 . 2                       | 9 . 2 + 7 . 1                         | 52 . 7 + 24 . 1                       | 44 . 4 - 8 . 1                        | 17 . 4 - 9 . 6                        | 24 . 5 + 3 . 1                        | 27.1                                  |\\n| PPO                                   | 26 . 9 + 22 . 3                       | 20 . 4 + 18 . 3                       | 76 . 4 + 47 . 8                       | 69 . 2 + 16 . 7                       | 28 . 8 + 1 . 8                        | 37 . 9 + 16 . 5                       | 43.3                                  |\\n| GRPO                                  | 23 . 1 + 18 . 5                       | 14 . 6 + 12 . 5                       | 76 . 9 + 48 . 3                       | 61 . 6 + 9 . 1                        | 19 . 0 - 8 . 0                        | 34 . 9 + 13 . 5                       | 38.3                                  |\\n| FlowRL                                | 24 . 0 + 19 . 4                       | 21 . 9 + 19 . 8                       | 73 . 8 + 45 . 2                       | 80 . 8 + 28 . 3                       | 38 . 2 + 11 . 2                       | 51 . 8 + 30 . 4                       | 48.4                                  |\\n| Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  |\\n| Backbone                              | 4.4                                   | 2.1                                   | 30.8                                  | 54.5                                  | 22.4                                  | 24.0                                  | 23.0                                  |\\n| R++                                   | 11 . 0 + 6 . 6                        | 5 . 4 + 3 . 3                         | 66 . 7 + 35 . 9                       | 54 . 3 - 0 . 2                        | 24 . 4 + 2 . 0                        | 27 . 3 + 3 . 3                        | 31.5                                  |\\n| PPO                                   | 9 . 4 + 5 . 0                         | 7 . 3 + 5 . 2                         | 63 . 4 + 32 . 6                       | 58 . 0 + 3 . 5                        | 26 . 5 + 4 . 1                        | 27 . 3 + 3 . 3                        | 32.0                                  |\\n| GRPO                                  | 13 . 5 + 9 . 1                        | 9 . 8 + 7 . 7                         | 64 . 5 + 33 . 7                       | 57 . 1 + 2 . 6                        | 23 . 1 + 0 . 7                        | 26 . 9 + 2 . 9                        | 32.5                                  |\\n| FlowRL                                | 15 . 4 + 11 . 0                       | 10 . 8 + 8 . 7                        | 54 . 5 + 23 . 7                       | 67 . 0 + 12 . 5                       | 31 . 4 + 9 . 0                        | 34 . 6 + 10 . 6                       | 35.6                                  |\\n\\nTable 2 | Results on code benchmarks. We report metrics with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL achieves the strongest performance across all three benchmarks, demonstrating its effectiveness in code reasoning tasks.\\n\\n| Models                           | LiveCodeBench                    | LiveCodeBench                    | CodeForces                       | CodeForces      | HumanEval+     |\\n|----------------------------------|----------------------------------|----------------------------------|----------------------------------|-----------------|----------------|\\n|                                  | Avg@16                           | Pass@16                          | Rating                           | Percentile      | Avg@16         |\\n| DeepSeek-R1-Distill-Qwen-7B, Max | DeepSeek-R1-Distill-Qwen-7B, Max | DeepSeek-R1-Distill-Qwen-7B, Max | DeepSeek-R1-Distill-Qwen-7B, Max | Response        | Len=8K         |\\n| Backbone                         | 30.7                             | 49.5                             | 886.7                            | 19.4            | 80.9           |\\n| R++                              | 30 . 5 - 0 . 2                   | 52 . 7 + 3 . 2                   | 1208 . 0 + 321 . 3               | 56 . 8 + 37 . 4 | 76 . 6 - 4 . 3 |\\n| PPO                              | 35 . 1 + 4 . 4                   | 54 . 5 + 5 . 0                   | 1403 . 1 + 516 . 4               | 73 . 7 + 54 . 3 | 82 . 3 + 1 . 4 |\\n| GRPO                             | 32 . 8 + 2 . 1                   | 52 . 3 + 2 . 8                   | 1313 . 8 + 427 . 1               | 67 . 1 + 47 . 7 | 80 . 1 - 0 . 8 |\\n| FlowRL                           | 37 . 4 + 6 . 7                   | 56 . 3 + 6 . 8                   | 1549 . 5 + 662 . 8               | 83 . 3 + 63 . 9 | 83 . 3 + 2 . 4 |\\n\\nuse sampling parameters of temperature =0.6 and top_p =0.95 for all evaluations. The response length for evaluation is set to 8,192, consistent with the training configuration.\\n\\n## 6. Results\\n\\n## 6.1. Main Results\\n\\nOur experimental results, summarized in Table 1 and Table 2, demonstrate that FlowRL consistently outperforms all reward-maximization baselines across both math and code reasoning domains. Table 1 reports results on math reasoning benchmarks using both 7B and 32B base models, while Table 2 presents the corresponding results on code reasoning tasks. On math reasoning tasks, FlowRL achieves the highest average accuracy of 35.6% with the 7B model and 48.4% with the 32B model, surpassing PPO by 5.1% and GRPO by 10.1% on the 32B model. FlowRL shows strong improvements on challenging benchmarks like MATH-500 and Olympiad problems, demonstrating consistent gains\\n\\nTable 3 | Ablation study on FlowRL with Qwen2.5-7B as the base model. Avg@16 accuracy is reported across six math reasoning benchmarks. IS denotes importance sampling.\\n\\n| Method               |   AIME 2024 |   AIME 2025 |   AMC 2023 |   MATH-500 |   Minerva |   Olympiad |   Avg |\\n|----------------------|-------------|-------------|------------|------------|-----------|------------|-------|\\n| FlowRL               |       15.41 |       10.83 |      54.53 |      66.96 |     31.41 |      34.61 | 35.63 |\\n| w/o IS               |        6.25 |        7.91 |      41.4  |      56.97 |     22.19 |      25.52 | 26.71 |\\n| Zhang et al. [2025a] |       10.41 |        6.66 |      53.75 |      66.5  |     30.97 |      33.72 | 33.67 |\\n\\nacross diverse mathematical domains. On code generation tasks, FlowRL achieves compelling improvements with the highest Avg@16 score of 37.43% on LiveCodeBench, a Codeforces rating of 1549.47 with 83.3% percentile ranking, and 83.28% accuracy on HumanEval+, outperforming all baselines across the board. These consistent performance gains across both domains and model scales provide strong empirical evidence that FlowRL's flow-balanced optimization successfully enhances generalization. This improvement comes from promoting diverse solution exploration compared to previous reward-maximizing RL approaches.\\n\\n## 6.2. Ablation Studies\\n\\nWe conduct ablation studies on importance sampling and the 𝛽 hyperparameter. For importance sampling, we compared the performance with and without it, and implemented a combined loss approach proposed by Zhang et al. [2025a] that simultaneously optimizes both GFlowNets and PPO objectives. This combined loss focuses on optimizing diffusion models, and we adapt it to long CoT reasoning tasks for comparison. Table 3 demonstrates that importance sampling substantially improves FlowRL performance across all math reasoning benchmarks. Compared to Zhang et al. [2025a], using importance sampling as a trajectory-level ratio is more suitable than the combined loss of GFlowNets and PPO. The performance drop without importance sampling (from 35.63% to 26.71%) highlights the critical role of correcting for distribution mismatch between rollout generation and policy training. For the hyperparam-\\n\\nFigure 3 | Ablation study on the 𝛽 in FlowRL. 𝛽 = 15 (highlighted in blue) achieves the best performance.\\n\\neter 𝛽 , we conduct a series of parameter ablation studies, and Figure 3 shows that 𝛽 = 15 achieves optimal performance, with detailed results shown in Table 7.\\n\\n## 7. Analysis\\n\\n## 7.1. Diversity Analysis\\n\\nTo assess solution diversity , we follow the approach of Yu et al. [2025a] and employ GPT-4o-mini [OpenAI, 2024] to evaluate all responses generated by each method on AIME 24/25. The evaluation prompt is shown in Appendix C. As shown in Figure 4, FlowRL achieves higher diversity scores compared to baseline methods. This demonstrates that FlowRL improves sample diversity compared to baselines, which tend to exhibit repetitive solution patterns. This diversity evaluation reveals\\n\\nTable 4 | Case study comparing GRPO and FlowRL rollouts on an AIME problem. GRPO exhibits repetitive patterns (AM-GM × 3, identity loops × 2), while FlowRL follows a more diverse solution path.\\n\\nsignificant differences in exploration patterns across methods. This nearly doubling of diversity score compared to the strongest baseline (PPO) indicates that FlowRL generates qualitatively different solution approaches rather than minor variations of the same strategy. The diversity analysis provides empirical validation of our core hypothesis that flow-balanced optimization promotes mode coverage in complex reasoning tasks.\\n\\n## 7.2. Case Study\\n\\nTable 4 illustrates the behavioral differences between GRPO and FlowRL on a representative AIME problem. GRPO exhibits repetitive patterns, applying AMGM three times and getting stuck in identity loops, failing to solve the problem. FlowRL explores more diverse actions: it sets 𝑎 = 𝑏 , derives a cubic equation, finds the rational root, and reaches the correct answer. This shows that FlowRL successfully avoids the repetitive exploration patterns. The contrast reveals fundamental differences in exploration strategies: GRPO's reward-maximizing approach leads to exploitation of familiar techniques (AM-GM inequality) without exploring alternatives, eventually reaching contradictory conclusions like 𝑎 = 𝑏 = 𝑐 . In contrast, FlowRL's distribution-matching enables strategic decisions such as the symmetry assumption 𝑎 = 𝑏 , which\\n\\nFigure 4 | GPT-judged diversity scores on rollouts of AIME 24/25 problems. FlowRL generates more diverse solutions than R++, GRPO, and PPO.\\n\\ntransforms the problem into a tractable cubic equation 𝑎 3 -27 𝑎 + 46 = 0, allowing systematic solution through rational root testing and polynomial factorization.\\n\\n## 8. Conclusion\\n\\nIn this work, we introduce FlowRL, which transforms scalar rewards into normalized target distributions using a learnable partition function and minimizes the reverse KL divergence between the policy and target distribution. We demonstrate that this approach is theoretically equivalent to trajectory balance objectives from GFlowNets and implicitly maximizes both reward and entropy, thereby promoting diverse reasoning trajectories. To further address gradient explosion and sampling mismatch issues in long CoT reasoning, we incorporate importance sampling and length normalization. Through experiments on math and code reasoning benchmarks, FlowRL achieves consistent improvements across all tasks compared to GRPO and PPO. Our diversity analysis and case studies confirm that FlowRL generates more varied solution approaches while avoiding repetitive patterns.\\n\\n## Acknowledgments\\n\\nWe are grateful to Mingqian Feng and Yuetai Li for their valuable discussions and feedback, which helped improve the quality of this work.\\n\\n## References\\n\\n- Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of entropy on policy optimization. In International conference on machine learning , pages 151-160. PMLR, 2019.\\n- Brian R Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, and Bhavya Kailkhura. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable llm post-training. arXiv preprint arXiv:2503.18929 , 2025.\\n- Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS) , 2021.\\n- Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research , 24(210):1-55, 2023a. URL http: //jmlr.org/papers/v24/22-0364.html .\\n- Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. The Journal of Machine Learning Research , 24(1):10006-10060, 2023b.\\n- Chen-Hao Chao, Chien Feng, Wei-Fang Sun, Cheng-Kuang Lee, Simon See, and Chun-Yi Lee. Maximum entropy reinforcement learning via energy-based normalizing flow. arXiv preprint arXiv:2405.13629 , 2024.\\n- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\\n\\n- Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.\\n- Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758 , 2025.\\n- Miruna Cretu, Charles Harris, Ilia Igashov, Arne Schneuing, Marwin Segler, Bruno Correia, Julien Roy, Emmanuel Bengio, and Pietro Liò. Synflownet: Design of diverse and novel molecules with synthesis constraints. arXiv preprint arXiv:2405.01155 , 2024.\\n- Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617 , 2025.\\n- DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 .\\n- Tristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, and Yoshua Bengio. Discrete probabilistic inference as control in multi-path environments. arXiv preprint arXiv:2402.10309 , 2024.\\n- Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849 , 2025.\\n- Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in neural information processing systems , 32, 2019.\\n- Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257 , 2021.\\n- Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning , pages 10835-10866. PMLR, 2023.\\n- Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\\n- Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning , pages 1861-1870. Pmlr, 2018.\\n- Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 , 2024.\\n- Haoran He, Can Chang, Huazhe Xu, and Ling Pan. Looking backward: Retrospective backward synthesis for goal-conditioned GFlownets. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?id=fNMKqyvuZT .\\n- Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and R M Neal. The 'wake-sleep' algorithm for unsupervised neural networks. Science , 268 5214:1158-61, 1995.\\n- Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. arXiv preprint arXiv:2310.04363 , 2023.\\n\\n- Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/f orum?id=Ouj6p4ca60 .\\n- Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv. org/abs/2501 , 3262:32-33, 2025.\\n- Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena Simine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. International Conference on Machine Learning (ICML) , 2022.\\n- Moksh Jain, Tristan Deleu, Jason S. Hartford, Cheng-Hao Liu, Alex Hernández-García, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. ArXiv , abs/2302.00615, 2023a. URL https: //api.semanticscholar.org/CorpusID:256459319 .\\n- Moksh Jain, Sharath Chandra Raparthy, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective GFlowNets. International Conference on Machine Learning (ICML) , 2023b.\\n- Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.\\n- Koray Kavukcuoglu. Gemini 2.5: Our most intelligent AI model, 2025. URL https://blog.goo gle/technology/google-deepmind/gemini-model-thinking-updates-march-2025/ . Google Blog (The Keyword), Published Mar. 25, 2025.\\n- Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search gflownets. ArXiv , abs/2310.02710, 2023.\\n- Minsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan, Woochang Kim, Jinkyoo Park, Emmanuel Bengio, and Yoshua Bengio. Learning to scale logits for temperature-conditional gflownets, 2024.\\n- Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et al. Learning diverse attacks on large language models for robust red-teaming and safety tuning. arXiv preprint arXiv:2405.18540 , 2024.\\n- Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 3843-3857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/18abb eef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf .\\n- Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050 , 2023a.\\n- Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. In The Twelfth International Conference on Learning Representations , 2023b.\\n\\n- Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t .\\n- Dianbo Liu, Moksh Jain, Bonaventure F. P. Dossou, Qianli Shen, Salem Lahlou, Anirudh Goyal, Nikolay Malkin, Chris C. Emezue, Dinghuai Zhang, Nadhir Hassen, Xu Ji, Kenji Kawaguchi, and Yoshua Bengio. Gflowout: Dropout with generative flow networks. In International Conference on Machine Learning , 2022.\\n- Mingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507 , 2025a.\\n- Zhen Liu, Tim Z Xiao, , Weiyang Liu, Yoshua Bengio, and Dinghuai Zhang. Efficient diversity-preserving diffusion alignment via gradient-informed gflownets. In ICLR , 2025b.\\n- Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, Ion Stoica, and Tianjun Zhang. Deepcoder: A fully open-source 14b coder at o3-mini level, 2025. Notion Blog.\\n- Jiangyan Ma, Emmanuel Bengio, Yoshua Bengio, and Dinghuai Zhang. Baking symmetry into gflownets.\\n- MAA. American mathematics competitions - amc. https://maa.org/ , 2023.\\n- MAA. American invitational mathematics examination - aime. https://maa.org/ , 2025.\\n- Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning , pages 23467-23483. PMLR, 2023.\\n- Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. Advances in Neural Information Processing Systems , 35: 5955-5967, 2022.\\n- Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. International Conference on Learning Representations (ICLR) , 2023.\\n- David McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053 , 2025.\\n- Sobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropy gflownets with soft q-learning. In International Conference on Artificial Intelligence and Statistics , pages 2593-2601. PMLR, 2024.\\n- OpenAI. Gpt-4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-effic ient-intelligence/ , 2024. Accessed: 2024.\\n- Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544 , 2022.\\n- Ling Pan, Moksh Jain, Kanika Madan, and Yoshua Bengio. Pre-training and fine-tuning generative flow networks, 2023a.\\n\\n- Ling Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. Better training of GFlowNets with local credit and incomplete trajectories. International Conference on Machine Learning (ICML) , 2023b.\\n- Ling Pan, Dinghuai Zhang, Aaron Courville, Longbo Huang, and Yoshua Bengio. Generative augmented flow networks. International Conference on Learning Representations (ICLR) , 2023c.\\n- Ling Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic generative flow networks. Uncertainty in Artificial Intelligence (UAI) , 2023d.\\n- Seohong Park, Qiyang Li, and Sergey Levine. Flow q-learning. In Forty-second International Conference on Machine Learning , 2025. URL https://openreview.net/forum?id=KVf2SFL1pi .\\n- Guilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/open-r1/codeforces , 2025.\\n- Samuel Pfrommer, Yixiao Huang, and Somayeh Sojoudi. Reinforcement learning for flow-matching policies. arXiv preprint arXiv:2507.15073 , 2025.\\n- Abhinav Rastogi, Albert Q Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910 , 2025.\\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\\n- Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\\n- Max W. Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet training. ArXiv , abs/2305.07170, 2023. URL https://api.semanticscholar.org/CorpusID:258676487 .\\n- Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 , 2024.\\n- Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460-9471, 2022.\\n- Richard S Sutton, Andrew G Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience , 11(1):126-134, 1999a.\\n- Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. Müller, editors, Advances in Neural Information Processing Systems , volume 12. MIT Press, 1999b. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0b ed98e80ade0a5c43b0f-Paper.pdf .\\n- Qwen Team. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.g ithub.io/blog/qwen2.5/ .\\n\\n- Daniil Tiapkin, Nikita Morozov, Alexey Naumov, and Dmitry P Vetrov. Generative flow networks as entropy-regularized rl. In International Conference on Artificial Intelligence and Statistics , pages 4213-4221. PMLR, 2024.\\n- Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939 , 2025.\\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824-24837, 2022.\\n- Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Training llms for divergent reasoning with minimal examples. In Forty-second International Conference on Machine Learning , 2025a.\\n- Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 , 2025b.\\n- Taeyoung Yun, Dinghuai Zhang, Jinkyoo Park, and Ling Pan. Learning to sample effective and diverse prompts for text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 23625-23635, 2025.\\n- Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems , 35:15476-15488, 2022.\\n- David W. Zhang, Corrado Rainone, Markus F. Peschl, and Roberto Bondesan. Robust scheduling with gflownets. ArXiv , abs/2302.05446, 2023a. URL https://api.semanticscholar.org/Corp usID:256827133 .\\n- Dinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models with GFlowNets and beyond. arXiv preprint arXiv:2209.02606v2 , 2022a.\\n- Dinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML) , 2022b.\\n- Dinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C. Courville, Yoshua Bengio, and Ling Pan. Let the flows tell: Solving graph combinatorial optimization problems with gflownets. ArXiv , abs/2305.17010, 2023b.\\n- Dinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization, 2024a.\\n- Dinghuai Zhang, Ling Pan, Ricky T. Q. Chen, Aaron Courville, and Yoshua Bengio. Distributional gflownets with quantile flows, 2024b.\\n- Dinghuai Zhang, Yizhe Zhang, Jiatao Gu, Ruixiang ZHANG, Joshua M. Susskind, Navdeep Jaitly, and Shuangfei Zhai. Improving GFlownets for text-to-image diffusion alignment. Transactions on Machine Learning Research , 2025a. ISSN 2835-8856. URL https://openreview.net/forum ?id=XDbY3qhM42 .\\n\\n- Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. A survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827 , 2025b.\\n- Mingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, and Yoshua Bengio. Phylogfn: Phylogenetic inference with generative flow networks, 2024.\\n- Heiko Zimmermann, Fredrik Lindsten, J.-W. van de Meent, and Christian Andersson Naesseth. A variational perspective on generative flow networks. ArXiv , abs/2210.07992, 2022. URL https: //api.semanticscholar.org/CorpusID:252907672 .\\n\\n## A. Proof of Proposition 1\\n\\nWe begin by analyzing the gradient of the Kullback-Leibler (KL) divergence between the policy 𝜋𝜃 ( y | x ) and the target reward distribution exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) :\\n\\n<!-- formula-not-decoded -->\\n\\nNext, consider the trajectory balance objective used in GFlowNets learning [Bartoldson et al., 2025, Bengio et al., 2023b, Lee et al., 2024], defined as:\\n\\n<!-- formula-not-decoded -->\\n\\nTaking the gradient of this objective with respect to 𝜃 yields:\\n\\n<!-- formula-not-decoded -->\\n\\nThus, minimizing the KL divergence is equivalent (up to a constant) to minimizing the trajectory balance loss, confirming Proposition 1.\\n\\n## B. Theoretical Analysis\\n\\nWe conduct an interpretation of FlowRL that clarifies the role of each component in the objective.\\n\\nProposition 5. Minimizing the KL divergence in Eq. 5 is equivalent (in terms of gradients) to jointly maximizing reward and policy entropy:\\n\\n<!-- formula-not-decoded -->\\n\\nRemark 6 ( FlowRL beyond reward maximization ) . Proposition 5 reveals that FlowRL can be interpreted as jointly maximizing expected reward and policy entropy. This shift encourages the policy to explore a broader set of high-quality solutions, enabling more diverse and generalizable behaviors on reasoning tasks. Our interpretation also aligns with prior work that views GFlowNets training as a form of maximum entropy RL [Deleu et al., 2024, Mohammadpour et al., 2024].\\n\\nThe proof of Proposition 5 is provided as below.\\n\\nRecall from Eq. 3 and Eq. 5 that the FlowRL objective is sourced from the minimization of a KL divergence:\\n\\n<!-- formula-not-decoded -->\\n\\nRearranging the terms, we obtain:\\n\\n<!-- formula-not-decoded -->\\n\\nFinally, we express the FlowRL objective in its compact form:\\n\\n<!-- formula-not-decoded -->\\n\\n\\uf8f0\\n\\n\\uf8fb\\n\\nTherefore, minimizing the FlowRL objective can be interpreted as jointly maximizing reward and entropy, while also aligning the policy with a structured prior. The reward term drives task performance, while the normalization term 𝑍𝜙 ( x ) ensures consistency with a properly normalized target distribution. This encourages the policy 𝜋𝜃 to cover the entire reward-weighted distribution rather than collapsing to a few high-reward modes. The reference policy 𝜋 ref provides inductive bias that regularizes the policy toward desirable structures, and the entropy term H( 𝜋𝜃 ) encourages diversity in sampled solutions. Together, these components promote better generalization of FlowRL.\\n\\n## C. GFlowNets\\n\\nWe follow the notation of [He et al., 2025, Madan et al., 2023] to introduce the fundamentals of GFlowNets. Let X denote the compositional objects and 𝑅 be a reward function that assigns nonnegative values to each object 𝑥 ∈ X . GFlowNets aim to learn a sequential, constructive sampling policy 𝜋 that generates objects 𝑥 with probabilities proportional to their rewards, i.e., 𝜋 ( 𝑥 ) ∝ 𝑅 ( 𝑥 ) . This process can be represented as a directed acyclic graph (DAG) G = (S , A) , where the vertices 𝑠 ∈ S are referred to as states , and the directed edges ( 𝑢 → 𝑣 ) ∈ A are called actions . The generation of an object 𝑥 ∈ X corresponds to a complete trajectory 𝜏 = ( 𝑠 0 → · · · → 𝑠 𝑛 ) ∈ T within the DAG, beginning at the initial state 𝑠 0 and ending at a terminal state 𝑠 𝑛 ∈ X . The state flow 𝐹 ( 𝑠 ) is defined as a non-negative weight assigned to each state 𝑠 ∈ S . The forward policy 𝑃𝐹 ( 𝑠 ′ | 𝑠 ) specifies the transition probability to a child state 𝑠 ′ , while the backward policy 𝑃𝐵 ( 𝑠 | 𝑠 ′ ) specifies the transition probability to a parent state 𝑠 . To this end, detailed balance objective enforces local flow consistency across every edge ( 𝑠 → 𝑠 ′ ) ∈ A :\\n\\n<!-- formula-not-decoded -->\\n\\nTo achieve this flow consistency, GFlowNets employ training objectives at different levels of granularity, including detailed balance [Bengio et al., 2023b], trajectory balance [Malkin et al., 2022], and subtrajectory balance [Madan et al., 2023]. Leveraging their diversity-seeking behavior, GFlowNets have been successfully applied across a range of domains, including molecule generation [Cretu et al., 2024], diffusion fine-tuning [Liu et al., 2025b, Zhang et al., 2025a], and amortized reasoning [Hu et al., 2024, Yu et al., 2025a]. Among various training objective in GFlowNets, trajectory balance maintains flow consistency at the trajectory level, defined as:\\n\\n<!-- formula-not-decoded -->\\n\\nFurthermore, sub-trajectory balance achieves local balance on arbitrary subpaths 𝜏𝑖 : 𝑗 = { 𝑠 𝑖 → · · ·→ 𝑠 𝑗 } , offering a more stable and less biased learning signal. We build on trajectory balance to extend our KL-based objective through a gradient-equivalence formulation (Prop. 1), and further improve it to better support long CoT reasoning in RL.\\n\\nTable 5 | Math reasoning performance (Avg@64) at temperature = 0 . 6. Relative improvements are shown as subscripts, with positive gains in green and negative changes in red. FlowRL consistently outperforms all baselines and achieves the best average score under this low-temperature setting.\\n\\n| Models                | AIME 2024             | AIME 2025             | AMC 2023              | MATH-500              | Minerva               | Olympiad              | Avg                   |\\n|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\\n| Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model |\\n| Backbone              | 4.37                  | 2.08                  | 30.78                 | 54.48                 | 22.38                 | 24.02                 | 23.02                 |\\n| R++                   | 10 . 57 + 6 . 20      | 5 . 10 + 3 . 02       | 66 . 02 + 35 . 24     | 54 . 29 - 0 . 19      | 24 . 47 + 2 . 09      | 27 . 30 + 3 . 28      | 31.29                 |\\n| PPO                   | 9 . 95 + 5 . 58       | 7 . 34 + 5 . 26       | 63 . 63 + 32 . 85     | 57 . 72 + 3 . 24      | 26 . 22 + 3 . 84      | 27 . 35 + 3 . 33      | 32.03                 |\\n| GRPO                  | 14 . 01 + 9 . 64      | 10 . 73 + 8 . 65      | 64 . 10 + 33 . 32     | 57 . 41 + 2 . 93      | 23 . 17 + 0 . 79      | 27 . 11 + 3 . 09      | 32.76                 |\\n| FlowRL                | 14 . 32 + 9 . 95      | 10 . 05 + 7 . 97      | 55 . 08 + 24 . 30     | 66 . 78 + 12 . 30     | 31 . 52 + 9 . 14      | 34 . 60 + 10 . 58     | 35.39                 |\\n\\nTable 6 | Math reasoning performance (Avg@64) at temperature = 1 . 0. Relative improvements are shown as subscripts, with positive gains in green. FlowRL maintains robust performance under higher generation randomness and continues to outperform all baselines on average.\\n\\n| Models                | AIME 2024             | AIME 2025             | AMC 2023              | MATH-500              | Minerva               | Olympiad              | Avg                   |\\n|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\\n| Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model |\\n| Backbone              | 3.39                  | 1.51                  | 23.90                 | 45.18                 | 16.98                 | 18.27                 | 18.20                 |\\n| R++                   | 10 . 63 + 7 . 24      | 4 . 63 + 3 . 12       | 66 . 99 + 43 . 09     | 54 . 36 + 9 . 18      | 23 . 89 + 6 . 91      | 26 . 65 + 8 . 38      | 31.19                 |\\n| PPO                   | 10 . 52 + 7 . 13      | 6 . 51 + 5 . 00       | 63 . 04 + 39 . 14     | 57 . 46 + 12 . 28     | 25 . 91 + 8 . 93      | 27 . 16 + 8 . 89      | 31.77                 |\\n| GRPO                  | 12 . 50 + 9 . 11      | 10 . 10 + 8 . 59      | 64 . 72 + 40 . 82     | 57 . 15 + 11 . 97     | 23 . 28 + 6 . 30      | 26 . 90 + 8 . 63      | 32.44                 |\\n| FlowRL                | 14 . 22 + 10 . 83     | 9 . 58 + 8 . 07       | 52 . 92 + 29 . 02     | 66 . 20 + 21 . 02     | 30 . 32 + 13 . 34     | 34 . 47 + 16 . 20     | 34.62                 |\\n\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\n\\n| Models   |   AIME 2024 |   AIME 2025 |   AMC 2023 |   MATH-500 |   Minerva |   Olympiad |   Avg |\\n|----------|-------------|-------------|------------|------------|-----------|------------|-------|\\n| 𝛽 = 5    |       13.54 |       10    |      56.09 |      58.91 |     20.79 |      28.72 | 31.34 |\\n| 𝛽 = 10   |       14.79 |       10.2  |      59.53 |      64.3  |     25.27 |      32.39 | 34.41 |\\n| 𝛽 = 15   |       15.41 |       10.83 |      54.53 |      66.96 |     31.41 |      34.61 | 35.63 |\\n| 𝛽 = 30   |       15    |       10.83 |      50.62 |      69.02 |     30.03 |      35.03 | 35.09 |\\n\\nTable 7 | Ablation study on the effect of the 𝛽 parameter in FlowRL. We report Avg@16 accuracy across six math reasoning benchmarks for different values of 𝛽 .\\n\\n## Diversity Evaluation Prompt\\n\\nSystem: You are evaluating the DIVERSITY of solution approaches for a mathematics competition problem. Focus on detecting even SUBTLE differences in methodology that indicate different problemsolving strategies.\\n\\n## PROBLEM:\\n\\n{problem}\\n\\n## 16 SOLUTION ATTEMPTS:\\n\\n{formatted_responses}\\n\\n## EVALUATION CRITERIA - Rate diversity from 1 to 5:\\n\\n## Score 1 - Minimal Diversity:\\n\\n- Same mathematical setup, same variable choices, same solution path\\n- 14+ responses use essentially identical approaches\\n- Only trivial differences (arithmetic, notation, wording)\\n- Indicates very low exploration/diversity in the generation process\\n\\n## Score 2 - Low Diversity:\\n\\n- 1-2 alternative approaches appear but are rare\\n- 11-13 responses use the same main approach\\n- Minor variations within the dominant method (different substitutions, orderings)\\n- Some exploration but heavily biased toward one strategy\\n\\n## Score 3 - Moderate Diversity:\\n\\n- 2-3 distinct alternative approaches present\\n- 7-10 responses use the most common approach\\n- Noticeable variation in problem setup or mathematical techniques\\n- Balanced mix showing reasonable exploration\\n\\n## Score 4 - High Diversity:\\n\\n- 3-4 distinct solution strategies well-represented\\n- 4-6 responses use the most common approach\\n- Multiple mathematical techniques and problem framings\\n- Strong evidence of diverse exploration strategies\\n\\n## Score 5 - Maximum Diversity:\\n\\n- 4+ distinctly different solution strategies\\n- No single approach dominates ( ≤ 3 responses use same method)\\n- Wide variety of mathematical techniques and creative approaches\\n\\nIMPORTANT: Focusing on the DIVERSITY of the attempted approaches. Return ONLY a number from 1 to 5.\\n\\n- Excellent exploration and generation diversity\",\n",
       " 'sections': [{'title': 'Content',\n",
       "   'content': 'arXiv:2509.15207v1  [cs.LG]  18 Sep 2025\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\n2025-09-17'},\n",
       "  {'title': 'FlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       "   'content': 'Xuekai Zhu 1 , Daixuan Cheng 6 , Dinghuai Zhang 3 , Hengli Li 5 , Kaiyan Zhang 4 , Che Jiang 4 , Youbang Sun 4 , Ermo Hua 4 , Yuxin Zuo 4 , Xingtai Lv 4 , Qizheng Zhang 7 , Lin Chen 1 , Fanghao Shao 1 , Bo Xue 1 , Yunchong Song 1 , Zhenjie Yang 1 , Ganqu Cui 2 , Ning Ding 4 , 2 , Jianfeng Gao 3 , Xiaodong Liu 3 , Bowen Zhou 4 , 2 ‡ , Hongyuan Mei 8 ‡ , Zhouhan Lin 1 , 2 ‡\\n1 Shanghai Jiao Tong University 2 Shanghai AI Laboratory 3 Microsoft Research 4 Tsinghua University 5 Peking University 6 Renmin University of China 7 Stanford University 8 Toyota Technological Institute at Chicago\\n/envelope hongyuanmei@gmail.com /envelope xuekaizhu0@gmail.com /github FlowRL ‡ Corresponding Authors.\\nAbstract | We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods ( e.g. , PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10 . 0% over GRPO and 5 . 1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.\\nFigure 1 | Top : Comparison between distribution-matching and reward-maximizing approaches. FlowRL (left) learns to match the full reward distribution, maintaining diversity across multiple modes with low KL divergence. In contrast, reward-maximizing methods like GRPO (right) concentrate on a single high-reward peak, leading to mode collapse and higher KL divergence. Bottom : Performance comparison. FlowRL consistently outperforms GRPO across math and code domains.\\nDistribution-matching: FlowRL\\nKL = 0.11\\nKL = 8.68\\nReward-maximizing\\n∶\\nR++, PPO and GRPO\\nMath Average Score\\nCodeForces Rating\\nFlowRL: Matching Reward Distributions for LLM Reasoning'},\n",
       "  {'title': '1. Introduction',\n",
       "   'content': \"Reinforcement learning (RL) plays a crucial role in the post-training of large language models (LLMs) [Zhang et al., 2025b]. A series of powerful reasoning models [Guo et al., 2025, Kavukcuoglu, 2025, Rastogi et al., 2025] have employed large-scale reinforcement learning to achieve strong performance on highly challenging benchmarks [He et al., 2024]. The evolution of RL algorithms for LLM reasoning has progressed through several key stages: REINFORCE [Sutton et al., 1999a] provides a solid baseline that is easy to implement and efficient in simple settings; PPO [Schulman et al., 2017] improves upon REINFORCE with better stability and efficiency in complex settings; GRPO [Shao et al., 2024] simplifies PPO training by eliminating value functions and relying on group comparisons, though at the cost of requiring more rollouts per update. However, all these methods share a fundamental limitation in their reward-maximizing objective.\\nReward-maximizing RL methods tend to overfit to the dominant mode of the reward distribution [Gao et al., 2023, Pan et al., 2022, Skalse et al., 2022, Zelikman et al., 2022]. This often results in limited diversity among generated reasoning paths and reduces generalization to less frequent yet valid logical outcomes [Hu et al., 2023]. As illustrated in Figure 1, GRPO neglects other meaningful modes. These drawbacks become especially pronounced in complex long chain-of-thought (CoT; Wei et al., 2022) reasoning, where capturing a diverse distribution of plausible solutions is essential for effective generalization [Liu et al., 2025a]. Recent approaches adjust the clip ratio [Yu et al., 2025b], augment the advantage function with an entropy-based term [Cheng et al., 2025], or selectively promote high-entropy tokens [Wang et al., 2025], thereby dynamically adapting the training data distribution and implicitly increasing diversity during training. This raises a fundamental question: How can we promote diverse exploration to prevent convergence to dominant solution patterns in RL training?\\nIn this paper, we propose FlowRL , a policy optimization algorithm that aligns the policy model with the full reward distribution, encouraging mode coverage. FlowRL achieves more efficient exploration by fundamentally shifting from reward maximization to reward distribution matching, thereby addressing the inherent mode-collapse limitations of previous RL approaches. As illustrated in Figure 1, the core idea of FlowRL is to introduce a learnable partition function that normalizes scalar rewards into a target distribution, and to minimize the reverse KL divergence between the policy and this reward-induced distribution. We develop this KL objective based on the trajectory balance formulation from GFlowNets [Bengio et al., 2023b], providing a gradient equivalence proof that bridges generative modeling and policy optimization. To address the challenges of long CoT training, we introduce two key technical solutions: length normalization to tackle gradient explosion issues that occur with variable-length CoT reasoning, and importance sampling to correct for the distribution mismatch between generated rollouts and the current policy.\\nWe compare FlowRL with mainstream RL algorithms including REINFORCE++, PPO, and GRPO across math and code domains, using both base and distilled LLMs (7B, 32B). In math domain, FlowRL outperforms GRPO and PPO by 10 . 0% and 5 . 1%, respectively, demonstrating consistent improvements across six challenging math benchmarks. Furthermore, FlowRL surpasses both PPO and GRPO on three challenging coding benchmarks, highlighting its strong generalization capabilities in code reasoning tasks. To understand what drives these performance gains, we analyze the diversity of generated reasoning paths. This diversity analysis confirms that FlowRL generates substantially more diverse rollouts than baseline methods, validating our approach's effectiveness in exploring multiple solution strategies.\\nContributions. We summarize the key contributions of this work as follows:\\nWe propose FlowRL, a policy optimization algorithm that shifts from reward maximization to\\n2\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nreward distribution matching via flow balance, encouraging diverse reasoning path exploration while addressing the inherent mode-collapse limitations of existing RL methods.\\nFlowRL outperforms GRPO and PPO by 10.0% and 5.1% respectively across math benchmarks and demonstrates strong generalization on code reasoning tasks, with diversity analysis confirming substantially more diverse solution exploration.\\nWe introduce length normalization and importance sampling to enable effective training on variablelength CoT reasoning, addressing gradient explosion and sampling mismatch issues.\"},\n",
       "  {'title': '2. Preliminaries',\n",
       "   'content': 'Reinforcement Learning for Reasoning. Weformulate reasoning as a conditional generation problem, where the policy model receives a question x ∈ X and generates an answer y ∈ Y . The objective is to learn a policy 𝜋𝜃 ( y | x ) that produces high-quality answers under task-specific reward signals 𝑟 . To better illustrate the policy optimization procedure, we provide a detailed formulation of GRPO below. For each question x , GRPO samples a group of answers { y 1 , y 2 , . . . , y 𝐺 } from old policy 𝜋𝜃𝑜𝑙𝑑 and updates the model by maximizing the following objective:\\nwhere 𝜖 and 𝜆 are hyper-parameters. Here, 𝐴𝑖 denotes the advantage, computed by normalizing the group reward values { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } as 𝐴𝑖 = 𝑟 𝑖 -mean ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) std ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) . Compared to GRPO, REINFORCE applies the policy gradient directly, without advantage normalization, clipping, or KL regularization. PPO uses a critic model to estimate the advantage and employs importance sampling to stabilize policy updates.\\nGFlowNets. Generative Flow Networks [Bengio et al., 2023a] are a probabilistic framework for training stochastic policies to sample discrete, compositional objects ( e.g. , graphs, sequences) in proportion to a given reward. As shown in Figure 2, the core principle of GFlowNets is to balance the forward and backward probability flows at each state, inspired by flow matching [Bengio et al., 2021]. The initial flow is estimated by 𝑍𝜙 ( 𝑠 0 ) at the initial state 𝑠 0 . The output flow is equal to the outcome reward 𝑟 ( 𝑠 𝑛 ) conditioned at the final state 𝑠 𝑛 . Following Lee et al. [2024], we use a 3-layer MLP to parameterize 𝑍𝜙 . This flow-balancing mechanism facilitates the discovery of diverse,\\nFigure 2 | GFlowNets [Bengio et al., 2023a], a flow-balance perspective on reinforcement learning. The initial flow 𝑍𝜙 ( 𝑠 0 ) injects probability mass into the environment, which is transported through intermediate states by the policy 𝜋𝜃 and accumulated at terminal states in proportion to the scalar rewards.\\n𝑠\\n!\\n𝑠\\n\"\\n𝑠\\n#\\n𝑠\\n$\\n𝑠\\n%\\n𝑠\\n&\\n𝑠\\n\\'\\n𝑠\\n(\\n𝑠\\n)\\n𝑠\\n*\\n𝑠\\n\"!\\n𝑠\\n+\\n𝑠\\n+\\n𝑆\\n!\\n𝑠\\n+\\n𝑠\\n+\\n𝑠\\n+\\nIn Flow\\nZ\\n\"\\n𝑠\\n#\\nOut Flow r\\n(𝜏)\\nhigh-reward solutions by ensuring proper exploration of the solution space. See Appendix C for detailed GFlowNets background.\\n3\\nFlowRL: Matching Reward Distributions for LLM Reasoning'},\n",
       "  {'title': '3. Methodology',\n",
       "   'content': 'In this section, we first formulate distribution matching in reinforcement learning through reverse KL divergence and establish its connection to trajectory balance from GFlowNets. To address the challenges of gradient explosion and sampling mismatch encountered during long CoT training, we further incorporate length normalization and importance sampling. Using this enhanced framework, we derive a flow-balanced objective, termed FlowRL .'},\n",
       "  {'title': '3.1. From Reward Maximization to Distribution Matching',\n",
       "   'content': 'As illustrated in Figure 1, recent powerful large reasoning models typically employ reward-maximizing RL algorithms, such as PPO or GRPO. However, these methods tend to optimize toward the dominant reward mode, frequently resulting in mode collapse and the neglect of other plausible, high-quality reasoning paths. To address this fundamental limitation, we propose optimizing the policy by aligning its output distribution to a target reward distribution. A simple yet effective way to achieve this is to minimize the reverse KL divergence 1 between the policy and this target. However, in long CoT reasoning tasks, the available supervision in RL is a scalar reward, rather than a full distribution. Moreover, enumerating or sampling all valid trajectories to recover the true reward distribution is computationally intractable.\\nInspired by energy-based modeling [Du and Mordatch, 2019, Hinton et al., 1995], we introduce a learnable partition function 𝑍𝜙 ( x ) to normalize scalar rewards into a valid target distribution. This allows us to minimize the reverse KL divergence between the policy and the reward-weighted distribution, formalized as:\\nwhere 𝑟 ( x , y ) is the reward function, 𝛽 is a hyperparameter, 𝑍𝜙 ( x ) is the learned partition function, and the resulting target distribution is defined as ˜ 𝜋 ( y | x ) = exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) . This objective encourages the policy to sample diverse, high-reward trajectories in proportion to their rewards, rather than collapsing to dominant modes as in standard reward maximization.\\nWhile the KL-based formulation provides a principled target distribution, we derive a more practical, RL-style objective that facilitates efficient policy optimization.\\nProposition 1. In terms of expected gradients, minimizing the KL objective in Eq. 2 is equivalent to minimizing the trajectory balance loss used in GFlowNet [Bartoldson et al., 2025, Lee et al., 2024, Malkin et al., 2022, 2023]:\\nRemark 2 ( Trajectory balance as a practical surrogate for KL minimization ) . Given the equivalence established in Proposition 1, the KL-based distribution matching objective can be reformulated as the trajectory balance loss. This reformulation provides a practical optimization approach by using a stable squared loss form rather than direct KL optimization, and by treating 𝑍𝜙 ( x ) as a learnable parameter rather than requiring explicit computation of the intractable partition function. The trajectory balance objective thus serves as a tractable surrogate for reward-guided KL minimization that can be directly integrated into existing RL frameworks.\\n1 We use reverse KL since we can only sample from the policy model, not the target reward distribution.\\n4\\nFlowRL: Matching Reward Distributions for LLM Reasoning'},\n",
       "  {'title': '3.2. FlowRL',\n",
       "   'content': 'As established in Proposition 1, the target reward distribution can be approximated by optimizing the trajectory balance objective. However, applying this objective directly to long CoT reasoning introduces two key challenges:\\nProblem I: Exploding gradients from long trajectories. Trajectory balance is a sequence-level objective, and applying it to long CoT reasoning with up to 8K tokens leads to exploding gradients and unstable updates. This issue is not observed in prior GFlowNets works, which typically operate on short trajectories in small discrete spaces. Specifically, the log-probability term log 𝜋𝜃 ( y | x ) decomposes into a token-wise sum, ˝ 𝑡 log 𝜋𝜃 ( y 𝑡 | y <𝑡 , x ) , causing the gradient norm to potentially scale with sequence length.\\nProblem II: Sampling mismatch. Mainstream RL algorithms such as PPO and GRPO commonly perform micro-batch updates and reuse trajectories collected from an old policy 𝜋𝜃 old , enabling data-efficient training. In contrast, the KL-based trajectory balance objective assumes fully onpolicy sampling, where responses are drawn from the current policy. This mismatch poses practical limitations when integrating trajectory balance into existing RL pipelines.\\nThese limitations motivate our reformulation that retains the benefits of distribution matching while addressing key practical challenges. To enable this reformulation, we first redefine the reward function following established practices in GFlowNets literature [Bartoldson et al., 2025, Lee et al., 2024, Yu et al., 2025a] by incorporating a reference model as a prior constraint on the reward distribution. Specifically , we modify the original exp ( 𝛽𝑟 ( x , y )) to include the reference model:\\nwhere 𝑟 ( x , y ) denotes the outcome reward commonly used in reinforcement learning and 𝜋 ref is the initial pre-trained model. We follow Guo et al. [2025] to use outcome-based reward signals, and apply group normalization to 𝑟 ( x , y ) as ˆ 𝑟 𝑖 = ( 𝑟 𝑖 -mean ( r ))/ std ( r ) , where r = { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } denotes the set of rewards within a sampled group. By substituting the redefined reward formulation Eq. 4 into Eq. 3, we derive the following objective 2 :\\nRemark 3 ( Reward shaping via length normalization ) . Trajectory balance treats both the initial flow and the outcome reward as sequence-level quantities. In contrast, standard policy optimization methods such as PPO or GRPO assign rewards at the token level and compute gradients at each step. However, for trajectories of varying lengths ( e.g. , CoT responses), this mismatch can cause the log-probability term log 𝜋𝜃 ( y | x ) = ˝ | y | 𝑡 = 1 log 𝜋𝜃 ( 𝑦 𝑡 | 𝑦 <𝑡 , x ) to scale with sequence length. To address this, we apply a form of reward shaping by normalizing log-probabilities with respect to sequence length. Specifically, we rescale the term as 1 | y | log 𝜋𝜃 ( y | x ) , balancing the contributions of long and short sequences and stabilizing the learning signal.\\nRemark 4 ( Importance sampling for data-efficient training ) . To mitigate sampling mismatch, we employ importance sampling inspired by PPO to stabilize policy updates with off-policy data. We re-weight stale trajectories using the importance ratio 𝑤 = 𝜋𝜃 ( y | x )/ 𝜋 old ( y | x ) , which serves as a coefficient in the surrogate loss. Since our objective focuses on optimizing trajectory balance rather than expected return, we detach the gradient from the current policy to prevent excessive policy drift: 𝑤 = detach [ 𝜋𝜃 ( y | x )]/ 𝜋 old ( y | x ) . For additional stability, we incorporate PPO-style clipping to detach\\n.\\n2 The substitution replaces 𝛽𝑟 ( x , y ) in trajectory balance objective Eq. 3 with 𝛽𝑟 ( x , y ) + log 𝜋 ref ( y | x ) to incorporate the reference model constraint.\\n5\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nIncorporating these improvements into Eq. 5, we arrive at the following FlowRL objective:\\nFlowRL\\nwhere the clipped importance weight 𝑤 and normalized reward ˆ 𝑟 ( x , y ) are defined as:\\nWe use this objective to update the policy parameters 𝜃 during training, and refer to this strategy as FlowRL . Implementation details and theoretical analysis are provided in § 5 and § B, respectively.'},\n",
       "  {'title': '4.1. Reinforcement Learning for Reasoning',\n",
       "   'content': \"Reinforcement learning has emerged as a powerful approach for large language models post-training on reasoning tasks [Guo et al., 2025, Lightman et al., 2023b, Schulman et al., 2017, Shao et al., 2024, Sutton et al., 1999b]. Most approaches employ reward-maximizing RL to optimize expected cumulative returns. Entropy regularization [Ahmed et al., 2019, Cheng et al., 2025, Haarnoja et al., 2018] is a classical technique for mitigating mode collapse by promoting diversity in the policy's output distribution, and has also been shown to enhance reasoning capabilities in various settings [Chao et al., 2024, Eysenbach and Levine, 2021]. However, for long CoT reasoning, the extended trajectory length (e.g., 8k-16k tokens) makes it difficult for the regularization signal to effectively influence reward-maximizing learning. Recent work [Cheng et al., 2025, Cui et al., 2025, Dong et al., 2025, Wang et al., 2025] has discovered that training with more diverse or high-entropy training data can further enhance training effectiveness. Compared to traditional entropy regularization, the above methods explicitly increase the proportion of low-probability (i.e., high-entropy) tokens in the training data. In our work, we address the mode-collapse problem by fundamentally shifting from reward maximization to reward distribution matching in our RL formulation.\"},\n",
       "  {'title': '4.2. GFlowNets',\n",
       "   'content': 'GFlowNets [Bengio et al., 2023a] represent a class of diversity-driven algorithms designed to balance probability flows across states. They have rich connections to probabilistic modeling methods [Ma et al., Malkin et al., 2023, Zhang et al., 2022a,b, 2024a, Zimmermann et al., 2022], and control methods [Pan et al., 2023b,c,d, Tiapkin et al., 2024, Zhang et al., 2024b]. This advantage has enabled GFlowNets to achieve successful applications in multiple downstream tasks, such as molecular drug discovery [Jain et al., 2022, 2023a,b, Kim et al., 2023, 2024, Liu et al., 2022, Pan et al., 2023a, Shen et al., 2023], phylogenetic inference [Zhou et al., 2024], and combinatorial optimization [Zhang et al., 2023a,b]. For generative AI, GFlowNets provide a powerful approach to align pretrained models in scenarios such as image generation [Yun et al., 2025, Zhang et al., 2025a] and language model fine-tuning [Hu et al., 2024, Lee et al., 2024, Yu et al., 2025a]. Another line of work primarily focuses on the theoretical aspects of GFlowNets. Recent theoretical studies have interpreted GFlowNets as solving a maximum entropy reinforcement learning problem within a modified Markov Decision Process (MDP) [Deleu et al., 2024, Mohammadpour et al., 2024, Tiapkin et al., 2024]. These theoretical contributions have\\n6\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\ninspired us to enhance reinforcement learning from a more foundational standpoint using GFlowNets principles. A comprehensive overview of GFlowNets theory can be found in Appendix C.'},\n",
       "  {'title': '4.3. Flow-Matching Policies',\n",
       "   'content': 'Flow matching simplifies diffusion-based approaches by learning vector fields that transport samples from prior to target distributions [Lipman et al., 2023]. Recent work has explored flow matching for policy optimization. McAllister et al. [2025] reformulates policy optimization using advantageweighted ratios from conditional flow matching loss, enabling flow-based policy training without expensive likelihood computations. Pfrommer et al. [2025] explored reward-weighted flow matching for improving policies beyond demonstration performance. Park et al. [2025] uses a separate one-step policy to avoid unstable backpropagation through time when training flow policies with RL. Zhang et al. [2025a] proposed a combined loss function integrating PPO and GFlowNets to optimize diffusion model alignment. However, these approaches focus on continuous control, image generation, or vision-action models, rather than addressing mode-collapse limitations in reward-maximizing RL. Inspired by flow matching principles, our work improves upon RL training to enhance training stability while promoting diverse solution exploration.'},\n",
       "  {'title': '5. Experiment Settings',\n",
       "   'content': 'Backbone Models. There are two learnable modules in Eq. 6: the policy model 𝜋𝜃 and the partition function 𝑍𝜙 . For the policy model 𝜋𝜃 , we use Qwen-2.5-7B/32B [Team, 2024] for math tasks and DeepSeek-R1-Distill-Qwen-7B [DeepSeek-AI, 2025] for code tasks, respectively. For partition function 𝑍𝜙 , following Lee et al. [2024], we use a randomly initialized 3-layer MLP with hidden dimensions matching those of the base model. The reference model 𝜋 ref is the corresponding fixed pretrained model. All training scripts are based on the veRL [Sheng et al., 2024]. For the reward function, following Lee et al. [2024], we set the hyperparameter 𝛽 = 15.\\nBaselines. We compare our method against three representative reward-maximization RL baselines: REINFORCE++ (R++; Hu et al., 2025, Sutton et al., 1999b), PPO [Schulman et al., 2017], and GRPO [Shao et al., 2024]. All baselines follow the official veRL recipes, with consistent training configurations. For fair comparison, all methods use the same learning rate, batch size, and training steps, and are evaluated at convergence using identical step counts.\\nTraining Configuration. We experiment on both math and code domains. For the math domain, we use the training set collected from DAPO [Yu et al., 2025b]. For the code domain, we follow the setup of DeepCoder [Luo et al., 2025], using their training set. For 7B model training, we use a single node equipped with 8 NVIDIA H800 GPUs (80GB memory each). For 32B model training, we scale to 4 nodes with 32 GPUs to accommodate the larger memory requirements. All experiments use max_prompt_length = 2048 and max_response_length = 8192 across both model sizes. We use a batch size of 512 for math reasoning tasks and 64 for code reasoning tasks. We set the learning rate to 1e-6 and enable dynamic batch sizing in veRL for efficient training. For GRPO and FlowRL, we configure rollout_n = 8, meaning each prompt generates 8 response rollouts as the group size.\\nEvaluation Configuration. For the math domain, we evaluate on six challenging benchmarks: AIME 2024/2025 [MAA, 2025], AMC 2023 [MAA, 2023], MATH-500 [Lightman et al., 2023a], Minerva [Lewkowycz et al., 2022], and Olympiad [He et al., 2024]. For the code domain, we evaluate on LiveCodeBench [Jain et al., 2024], CodeForces [Penedo et al., 2025], and HumanEval+ [Chen et al., 2021]. For all evaluation datasets, we perform 16 rollouts and report the average accuracy, denoted as Avg@16. We further report rating and percentile for Codeforces. During generation, we\\n7\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 1 | Results on math benchmarks. We report Avg@16 accuracy with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL outperforms all baselines across both 7B and 32B model scales.\\nTable 2 | Results on code benchmarks. We report metrics with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL achieves the strongest performance across all three benchmarks, demonstrating its effectiveness in code reasoning tasks.\\nuse sampling parameters of temperature =0.6 and top_p =0.95 for all evaluations. The response length for evaluation is set to 8,192, consistent with the training configuration.'},\n",
       "  {'title': '6.1. Main Results',\n",
       "   'content': \"Our experimental results, summarized in Table 1 and Table 2, demonstrate that FlowRL consistently outperforms all reward-maximization baselines across both math and code reasoning domains. Table 1 reports results on math reasoning benchmarks using both 7B and 32B base models, while Table 2 presents the corresponding results on code reasoning tasks. On math reasoning tasks, FlowRL achieves the highest average accuracy of 35.6% with the 7B model and 48.4% with the 32B model, surpassing PPO by 5.1% and GRPO by 10.1% on the 32B model. FlowRL shows strong improvements on challenging benchmarks like MATH-500 and Olympiad problems, demonstrating consistent gains\\n8\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 3 | Ablation study on FlowRL with Qwen2.5-7B as the base model. Avg@16 accuracy is reported across six math reasoning benchmarks. IS denotes importance sampling.\\nacross diverse mathematical domains. On code generation tasks, FlowRL achieves compelling improvements with the highest Avg@16 score of 37.43% on LiveCodeBench, a Codeforces rating of 1549.47 with 83.3% percentile ranking, and 83.28% accuracy on HumanEval+, outperforming all baselines across the board. These consistent performance gains across both domains and model scales provide strong empirical evidence that FlowRL's flow-balanced optimization successfully enhances generalization. This improvement comes from promoting diverse solution exploration compared to previous reward-maximizing RL approaches.\"},\n",
       "  {'title': '6.2. Ablation Studies',\n",
       "   'content': 'We conduct ablation studies on importance sampling and the 𝛽 hyperparameter. For importance sampling, we compared the performance with and without it, and implemented a combined loss approach proposed by Zhang et al. [2025a] that simultaneously optimizes both GFlowNets and PPO objectives. This combined loss focuses on optimizing diffusion models, and we adapt it to long CoT reasoning tasks for comparison. Table 3 demonstrates that importance sampling substantially improves FlowRL performance across all math reasoning benchmarks. Compared to Zhang et al. [2025a], using importance sampling as a trajectory-level ratio is more suitable than the combined loss of GFlowNets and PPO. The performance drop without importance sampling (from 35.63% to 26.71%) highlights the critical role of correcting for distribution mismatch between rollout generation and policy training. For the hyperparam-\\nFigure 3 | Ablation study on the 𝛽 in FlowRL. 𝛽 = 15 (highlighted in blue) achieves the best performance.\\n=5\\n=10\\n=15\\n=30\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\nAverage Score (%)\\n31.34\\n34.41\\n35.63\\n35.09\\neter 𝛽 , we conduct a series of parameter ablation studies, and Figure 3 shows that 𝛽 = 15 achieves optimal performance, with detailed results shown in Table 7.'},\n",
       "  {'title': '7.1. Diversity Analysis',\n",
       "   'content': 'To assess solution diversity , we follow the approach of Yu et al. [2025a] and employ GPT-4o-mini [OpenAI, 2024] to evaluate all responses generated by each method on AIME 24/25. The evaluation prompt is shown in Appendix C. As shown in Figure 4, FlowRL achieves higher diversity scores compared to baseline methods. This demonstrates that FlowRL improves sample diversity compared to baselines, which tend to exhibit repetitive solution patterns. This diversity evaluation reveals\\n9\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 4 | Case study comparing GRPO and FlowRL rollouts on an AIME problem. GRPO exhibits repetitive patterns (AM-GM × 3, identity loops × 2), while FlowRL follows a more diverse solution path.'},\n",
       "  {'title': 'Question',\n",
       "   'content': \"Let B be the set of rectangular boxes with surface area 54 and volume 23. Let 𝑟 be the radius of the smallest sphere that can contain each box in B . If 𝑟 2 = 𝑝 𝑞 with gcd ( 𝑝, 𝑞 ) = 1, find 𝑝 + 𝑞 .\\nGRPO\\n'. . .\\ndenote\\n𝑎, 𝑏, 𝑐\\n. . .\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n=\\n54,\\n𝑎𝑏𝑐\\n=\\n23 ...\\n𝑑\\n=\\n√\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2 ,\\n𝑟\\n=\\n𝑑\\n/\\n2\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n=\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n+\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n. . . AM-GM\\n×\\n3 :\\nAM-GM (1)\\n. . .\\nAM-GM (2)\\n. . .\\nAM-GM (3)\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n3\\nidentity loop\\n×\\n2 :\\nloop (1)\\n. . .\\nloop (2)\\n. . .\\n𝑎\\n=\\n𝑏\\n=\\n𝑐\\n(contradiction) .. .\\nback to\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n. . .\\nno factorization . . . '\\nFlowRL\\n'. . .\\nlet\\n𝑎, 𝑏, 𝑐\\nwith\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n=\\n54,\\n𝑎𝑏𝑐\\n=\\n23\\n. . .\\n𝑑\\n=\\n√\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2 ,\\n𝑟\\n=\\n𝑑\\n/\\n2\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n⇒\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n=\\n𝑠\\n2\\n-\\n54\\n. . .\\n𝑎\\n=\\n𝑏\\n. . .\\n𝑎\\n3\\n-\\n27\\n𝑎\\n+\\n46\\n=\\n0\\n. . .\\nrational root\\n𝑎\\n=\\n2 ...\\nfactor\\n(\\n𝑎\\n-\\n2\\n)(\\n𝑎\\n2\\n+\\n2\\n𝑎\\n-\\n23\\n)\\n. . .\\nbranch\\n𝑎\\n=\\n-\\n1\\n+\\n2\\n√\\n6 ...\\nback-sub\\n𝑐\\n=\\n23\\n/\\n𝑎\\n2\\n. . .\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n=\\n657\\n16\\n. . .\\n𝑟\\n2\\n=\\n657\\n64\\n. . .\\nAnswer 721 ...'\\nsignificant differences in exploration patterns across methods. This nearly doubling of diversity score compared to the strongest baseline (PPO) indicates that FlowRL generates qualitatively different solution approaches rather than minor variations of the same strategy. The diversity analysis provides empirical validation of our core hypothesis that flow-balanced optimization promotes mode coverage in complex reasoning tasks.\"},\n",
       "  {'title': '7.2. Case Study',\n",
       "   'content': \"Table 4 illustrates the behavioral differences between GRPO and FlowRL on a representative AIME problem. GRPO exhibits repetitive patterns, applying AMGM three times and getting stuck in identity loops, failing to solve the problem. FlowRL explores more diverse actions: it sets 𝑎 = 𝑏 , derives a cubic equation, finds the rational root, and reaches the correct answer. This shows that FlowRL successfully avoids the repetitive exploration patterns. The contrast reveals fundamental differences in exploration strategies: GRPO's reward-maximizing approach leads to exploitation of familiar techniques (AM-GM inequality) without exploring alternatives, eventually reaching contradictory conclusions like 𝑎 = 𝑏 = 𝑐 . In contrast, FlowRL's distribution-matching enables strategic decisions such as the symmetry assumption 𝑎 = 𝑏 , which\\nFigure 4 | GPT-judged diversity scores on rollouts of AIME 24/25 problems. FlowRL generates more diverse solutions than R++, GRPO, and PPO.\\nR++\\nGRPO\\nPPO\\nFlowRL\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nDiversity Score\\n1.11\\n1.23\\n1.31\\n2.28\\ntransforms the problem into a tractable cubic equation 𝑎 3 -27 𝑎 + 46 = 0, allowing systematic solution through rational root testing and polynomial factorization.\\n10\\nFlowRL: Matching Reward Distributions for LLM Reasoning\"},\n",
       "  {'title': '8. Conclusion',\n",
       "   'content': 'In this work, we introduce FlowRL, which transforms scalar rewards into normalized target distributions using a learnable partition function and minimizes the reverse KL divergence between the policy and target distribution. We demonstrate that this approach is theoretically equivalent to trajectory balance objectives from GFlowNets and implicitly maximizes both reward and entropy, thereby promoting diverse reasoning trajectories. To further address gradient explosion and sampling mismatch issues in long CoT reasoning, we incorporate importance sampling and length normalization. Through experiments on math and code reasoning benchmarks, FlowRL achieves consistent improvements across all tasks compared to GRPO and PPO. Our diversity analysis and case studies confirm that FlowRL generates more varied solution approaches while avoiding repetitive patterns.'},\n",
       "  {'title': 'Acknowledgments',\n",
       "   'content': 'We are grateful to Mingqian Feng and Yuetai Li for their valuable discussions and feedback, which helped improve the quality of this work.'},\n",
       "  {'title': 'References',\n",
       "   'content': \"Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of entropy on policy optimization. In International conference on machine learning , pages 151-160. PMLR, 2019.\\nBrian R Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, and Bhavya Kailkhura. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable llm post-training. arXiv preprint arXiv:2503.18929 , 2025.\\nEmmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS) , 2021.\\nYoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research , 24(210):1-55, 2023a. URL http: //jmlr.org/papers/v24/22-0364.html .\\nYoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. The Journal of Machine Learning Research , 24(1):10006-10060, 2023b.\\nChen-Hao Chao, Chien Feng, Wei-Fang Sun, Cheng-Kuang Lee, Simon See, and Chun-Yi Lee. Maximum entropy reinforcement learning via energy-based normalizing flow. arXiv preprint arXiv:2405.13629 , 2024.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\\n11\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.\\nDaixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758 , 2025.\\nMiruna Cretu, Charles Harris, Ilia Igashov, Arne Schneuing, Marwin Segler, Bruno Correia, Julien Roy, Emmanuel Bengio, and Pietro Liò. Synflownet: Design of diverse and novel molecules with synthesis constraints. arXiv preprint arXiv:2405.01155 , 2024.\\nGanqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617 , 2025.\\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 .\\nTristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, and Yoshua Bengio. Discrete probabilistic inference as control in multi-path environments. arXiv preprint arXiv:2402.10309 , 2024.\\nGuanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849 , 2025.\\nYilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in neural information processing systems , 32, 2019.\\nBenjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257 , 2021.\\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning , pages 10835-10866. PMLR, 2023.\\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning , pages 1861-1870. Pmlr, 2018.\\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 , 2024.\\nHaoran He, Can Chang, Huazhe Xu, and Ling Pan. Looking backward: Retrospective backward synthesis for goal-conditioned GFlownets. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?id=fNMKqyvuZT .\\nGeoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and R M Neal. The 'wake-sleep' algorithm for unsupervised neural networks. Science , 268 5214:1158-61, 1995.\\nEdward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. arXiv preprint arXiv:2310.04363 , 2023.\\n12\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nEdward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/f orum?id=Ouj6p4ca60 .\\nJian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv. org/abs/2501 , 3262:32-33, 2025.\\nMoksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena Simine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. International Conference on Machine Learning (ICML) , 2022.\\nMoksh Jain, Tristan Deleu, Jason S. Hartford, Cheng-Hao Liu, Alex Hernández-García, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. ArXiv , abs/2302.00615, 2023a. URL https: //api.semanticscholar.org/CorpusID:256459319 .\\nMoksh Jain, Sharath Chandra Raparthy, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective GFlowNets. International Conference on Machine Learning (ICML) , 2023b.\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.\\nKoray Kavukcuoglu. Gemini 2.5: Our most intelligent AI model, 2025. URL https://blog.goo gle/technology/google-deepmind/gemini-model-thinking-updates-march-2025/ . Google Blog (The Keyword), Published Mar. 25, 2025.\\nMinsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search gflownets. ArXiv , abs/2310.02710, 2023.\\nMinsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan, Woochang Kim, Jinkyoo Park, Emmanuel Bengio, and Yoshua Bengio. Learning to scale logits for temperature-conditional gflownets, 2024.\\nSeanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et al. Learning diverse attacks on large language models for robust red-teaming and safety tuning. arXiv preprint arXiv:2405.18540 , 2024.\\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 3843-3857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/18abb eef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf .\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050 , 2023a.\\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. In The Twelfth International Conference on Learning Representations , 2023b.\\n13\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nYaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t .\\nDianbo Liu, Moksh Jain, Bonaventure F. P. Dossou, Qianli Shen, Salem Lahlou, Anirudh Goyal, Nikolay Malkin, Chris C. Emezue, Dinghuai Zhang, Nadhir Hassen, Xu Ji, Kenji Kawaguchi, and Yoshua Bengio. Gflowout: Dropout with generative flow networks. In International Conference on Machine Learning , 2022.\\nMingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507 , 2025a.\\nZhen Liu, Tim Z Xiao, , Weiyang Liu, Yoshua Bengio, and Dinghuai Zhang. Efficient diversity-preserving diffusion alignment via gradient-informed gflownets. In ICLR , 2025b.\\nMichael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, Ion Stoica, and Tianjun Zhang. Deepcoder: A fully open-source 14b coder at o3-mini level, 2025. Notion Blog.\\nJiangyan Ma, Emmanuel Bengio, Yoshua Bengio, and Dinghuai Zhang. Baking symmetry into gflownets.\\nMAA. American mathematics competitions - amc. https://maa.org/ , 2023.\\nMAA. American invitational mathematics examination - aime. https://maa.org/ , 2025.\\nKanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning , pages 23467-23483. PMLR, 2023.\\nNikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. Advances in Neural Information Processing Systems , 35: 5955-5967, 2022.\\nNikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. International Conference on Learning Representations (ICLR) , 2023.\\nDavid McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053 , 2025.\\nSobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropy gflownets with soft q-learning. In International Conference on Artificial Intelligence and Statistics , pages 2593-2601. PMLR, 2024.\\nOpenAI. Gpt-4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-effic ient-intelligence/ , 2024. Accessed: 2024.\\nAlexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544 , 2022.\\nLing Pan, Moksh Jain, Kanika Madan, and Yoshua Bengio. Pre-training and fine-tuning generative flow networks, 2023a.\\n14\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nLing Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. Better training of GFlowNets with local credit and incomplete trajectories. International Conference on Machine Learning (ICML) , 2023b.\\nLing Pan, Dinghuai Zhang, Aaron Courville, Longbo Huang, and Yoshua Bengio. Generative augmented flow networks. International Conference on Learning Representations (ICLR) , 2023c.\\nLing Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic generative flow networks. Uncertainty in Artificial Intelligence (UAI) , 2023d.\\nSeohong Park, Qiyang Li, and Sergey Levine. Flow q-learning. In Forty-second International Conference on Machine Learning , 2025. URL https://openreview.net/forum?id=KVf2SFL1pi .\\nGuilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/open-r1/codeforces , 2025.\\nSamuel Pfrommer, Yixiao Huang, and Somayeh Sojoudi. Reinforcement learning for flow-matching policies. arXiv preprint arXiv:2507.15073 , 2025.\\nAbhinav Rastogi, Albert Q Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910 , 2025.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\\nMax W. Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet training. ArXiv , abs/2305.07170, 2023. URL https://api.semanticscholar.org/CorpusID:258676487 .\\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 , 2024.\\nJoar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460-9471, 2022.\\nRichard S Sutton, Andrew G Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience , 11(1):126-134, 1999a.\\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. Müller, editors, Advances in Neural Information Processing Systems , volume 12. MIT Press, 1999b. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0b ed98e80ade0a5c43b0f-Paper.pdf .\\nQwen Team. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.g ithub.io/blog/qwen2.5/ .\\n15\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nDaniil Tiapkin, Nikita Morozov, Alexey Naumov, and Dmitry P Vetrov. Generative flow networks as entropy-regularized rl. In International Conference on Artificial Intelligence and Statistics , pages 4213-4221. PMLR, 2024.\\nShenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939 , 2025.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824-24837, 2022.\\nFangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Training llms for divergent reasoning with minimal examples. In Forty-second International Conference on Machine Learning , 2025a.\\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 , 2025b.\\nTaeyoung Yun, Dinghuai Zhang, Jinkyoo Park, and Ling Pan. Learning to sample effective and diverse prompts for text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 23625-23635, 2025.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems , 35:15476-15488, 2022.\\nDavid W. Zhang, Corrado Rainone, Markus F. Peschl, and Roberto Bondesan. Robust scheduling with gflownets. ArXiv , abs/2302.05446, 2023a. URL https://api.semanticscholar.org/Corp usID:256827133 .\\nDinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models with GFlowNets and beyond. arXiv preprint arXiv:2209.02606v2 , 2022a.\\nDinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML) , 2022b.\\nDinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C. Courville, Yoshua Bengio, and Ling Pan. Let the flows tell: Solving graph combinatorial optimization problems with gflownets. ArXiv , abs/2305.17010, 2023b.\\nDinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization, 2024a.\\nDinghuai Zhang, Ling Pan, Ricky T. Q. Chen, Aaron Courville, and Yoshua Bengio. Distributional gflownets with quantile flows, 2024b.\\nDinghuai Zhang, Yizhe Zhang, Jiatao Gu, Ruixiang ZHANG, Joshua M. Susskind, Navdeep Jaitly, and Shuangfei Zhai. Improving GFlownets for text-to-image diffusion alignment. Transactions on Machine Learning Research , 2025a. ISSN 2835-8856. URL https://openreview.net/forum ?id=XDbY3qhM42 .\\n16\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nKaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. A survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827 , 2025b.\\nMingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, and Yoshua Bengio. Phylogfn: Phylogenetic inference with generative flow networks, 2024.\\nHeiko Zimmermann, Fredrik Lindsten, J.-W. van de Meent, and Christian Andersson Naesseth. A variational perspective on generative flow networks. ArXiv , abs/2210.07992, 2022. URL https: //api.semanticscholar.org/CorpusID:252907672 .\\n17\\nFlowRL: Matching Reward Distributions for LLM Reasoning\"},\n",
       "  {'title': 'A. Proof of Proposition 1',\n",
       "   'content': 'We begin by analyzing the gradient of the Kullback-Leibler (KL) divergence between the policy 𝜋𝜃 ( y | x ) and the target reward distribution exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) :\\nNext, consider the trajectory balance objective used in GFlowNets learning [Bartoldson et al., 2025, Bengio et al., 2023b, Lee et al., 2024], defined as:\\nTaking the gradient of this objective with respect to 𝜃 yields:\\nThus, minimizing the KL divergence is equivalent (up to a constant) to minimizing the trajectory balance loss, confirming Proposition 1.'},\n",
       "  {'title': 'B. Theoretical Analysis',\n",
       "   'content': 'We conduct an interpretation of FlowRL that clarifies the role of each component in the objective.\\nProposition 5. Minimizing the KL divergence in Eq. 5 is equivalent (in terms of gradients) to jointly maximizing reward and policy entropy:\\nRemark 6 ( FlowRL beyond reward maximization ) . Proposition 5 reveals that FlowRL can be interpreted as jointly maximizing expected reward and policy entropy. This shift encourages the policy to explore a broader set of high-quality solutions, enabling more diverse and generalizable behaviors on reasoning tasks. Our interpretation also aligns with prior work that views GFlowNets training as a form of maximum entropy RL [Deleu et al., 2024, Mohammadpour et al., 2024].\\n18\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nThe proof of Proposition 5 is provided as below.\\nRecall from Eq. 3 and Eq. 5 that the FlowRL objective is sourced from the minimization of a KL divergence:\\nRearranging the terms, we obtain:\\nFinally, we express the FlowRL objective in its compact form:\\n\\uf8f0\\n\\uf8fb\\nTherefore, minimizing the FlowRL objective can be interpreted as jointly maximizing reward and entropy, while also aligning the policy with a structured prior. The reward term drives task performance, while the normalization term 𝑍𝜙 ( x ) ensures consistency with a properly normalized target distribution. This encourages the policy 𝜋𝜃 to cover the entire reward-weighted distribution rather than collapsing to a few high-reward modes. The reference policy 𝜋 ref provides inductive bias that regularizes the policy toward desirable structures, and the entropy term H( 𝜋𝜃 ) encourages diversity in sampled solutions. Together, these components promote better generalization of FlowRL.'},\n",
       "  {'title': 'C. GFlowNets',\n",
       "   'content': 'We follow the notation of [He et al., 2025, Madan et al., 2023] to introduce the fundamentals of GFlowNets. Let X denote the compositional objects and 𝑅 be a reward function that assigns nonnegative values to each object 𝑥 ∈ X . GFlowNets aim to learn a sequential, constructive sampling policy 𝜋 that generates objects 𝑥 with probabilities proportional to their rewards, i.e., 𝜋 ( 𝑥 ) ∝ 𝑅 ( 𝑥 ) . This process can be represented as a directed acyclic graph (DAG) G = (S , A) , where the vertices 𝑠 ∈ S are referred to as states , and the directed edges ( 𝑢 → 𝑣 ) ∈ A are called actions . The generation of an object 𝑥 ∈ X corresponds to a complete trajectory 𝜏 = ( 𝑠 0 → · · · → 𝑠 𝑛 ) ∈ T within the DAG, beginning at the initial state 𝑠 0 and ending at a terminal state 𝑠 𝑛 ∈ X . The state flow 𝐹 ( 𝑠 ) is defined as a non-negative weight assigned to each state 𝑠 ∈ S . The forward policy 𝑃𝐹 ( 𝑠 ′ | 𝑠 ) specifies the transition probability to a child state 𝑠 ′ , while the backward policy 𝑃𝐵 ( 𝑠 | 𝑠 ′ ) specifies the transition probability to a parent state 𝑠 . To this end, detailed balance objective enforces local flow consistency across every edge ( 𝑠 → 𝑠 ′ ) ∈ A :\\n19\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTo achieve this flow consistency, GFlowNets employ training objectives at different levels of granularity, including detailed balance [Bengio et al., 2023b], trajectory balance [Malkin et al., 2022], and subtrajectory balance [Madan et al., 2023]. Leveraging their diversity-seeking behavior, GFlowNets have been successfully applied across a range of domains, including molecule generation [Cretu et al., 2024], diffusion fine-tuning [Liu et al., 2025b, Zhang et al., 2025a], and amortized reasoning [Hu et al., 2024, Yu et al., 2025a]. Among various training objective in GFlowNets, trajectory balance maintains flow consistency at the trajectory level, defined as:\\nFurthermore, sub-trajectory balance achieves local balance on arbitrary subpaths 𝜏𝑖 : 𝑗 = { 𝑠 𝑖 → · · ·→ 𝑠 𝑗 } , offering a more stable and less biased learning signal. We build on trajectory balance to extend our KL-based objective through a gradient-equivalence formulation (Prop. 1), and further improve it to better support long CoT reasoning in RL.\\nTable 5 | Math reasoning performance (Avg@64) at temperature = 0 . 6. Relative improvements are shown as subscripts, with positive gains in green and negative changes in red. FlowRL consistently outperforms all baselines and achieves the best average score under this low-temperature setting.\\nTable 6 | Math reasoning performance (Avg@64) at temperature = 1 . 0. Relative improvements are shown as subscripts, with positive gains in green. FlowRL maintains robust performance under higher generation randomness and continues to outperform all baselines on average.\\n20\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 7 | Ablation study on the effect of the 𝛽 parameter in FlowRL. We report Avg@16 accuracy across six math reasoning benchmarks for different values of 𝛽 .'},\n",
       "  {'title': 'Diversity Evaluation Prompt',\n",
       "   'content': 'System: You are evaluating the DIVERSITY of solution approaches for a mathematics competition problem. Focus on detecting even SUBTLE differences in methodology that indicate different problemsolving strategies.'},\n",
       "  {'title': 'PROBLEM:', 'content': '{problem}'},\n",
       "  {'title': '16 SOLUTION ATTEMPTS:', 'content': '{formatted_responses}'},\n",
       "  {'title': 'Score 1 - Minimal Diversity:',\n",
       "   'content': 'Same mathematical setup, same variable choices, same solution path\\n14+ responses use essentially identical approaches\\nOnly trivial differences (arithmetic, notation, wording)\\nIndicates very low exploration/diversity in the generation process'},\n",
       "  {'title': 'Score 2 - Low Diversity:',\n",
       "   'content': '1-2 alternative approaches appear but are rare\\n11-13 responses use the same main approach\\nMinor variations within the dominant method (different substitutions, orderings)\\nSome exploration but heavily biased toward one strategy'},\n",
       "  {'title': 'Score 3 - Moderate Diversity:',\n",
       "   'content': '2-3 distinct alternative approaches present\\n7-10 responses use the most common approach\\nNoticeable variation in problem setup or mathematical techniques\\nBalanced mix showing reasonable exploration'},\n",
       "  {'title': 'Score 4 - High Diversity:',\n",
       "   'content': '3-4 distinct solution strategies well-represented\\n4-6 responses use the most common approach\\nMultiple mathematical techniques and problem framings\\nStrong evidence of diverse exploration strategies'},\n",
       "  {'title': 'Score 5 - Maximum Diversity:',\n",
       "   'content': '4+ distinctly different solution strategies\\nNo single approach dominates ( ≤ 3 responses use same method)\\nWide variety of mathematical techniques and creative approaches\\nIMPORTANT: Focusing on the DIVERSITY of the attempted approaches. Return ONLY a number from 1 to 5.\\nExcellent exploration and generation diversity\\n21'}],\n",
       " 'is_processed': True,\n",
       " 'metrics': {'accuracy': 35.6},\n",
       " 'research_area': 'Artificial Intelligence',\n",
       " 'research_areas_all': ['Artificial Intelligence',\n",
       "  'Computer Vision and Pattern Recognition',\n",
       "  'Machine Learning'],\n",
       " 'word_count': 10103,\n",
       " 'author_count': 8,\n",
       " 'institutions': ['Tsinghua University',\n",
       "  'Shanghai Jiao Tong University',\n",
       "  'Peking University',\n",
       "  'Toyota Technological Institute',\n",
       "  'University of China']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_with_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arxiv_id': '2509.15217v1',\n",
       " 'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       " 'abstract': 'Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\\\\%\\\\text{-}4.8\\\\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\\\\%\\\\text{-}3.9\\\\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.',\n",
       " 'authors': ['Yue Xin',\n",
       "  'Wenyuan Wang',\n",
       "  'Rui Pan',\n",
       "  'Ruida Wang',\n",
       "  'Howard Meng',\n",
       "  'Renjie Pi',\n",
       "  'Shizhe Diao',\n",
       "  'Tong Zhang'],\n",
       " 'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       " 'primary_category': 'cs.AI',\n",
       " 'published': '2025-09-18T17:59:11Z',\n",
       " 'updated': '2025-09-18T17:59:11Z',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2509.15217v1',\n",
       " 'arxiv_url': 'http://arxiv.org/abs/2509.15217v1',\n",
       " 'doi': '',\n",
       " 'journal_ref': '',\n",
       " 'content': \"## FlowRL: Matching Reward Distributions for LLM Reasoning\\n\\nXuekai Zhu 1 , Daixuan Cheng 6 , Dinghuai Zhang 3 , Hengli Li 5 , Kaiyan Zhang 4 , Che Jiang 4 , Youbang Sun 4 , Ermo Hua 4 , Yuxin Zuo 4 , Xingtai Lv 4 , Qizheng Zhang 7 , Lin Chen 1 , Fanghao Shao 1 , Bo Xue 1 , Yunchong Song 1 , Zhenjie Yang 1 , Ganqu Cui 2 , Ning Ding 4 , 2 , Jianfeng Gao 3 , Xiaodong Liu 3 , Bowen Zhou 4 , 2 ‡ , Hongyuan Mei 8 ‡ , Zhouhan Lin 1 , 2 ‡\\n\\n1 Shanghai Jiao Tong University 2 Shanghai AI Laboratory 3 Microsoft Research 4 Tsinghua University 5 Peking University 6 Renmin University of China 7 Stanford University 8 Toyota Technological Institute at Chicago\\n\\n/envelope hongyuanmei@gmail.com /envelope xuekaizhu0@gmail.com /github FlowRL ‡ Corresponding Authors.\\n\\nAbstract | We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods ( e.g. , PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10 . 0% over GRPO and 5 . 1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.\\n\\nFigure 1 | Top : Comparison between distribution-matching and reward-maximizing approaches. FlowRL (left) learns to match the full reward distribution, maintaining diversity across multiple modes with low KL divergence. In contrast, reward-maximizing methods like GRPO (right) concentrate on a single high-reward peak, leading to mode collapse and higher KL divergence. Bottom : Performance comparison. FlowRL consistently outperforms GRPO across math and code domains.\\n\\n## 1. Introduction\\n\\nReinforcement learning (RL) plays a crucial role in the post-training of large language models (LLMs) [Zhang et al., 2025b]. A series of powerful reasoning models [Guo et al., 2025, Kavukcuoglu, 2025, Rastogi et al., 2025] have employed large-scale reinforcement learning to achieve strong performance on highly challenging benchmarks [He et al., 2024]. The evolution of RL algorithms for LLM reasoning has progressed through several key stages: REINFORCE [Sutton et al., 1999a] provides a solid baseline that is easy to implement and efficient in simple settings; PPO [Schulman et al., 2017] improves upon REINFORCE with better stability and efficiency in complex settings; GRPO [Shao et al., 2024] simplifies PPO training by eliminating value functions and relying on group comparisons, though at the cost of requiring more rollouts per update. However, all these methods share a fundamental limitation in their reward-maximizing objective.\\n\\nReward-maximizing RL methods tend to overfit to the dominant mode of the reward distribution [Gao et al., 2023, Pan et al., 2022, Skalse et al., 2022, Zelikman et al., 2022]. This often results in limited diversity among generated reasoning paths and reduces generalization to less frequent yet valid logical outcomes [Hu et al., 2023]. As illustrated in Figure 1, GRPO neglects other meaningful modes. These drawbacks become especially pronounced in complex long chain-of-thought (CoT; Wei et al., 2022) reasoning, where capturing a diverse distribution of plausible solutions is essential for effective generalization [Liu et al., 2025a]. Recent approaches adjust the clip ratio [Yu et al., 2025b], augment the advantage function with an entropy-based term [Cheng et al., 2025], or selectively promote high-entropy tokens [Wang et al., 2025], thereby dynamically adapting the training data distribution and implicitly increasing diversity during training. This raises a fundamental question: How can we promote diverse exploration to prevent convergence to dominant solution patterns in RL training?\\n\\nIn this paper, we propose FlowRL , a policy optimization algorithm that aligns the policy model with the full reward distribution, encouraging mode coverage. FlowRL achieves more efficient exploration by fundamentally shifting from reward maximization to reward distribution matching, thereby addressing the inherent mode-collapse limitations of previous RL approaches. As illustrated in Figure 1, the core idea of FlowRL is to introduce a learnable partition function that normalizes scalar rewards into a target distribution, and to minimize the reverse KL divergence between the policy and this reward-induced distribution. We develop this KL objective based on the trajectory balance formulation from GFlowNets [Bengio et al., 2023b], providing a gradient equivalence proof that bridges generative modeling and policy optimization. To address the challenges of long CoT training, we introduce two key technical solutions: length normalization to tackle gradient explosion issues that occur with variable-length CoT reasoning, and importance sampling to correct for the distribution mismatch between generated rollouts and the current policy.\\n\\nWe compare FlowRL with mainstream RL algorithms including REINFORCE++, PPO, and GRPO across math and code domains, using both base and distilled LLMs (7B, 32B). In math domain, FlowRL outperforms GRPO and PPO by 10 . 0% and 5 . 1%, respectively, demonstrating consistent improvements across six challenging math benchmarks. Furthermore, FlowRL surpasses both PPO and GRPO on three challenging coding benchmarks, highlighting its strong generalization capabilities in code reasoning tasks. To understand what drives these performance gains, we analyze the diversity of generated reasoning paths. This diversity analysis confirms that FlowRL generates substantially more diverse rollouts than baseline methods, validating our approach's effectiveness in exploring multiple solution strategies.\\n\\nContributions. We summarize the key contributions of this work as follows:\\n\\n- We propose FlowRL, a policy optimization algorithm that shifts from reward maximization to\\n\\n- reward distribution matching via flow balance, encouraging diverse reasoning path exploration while addressing the inherent mode-collapse limitations of existing RL methods.\\n- FlowRL outperforms GRPO and PPO by 10.0% and 5.1% respectively across math benchmarks and demonstrates strong generalization on code reasoning tasks, with diversity analysis confirming substantially more diverse solution exploration.\\n- We introduce length normalization and importance sampling to enable effective training on variablelength CoT reasoning, addressing gradient explosion and sampling mismatch issues.\\n\\n## 2. Preliminaries\\n\\nReinforcement Learning for Reasoning. Weformulate reasoning as a conditional generation problem, where the policy model receives a question x ∈ X and generates an answer y ∈ Y . The objective is to learn a policy 𝜋𝜃 ( y | x ) that produces high-quality answers under task-specific reward signals 𝑟 . To better illustrate the policy optimization procedure, we provide a detailed formulation of GRPO below. For each question x , GRPO samples a group of answers { y 1 , y 2 , . . . , y 𝐺 } from old policy 𝜋𝜃𝑜𝑙𝑑 and updates the model by maximizing the following objective:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝜖 and 𝜆 are hyper-parameters. Here, 𝐴𝑖 denotes the advantage, computed by normalizing the group reward values { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } as 𝐴𝑖 = 𝑟 𝑖 -mean ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) std ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) . Compared to GRPO, REINFORCE applies the policy gradient directly, without advantage normalization, clipping, or KL regularization. PPO uses a critic model to estimate the advantage and employs importance sampling to stabilize policy updates.\\n\\nGFlowNets. Generative Flow Networks [Bengio et al., 2023a] are a probabilistic framework for training stochastic policies to sample discrete, compositional objects ( e.g. , graphs, sequences) in proportion to a given reward. As shown in Figure 2, the core principle of GFlowNets is to balance the forward and backward probability flows at each state, inspired by flow matching [Bengio et al., 2021]. The initial flow is estimated by 𝑍𝜙 ( 𝑠 0 ) at the initial state 𝑠 0 . The output flow is equal to the outcome reward 𝑟 ( 𝑠 𝑛 ) conditioned at the final state 𝑠 𝑛 . Following Lee et al. [2024], we use a 3-layer MLP to parameterize 𝑍𝜙 . This flow-balancing mechanism facilitates the discovery of diverse,\\n\\nFigure 2 | GFlowNets [Bengio et al., 2023a], a flow-balance perspective on reinforcement learning. The initial flow 𝑍𝜙 ( 𝑠 0 ) injects probability mass into the environment, which is transported through intermediate states by the policy 𝜋𝜃 and accumulated at terminal states in proportion to the scalar rewards.\\n\\nhigh-reward solutions by ensuring proper exploration of the solution space. See Appendix C for detailed GFlowNets background.\\n\\n## 3. Methodology\\n\\nIn this section, we first formulate distribution matching in reinforcement learning through reverse KL divergence and establish its connection to trajectory balance from GFlowNets. To address the challenges of gradient explosion and sampling mismatch encountered during long CoT training, we further incorporate length normalization and importance sampling. Using this enhanced framework, we derive a flow-balanced objective, termed FlowRL .\\n\\n## 3.1. From Reward Maximization to Distribution Matching\\n\\nAs illustrated in Figure 1, recent powerful large reasoning models typically employ reward-maximizing RL algorithms, such as PPO or GRPO. However, these methods tend to optimize toward the dominant reward mode, frequently resulting in mode collapse and the neglect of other plausible, high-quality reasoning paths. To address this fundamental limitation, we propose optimizing the policy by aligning its output distribution to a target reward distribution. A simple yet effective way to achieve this is to minimize the reverse KL divergence 1 between the policy and this target. However, in long CoT reasoning tasks, the available supervision in RL is a scalar reward, rather than a full distribution. Moreover, enumerating or sampling all valid trajectories to recover the true reward distribution is computationally intractable.\\n\\nInspired by energy-based modeling [Du and Mordatch, 2019, Hinton et al., 1995], we introduce a learnable partition function 𝑍𝜙 ( x ) to normalize scalar rewards into a valid target distribution. This allows us to minimize the reverse KL divergence between the policy and the reward-weighted distribution, formalized as:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝑟 ( x , y ) is the reward function, 𝛽 is a hyperparameter, 𝑍𝜙 ( x ) is the learned partition function, and the resulting target distribution is defined as ˜ 𝜋 ( y | x ) = exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) . This objective encourages the policy to sample diverse, high-reward trajectories in proportion to their rewards, rather than collapsing to dominant modes as in standard reward maximization.\\n\\nWhile the KL-based formulation provides a principled target distribution, we derive a more practical, RL-style objective that facilitates efficient policy optimization.\\n\\nProposition 1. In terms of expected gradients, minimizing the KL objective in Eq. 2 is equivalent to minimizing the trajectory balance loss used in GFlowNet [Bartoldson et al., 2025, Lee et al., 2024, Malkin et al., 2022, 2023]:\\n\\n<!-- formula-not-decoded -->\\n\\nRemark 2 ( Trajectory balance as a practical surrogate for KL minimization ) . Given the equivalence established in Proposition 1, the KL-based distribution matching objective can be reformulated as the trajectory balance loss. This reformulation provides a practical optimization approach by using a stable squared loss form rather than direct KL optimization, and by treating 𝑍𝜙 ( x ) as a learnable parameter rather than requiring explicit computation of the intractable partition function. The trajectory balance objective thus serves as a tractable surrogate for reward-guided KL minimization that can be directly integrated into existing RL frameworks.\\n\\n1 We use reverse KL since we can only sample from the policy model, not the target reward distribution.\\n\\n## 3.2. FlowRL\\n\\nAs established in Proposition 1, the target reward distribution can be approximated by optimizing the trajectory balance objective. However, applying this objective directly to long CoT reasoning introduces two key challenges:\\n\\nProblem I: Exploding gradients from long trajectories. Trajectory balance is a sequence-level objective, and applying it to long CoT reasoning with up to 8K tokens leads to exploding gradients and unstable updates. This issue is not observed in prior GFlowNets works, which typically operate on short trajectories in small discrete spaces. Specifically, the log-probability term log 𝜋𝜃 ( y | x ) decomposes into a token-wise sum, ˝ 𝑡 log 𝜋𝜃 ( y 𝑡 | y &lt;𝑡 , x ) , causing the gradient norm to potentially scale with sequence length.\\n\\nProblem II: Sampling mismatch. Mainstream RL algorithms such as PPO and GRPO commonly perform micro-batch updates and reuse trajectories collected from an old policy 𝜋𝜃 old , enabling data-efficient training. In contrast, the KL-based trajectory balance objective assumes fully onpolicy sampling, where responses are drawn from the current policy. This mismatch poses practical limitations when integrating trajectory balance into existing RL pipelines.\\n\\nThese limitations motivate our reformulation that retains the benefits of distribution matching while addressing key practical challenges. To enable this reformulation, we first redefine the reward function following established practices in GFlowNets literature [Bartoldson et al., 2025, Lee et al., 2024, Yu et al., 2025a] by incorporating a reference model as a prior constraint on the reward distribution. Specifically , we modify the original exp ( 𝛽𝑟 ( x , y )) to include the reference model:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝑟 ( x , y ) denotes the outcome reward commonly used in reinforcement learning and 𝜋 ref is the initial pre-trained model. We follow Guo et al. [2025] to use outcome-based reward signals, and apply group normalization to 𝑟 ( x , y ) as ˆ 𝑟 𝑖 = ( 𝑟 𝑖 -mean ( r ))/ std ( r ) , where r = { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } denotes the set of rewards within a sampled group. By substituting the redefined reward formulation Eq. 4 into Eq. 3, we derive the following objective 2 :\\n\\n<!-- formula-not-decoded -->\\n\\nRemark 3 ( Reward shaping via length normalization ) . Trajectory balance treats both the initial flow and the outcome reward as sequence-level quantities. In contrast, standard policy optimization methods such as PPO or GRPO assign rewards at the token level and compute gradients at each step. However, for trajectories of varying lengths ( e.g. , CoT responses), this mismatch can cause the log-probability term log 𝜋𝜃 ( y | x ) = ˝ | y | 𝑡 = 1 log 𝜋𝜃 ( 𝑦 𝑡 | 𝑦 &lt;𝑡 , x ) to scale with sequence length. To address this, we apply a form of reward shaping by normalizing log-probabilities with respect to sequence length. Specifically, we rescale the term as 1 | y | log 𝜋𝜃 ( y | x ) , balancing the contributions of long and short sequences and stabilizing the learning signal.\\n\\nRemark 4 ( Importance sampling for data-efficient training ) . To mitigate sampling mismatch, we employ importance sampling inspired by PPO to stabilize policy updates with off-policy data. We re-weight stale trajectories using the importance ratio 𝑤 = 𝜋𝜃 ( y | x )/ 𝜋 old ( y | x ) , which serves as a coefficient in the surrogate loss. Since our objective focuses on optimizing trajectory balance rather than expected return, we detach the gradient from the current policy to prevent excessive policy drift: 𝑤 = detach [ 𝜋𝜃 ( y | x )]/ 𝜋 old ( y | x ) . For additional stability, we incorporate PPO-style clipping to detach\\n\\n.\\n\\n<!-- formula-not-decoded -->\\n\\n2 The substitution replaces 𝛽𝑟 ( x , y ) in trajectory balance objective Eq. 3 with 𝛽𝑟 ( x , y ) + log 𝜋 ref ( y | x ) to incorporate the reference model constraint.\\n\\nIncorporating these improvements into Eq. 5, we arrive at the following FlowRL objective:\\n\\nFlowRL\\n\\n<!-- formula-not-decoded -->\\n\\nwhere the clipped importance weight 𝑤 and normalized reward ˆ 𝑟 ( x , y ) are defined as:\\n\\n<!-- formula-not-decoded -->\\n\\nWe use this objective to update the policy parameters 𝜃 during training, and refer to this strategy as FlowRL . Implementation details and theoretical analysis are provided in § 5 and § B, respectively.\\n\\n## 4. Related Work\\n\\n## 4.1. Reinforcement Learning for Reasoning\\n\\nReinforcement learning has emerged as a powerful approach for large language models post-training on reasoning tasks [Guo et al., 2025, Lightman et al., 2023b, Schulman et al., 2017, Shao et al., 2024, Sutton et al., 1999b]. Most approaches employ reward-maximizing RL to optimize expected cumulative returns. Entropy regularization [Ahmed et al., 2019, Cheng et al., 2025, Haarnoja et al., 2018] is a classical technique for mitigating mode collapse by promoting diversity in the policy's output distribution, and has also been shown to enhance reasoning capabilities in various settings [Chao et al., 2024, Eysenbach and Levine, 2021]. However, for long CoT reasoning, the extended trajectory length (e.g., 8k-16k tokens) makes it difficult for the regularization signal to effectively influence reward-maximizing learning. Recent work [Cheng et al., 2025, Cui et al., 2025, Dong et al., 2025, Wang et al., 2025] has discovered that training with more diverse or high-entropy training data can further enhance training effectiveness. Compared to traditional entropy regularization, the above methods explicitly increase the proportion of low-probability (i.e., high-entropy) tokens in the training data. In our work, we address the mode-collapse problem by fundamentally shifting from reward maximization to reward distribution matching in our RL formulation.\\n\\n## 4.2. GFlowNets\\n\\nGFlowNets [Bengio et al., 2023a] represent a class of diversity-driven algorithms designed to balance probability flows across states. They have rich connections to probabilistic modeling methods [Ma et al., Malkin et al., 2023, Zhang et al., 2022a,b, 2024a, Zimmermann et al., 2022], and control methods [Pan et al., 2023b,c,d, Tiapkin et al., 2024, Zhang et al., 2024b]. This advantage has enabled GFlowNets to achieve successful applications in multiple downstream tasks, such as molecular drug discovery [Jain et al., 2022, 2023a,b, Kim et al., 2023, 2024, Liu et al., 2022, Pan et al., 2023a, Shen et al., 2023], phylogenetic inference [Zhou et al., 2024], and combinatorial optimization [Zhang et al., 2023a,b]. For generative AI, GFlowNets provide a powerful approach to align pretrained models in scenarios such as image generation [Yun et al., 2025, Zhang et al., 2025a] and language model fine-tuning [Hu et al., 2024, Lee et al., 2024, Yu et al., 2025a]. Another line of work primarily focuses on the theoretical aspects of GFlowNets. Recent theoretical studies have interpreted GFlowNets as solving a maximum entropy reinforcement learning problem within a modified Markov Decision Process (MDP) [Deleu et al., 2024, Mohammadpour et al., 2024, Tiapkin et al., 2024]. These theoretical contributions have\\n\\ninspired us to enhance reinforcement learning from a more foundational standpoint using GFlowNets principles. A comprehensive overview of GFlowNets theory can be found in Appendix C.\\n\\n## 4.3. Flow-Matching Policies\\n\\nFlow matching simplifies diffusion-based approaches by learning vector fields that transport samples from prior to target distributions [Lipman et al., 2023]. Recent work has explored flow matching for policy optimization. McAllister et al. [2025] reformulates policy optimization using advantageweighted ratios from conditional flow matching loss, enabling flow-based policy training without expensive likelihood computations. Pfrommer et al. [2025] explored reward-weighted flow matching for improving policies beyond demonstration performance. Park et al. [2025] uses a separate one-step policy to avoid unstable backpropagation through time when training flow policies with RL. Zhang et al. [2025a] proposed a combined loss function integrating PPO and GFlowNets to optimize diffusion model alignment. However, these approaches focus on continuous control, image generation, or vision-action models, rather than addressing mode-collapse limitations in reward-maximizing RL. Inspired by flow matching principles, our work improves upon RL training to enhance training stability while promoting diverse solution exploration.\\n\\n## 5. Experiment Settings\\n\\nBackbone Models. There are two learnable modules in Eq. 6: the policy model 𝜋𝜃 and the partition function 𝑍𝜙 . For the policy model 𝜋𝜃 , we use Qwen-2.5-7B/32B [Team, 2024] for math tasks and DeepSeek-R1-Distill-Qwen-7B [DeepSeek-AI, 2025] for code tasks, respectively. For partition function 𝑍𝜙 , following Lee et al. [2024], we use a randomly initialized 3-layer MLP with hidden dimensions matching those of the base model. The reference model 𝜋 ref is the corresponding fixed pretrained model. All training scripts are based on the veRL [Sheng et al., 2024]. For the reward function, following Lee et al. [2024], we set the hyperparameter 𝛽 = 15.\\n\\nBaselines. We compare our method against three representative reward-maximization RL baselines: REINFORCE++ (R++; Hu et al., 2025, Sutton et al., 1999b), PPO [Schulman et al., 2017], and GRPO [Shao et al., 2024]. All baselines follow the official veRL recipes, with consistent training configurations. For fair comparison, all methods use the same learning rate, batch size, and training steps, and are evaluated at convergence using identical step counts.\\n\\nTraining Configuration. We experiment on both math and code domains. For the math domain, we use the training set collected from DAPO [Yu et al., 2025b]. For the code domain, we follow the setup of DeepCoder [Luo et al., 2025], using their training set. For 7B model training, we use a single node equipped with 8 NVIDIA H800 GPUs (80GB memory each). For 32B model training, we scale to 4 nodes with 32 GPUs to accommodate the larger memory requirements. All experiments use max_prompt_length = 2048 and max_response_length = 8192 across both model sizes. We use a batch size of 512 for math reasoning tasks and 64 for code reasoning tasks. We set the learning rate to 1e-6 and enable dynamic batch sizing in veRL for efficient training. For GRPO and FlowRL, we configure rollout_n = 8, meaning each prompt generates 8 response rollouts as the group size.\\n\\nEvaluation Configuration. For the math domain, we evaluate on six challenging benchmarks: AIME 2024/2025 [MAA, 2025], AMC 2023 [MAA, 2023], MATH-500 [Lightman et al., 2023a], Minerva [Lewkowycz et al., 2022], and Olympiad [He et al., 2024]. For the code domain, we evaluate on LiveCodeBench [Jain et al., 2024], CodeForces [Penedo et al., 2025], and HumanEval+ [Chen et al., 2021]. For all evaluation datasets, we perform 16 rollouts and report the average accuracy, denoted as Avg@16. We further report rating and percentile for Codeforces. During generation, we\\n\\nTable 1 | Results on math benchmarks. We report Avg@16 accuracy with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL outperforms all baselines across both 7B and 32B model scales.\\n\\n|                                       | AIME24                                | AIME25                                | AMC23                                 | MATH500                               | Minerva                               | Olympiad                              | Avg                                   |\\n|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|\\n| Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K |\\n| Backbone                              | 4.6                                   | 2.1                                   | 28.6                                  | 52.5                                  | 27.0                                  | 21.4                                  | 22.7                                  |\\n| R++                                   | 14 . 8 + 10 . 2                       | 9 . 2 + 7 . 1                         | 52 . 7 + 24 . 1                       | 44 . 4 - 8 . 1                        | 17 . 4 - 9 . 6                        | 24 . 5 + 3 . 1                        | 27.1                                  |\\n| PPO                                   | 26 . 9 + 22 . 3                       | 20 . 4 + 18 . 3                       | 76 . 4 + 47 . 8                       | 69 . 2 + 16 . 7                       | 28 . 8 + 1 . 8                        | 37 . 9 + 16 . 5                       | 43.3                                  |\\n| GRPO                                  | 23 . 1 + 18 . 5                       | 14 . 6 + 12 . 5                       | 76 . 9 + 48 . 3                       | 61 . 6 + 9 . 1                        | 19 . 0 - 8 . 0                        | 34 . 9 + 13 . 5                       | 38.3                                  |\\n| FlowRL                                | 24 . 0 + 19 . 4                       | 21 . 9 + 19 . 8                       | 73 . 8 + 45 . 2                       | 80 . 8 + 28 . 3                       | 38 . 2 + 11 . 2                       | 51 . 8 + 30 . 4                       | 48.4                                  |\\n| Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  |\\n| Backbone                              | 4.4                                   | 2.1                                   | 30.8                                  | 54.5                                  | 22.4                                  | 24.0                                  | 23.0                                  |\\n| R++                                   | 11 . 0 + 6 . 6                        | 5 . 4 + 3 . 3                         | 66 . 7 + 35 . 9                       | 54 . 3 - 0 . 2                        | 24 . 4 + 2 . 0                        | 27 . 3 + 3 . 3                        | 31.5                                  |\\n| PPO                                   | 9 . 4 + 5 . 0                         | 7 . 3 + 5 . 2                         | 63 . 4 + 32 . 6                       | 58 . 0 + 3 . 5                        | 26 . 5 + 4 . 1                        | 27 . 3 + 3 . 3                        | 32.0                                  |\\n| GRPO                                  | 13 . 5 + 9 . 1                        | 9 . 8 + 7 . 7                         | 64 . 5 + 33 . 7                       | 57 . 1 + 2 . 6                        | 23 . 1 + 0 . 7                        | 26 . 9 + 2 . 9                        | 32.5                                  |\\n| FlowRL                                | 15 . 4 + 11 . 0                       | 10 . 8 + 8 . 7                        | 54 . 5 + 23 . 7                       | 67 . 0 + 12 . 5                       | 31 . 4 + 9 . 0                        | 34 . 6 + 10 . 6                       | 35.6                                  |\\n\\nTable 2 | Results on code benchmarks. We report metrics with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL achieves the strongest performance across all three benchmarks, demonstrating its effectiveness in code reasoning tasks.\\n\\n| Models                           | LiveCodeBench                    | LiveCodeBench                    | CodeForces                       | CodeForces      | HumanEval+     |\\n|----------------------------------|----------------------------------|----------------------------------|----------------------------------|-----------------|----------------|\\n|                                  | Avg@16                           | Pass@16                          | Rating                           | Percentile      | Avg@16         |\\n| DeepSeek-R1-Distill-Qwen-7B, Max | DeepSeek-R1-Distill-Qwen-7B, Max | DeepSeek-R1-Distill-Qwen-7B, Max | DeepSeek-R1-Distill-Qwen-7B, Max | Response        | Len=8K         |\\n| Backbone                         | 30.7                             | 49.5                             | 886.7                            | 19.4            | 80.9           |\\n| R++                              | 30 . 5 - 0 . 2                   | 52 . 7 + 3 . 2                   | 1208 . 0 + 321 . 3               | 56 . 8 + 37 . 4 | 76 . 6 - 4 . 3 |\\n| PPO                              | 35 . 1 + 4 . 4                   | 54 . 5 + 5 . 0                   | 1403 . 1 + 516 . 4               | 73 . 7 + 54 . 3 | 82 . 3 + 1 . 4 |\\n| GRPO                             | 32 . 8 + 2 . 1                   | 52 . 3 + 2 . 8                   | 1313 . 8 + 427 . 1               | 67 . 1 + 47 . 7 | 80 . 1 - 0 . 8 |\\n| FlowRL                           | 37 . 4 + 6 . 7                   | 56 . 3 + 6 . 8                   | 1549 . 5 + 662 . 8               | 83 . 3 + 63 . 9 | 83 . 3 + 2 . 4 |\\n\\nuse sampling parameters of temperature =0.6 and top_p =0.95 for all evaluations. The response length for evaluation is set to 8,192, consistent with the training configuration.\\n\\n## 6. Results\\n\\n## 6.1. Main Results\\n\\nOur experimental results, summarized in Table 1 and Table 2, demonstrate that FlowRL consistently outperforms all reward-maximization baselines across both math and code reasoning domains. Table 1 reports results on math reasoning benchmarks using both 7B and 32B base models, while Table 2 presents the corresponding results on code reasoning tasks. On math reasoning tasks, FlowRL achieves the highest average accuracy of 35.6% with the 7B model and 48.4% with the 32B model, surpassing PPO by 5.1% and GRPO by 10.1% on the 32B model. FlowRL shows strong improvements on challenging benchmarks like MATH-500 and Olympiad problems, demonstrating consistent gains\\n\\nTable 3 | Ablation study on FlowRL with Qwen2.5-7B as the base model. Avg@16 accuracy is reported across six math reasoning benchmarks. IS denotes importance sampling.\\n\\n| Method               |   AIME 2024 |   AIME 2025 |   AMC 2023 |   MATH-500 |   Minerva |   Olympiad |   Avg |\\n|----------------------|-------------|-------------|------------|------------|-----------|------------|-------|\\n| FlowRL               |       15.41 |       10.83 |      54.53 |      66.96 |     31.41 |      34.61 | 35.63 |\\n| w/o IS               |        6.25 |        7.91 |      41.4  |      56.97 |     22.19 |      25.52 | 26.71 |\\n| Zhang et al. [2025a] |       10.41 |        6.66 |      53.75 |      66.5  |     30.97 |      33.72 | 33.67 |\\n\\nacross diverse mathematical domains. On code generation tasks, FlowRL achieves compelling improvements with the highest Avg@16 score of 37.43% on LiveCodeBench, a Codeforces rating of 1549.47 with 83.3% percentile ranking, and 83.28% accuracy on HumanEval+, outperforming all baselines across the board. These consistent performance gains across both domains and model scales provide strong empirical evidence that FlowRL's flow-balanced optimization successfully enhances generalization. This improvement comes from promoting diverse solution exploration compared to previous reward-maximizing RL approaches.\\n\\n## 6.2. Ablation Studies\\n\\nWe conduct ablation studies on importance sampling and the 𝛽 hyperparameter. For importance sampling, we compared the performance with and without it, and implemented a combined loss approach proposed by Zhang et al. [2025a] that simultaneously optimizes both GFlowNets and PPO objectives. This combined loss focuses on optimizing diffusion models, and we adapt it to long CoT reasoning tasks for comparison. Table 3 demonstrates that importance sampling substantially improves FlowRL performance across all math reasoning benchmarks. Compared to Zhang et al. [2025a], using importance sampling as a trajectory-level ratio is more suitable than the combined loss of GFlowNets and PPO. The performance drop without importance sampling (from 35.63% to 26.71%) highlights the critical role of correcting for distribution mismatch between rollout generation and policy training. For the hyperparam-\\n\\nFigure 3 | Ablation study on the 𝛽 in FlowRL. 𝛽 = 15 (highlighted in blue) achieves the best performance.\\n\\neter 𝛽 , we conduct a series of parameter ablation studies, and Figure 3 shows that 𝛽 = 15 achieves optimal performance, with detailed results shown in Table 7.\\n\\n## 7. Analysis\\n\\n## 7.1. Diversity Analysis\\n\\nTo assess solution diversity , we follow the approach of Yu et al. [2025a] and employ GPT-4o-mini [OpenAI, 2024] to evaluate all responses generated by each method on AIME 24/25. The evaluation prompt is shown in Appendix C. As shown in Figure 4, FlowRL achieves higher diversity scores compared to baseline methods. This demonstrates that FlowRL improves sample diversity compared to baselines, which tend to exhibit repetitive solution patterns. This diversity evaluation reveals\\n\\nTable 4 | Case study comparing GRPO and FlowRL rollouts on an AIME problem. GRPO exhibits repetitive patterns (AM-GM × 3, identity loops × 2), while FlowRL follows a more diverse solution path.\\n\\nsignificant differences in exploration patterns across methods. This nearly doubling of diversity score compared to the strongest baseline (PPO) indicates that FlowRL generates qualitatively different solution approaches rather than minor variations of the same strategy. The diversity analysis provides empirical validation of our core hypothesis that flow-balanced optimization promotes mode coverage in complex reasoning tasks.\\n\\n## 7.2. Case Study\\n\\nTable 4 illustrates the behavioral differences between GRPO and FlowRL on a representative AIME problem. GRPO exhibits repetitive patterns, applying AMGM three times and getting stuck in identity loops, failing to solve the problem. FlowRL explores more diverse actions: it sets 𝑎 = 𝑏 , derives a cubic equation, finds the rational root, and reaches the correct answer. This shows that FlowRL successfully avoids the repetitive exploration patterns. The contrast reveals fundamental differences in exploration strategies: GRPO's reward-maximizing approach leads to exploitation of familiar techniques (AM-GM inequality) without exploring alternatives, eventually reaching contradictory conclusions like 𝑎 = 𝑏 = 𝑐 . In contrast, FlowRL's distribution-matching enables strategic decisions such as the symmetry assumption 𝑎 = 𝑏 , which\\n\\nFigure 4 | GPT-judged diversity scores on rollouts of AIME 24/25 problems. FlowRL generates more diverse solutions than R++, GRPO, and PPO.\\n\\ntransforms the problem into a tractable cubic equation 𝑎 3 -27 𝑎 + 46 = 0, allowing systematic solution through rational root testing and polynomial factorization.\\n\\n## 8. Conclusion\\n\\nIn this work, we introduce FlowRL, which transforms scalar rewards into normalized target distributions using a learnable partition function and minimizes the reverse KL divergence between the policy and target distribution. We demonstrate that this approach is theoretically equivalent to trajectory balance objectives from GFlowNets and implicitly maximizes both reward and entropy, thereby promoting diverse reasoning trajectories. To further address gradient explosion and sampling mismatch issues in long CoT reasoning, we incorporate importance sampling and length normalization. Through experiments on math and code reasoning benchmarks, FlowRL achieves consistent improvements across all tasks compared to GRPO and PPO. Our diversity analysis and case studies confirm that FlowRL generates more varied solution approaches while avoiding repetitive patterns.\\n\\n## Acknowledgments\\n\\nWe are grateful to Mingqian Feng and Yuetai Li for their valuable discussions and feedback, which helped improve the quality of this work.\\n\\n## References\\n\\n- Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of entropy on policy optimization. In International conference on machine learning , pages 151-160. PMLR, 2019.\\n- Brian R Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, and Bhavya Kailkhura. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable llm post-training. arXiv preprint arXiv:2503.18929 , 2025.\\n- Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS) , 2021.\\n- Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research , 24(210):1-55, 2023a. URL http: //jmlr.org/papers/v24/22-0364.html .\\n- Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. The Journal of Machine Learning Research , 24(1):10006-10060, 2023b.\\n- Chen-Hao Chao, Chien Feng, Wei-Fang Sun, Cheng-Kuang Lee, Simon See, and Chun-Yi Lee. Maximum entropy reinforcement learning via energy-based normalizing flow. arXiv preprint arXiv:2405.13629 , 2024.\\n- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\\n\\n- Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.\\n- Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758 , 2025.\\n- Miruna Cretu, Charles Harris, Ilia Igashov, Arne Schneuing, Marwin Segler, Bruno Correia, Julien Roy, Emmanuel Bengio, and Pietro Liò. Synflownet: Design of diverse and novel molecules with synthesis constraints. arXiv preprint arXiv:2405.01155 , 2024.\\n- Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617 , 2025.\\n- DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 .\\n- Tristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, and Yoshua Bengio. Discrete probabilistic inference as control in multi-path environments. arXiv preprint arXiv:2402.10309 , 2024.\\n- Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849 , 2025.\\n- Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in neural information processing systems , 32, 2019.\\n- Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257 , 2021.\\n- Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning , pages 10835-10866. PMLR, 2023.\\n- Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\\n- Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning , pages 1861-1870. Pmlr, 2018.\\n- Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 , 2024.\\n- Haoran He, Can Chang, Huazhe Xu, and Ling Pan. Looking backward: Retrospective backward synthesis for goal-conditioned GFlownets. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?id=fNMKqyvuZT .\\n- Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and R M Neal. The 'wake-sleep' algorithm for unsupervised neural networks. Science , 268 5214:1158-61, 1995.\\n- Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. arXiv preprint arXiv:2310.04363 , 2023.\\n\\n- Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/f orum?id=Ouj6p4ca60 .\\n- Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv. org/abs/2501 , 3262:32-33, 2025.\\n- Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena Simine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. International Conference on Machine Learning (ICML) , 2022.\\n- Moksh Jain, Tristan Deleu, Jason S. Hartford, Cheng-Hao Liu, Alex Hernández-García, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. ArXiv , abs/2302.00615, 2023a. URL https: //api.semanticscholar.org/CorpusID:256459319 .\\n- Moksh Jain, Sharath Chandra Raparthy, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective GFlowNets. International Conference on Machine Learning (ICML) , 2023b.\\n- Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.\\n- Koray Kavukcuoglu. Gemini 2.5: Our most intelligent AI model, 2025. URL https://blog.goo gle/technology/google-deepmind/gemini-model-thinking-updates-march-2025/ . Google Blog (The Keyword), Published Mar. 25, 2025.\\n- Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search gflownets. ArXiv , abs/2310.02710, 2023.\\n- Minsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan, Woochang Kim, Jinkyoo Park, Emmanuel Bengio, and Yoshua Bengio. Learning to scale logits for temperature-conditional gflownets, 2024.\\n- Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et al. Learning diverse attacks on large language models for robust red-teaming and safety tuning. arXiv preprint arXiv:2405.18540 , 2024.\\n- Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 3843-3857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/18abb eef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf .\\n- Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050 , 2023a.\\n- Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. In The Twelfth International Conference on Learning Representations , 2023b.\\n\\n- Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t .\\n- Dianbo Liu, Moksh Jain, Bonaventure F. P. Dossou, Qianli Shen, Salem Lahlou, Anirudh Goyal, Nikolay Malkin, Chris C. Emezue, Dinghuai Zhang, Nadhir Hassen, Xu Ji, Kenji Kawaguchi, and Yoshua Bengio. Gflowout: Dropout with generative flow networks. In International Conference on Machine Learning , 2022.\\n- Mingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507 , 2025a.\\n- Zhen Liu, Tim Z Xiao, , Weiyang Liu, Yoshua Bengio, and Dinghuai Zhang. Efficient diversity-preserving diffusion alignment via gradient-informed gflownets. In ICLR , 2025b.\\n- Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, Ion Stoica, and Tianjun Zhang. Deepcoder: A fully open-source 14b coder at o3-mini level, 2025. Notion Blog.\\n- Jiangyan Ma, Emmanuel Bengio, Yoshua Bengio, and Dinghuai Zhang. Baking symmetry into gflownets.\\n- MAA. American mathematics competitions - amc. https://maa.org/ , 2023.\\n- MAA. American invitational mathematics examination - aime. https://maa.org/ , 2025.\\n- Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning , pages 23467-23483. PMLR, 2023.\\n- Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. Advances in Neural Information Processing Systems , 35: 5955-5967, 2022.\\n- Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. International Conference on Learning Representations (ICLR) , 2023.\\n- David McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053 , 2025.\\n- Sobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropy gflownets with soft q-learning. In International Conference on Artificial Intelligence and Statistics , pages 2593-2601. PMLR, 2024.\\n- OpenAI. Gpt-4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-effic ient-intelligence/ , 2024. Accessed: 2024.\\n- Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544 , 2022.\\n- Ling Pan, Moksh Jain, Kanika Madan, and Yoshua Bengio. Pre-training and fine-tuning generative flow networks, 2023a.\\n\\n- Ling Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. Better training of GFlowNets with local credit and incomplete trajectories. International Conference on Machine Learning (ICML) , 2023b.\\n- Ling Pan, Dinghuai Zhang, Aaron Courville, Longbo Huang, and Yoshua Bengio. Generative augmented flow networks. International Conference on Learning Representations (ICLR) , 2023c.\\n- Ling Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic generative flow networks. Uncertainty in Artificial Intelligence (UAI) , 2023d.\\n- Seohong Park, Qiyang Li, and Sergey Levine. Flow q-learning. In Forty-second International Conference on Machine Learning , 2025. URL https://openreview.net/forum?id=KVf2SFL1pi .\\n- Guilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/open-r1/codeforces , 2025.\\n- Samuel Pfrommer, Yixiao Huang, and Somayeh Sojoudi. Reinforcement learning for flow-matching policies. arXiv preprint arXiv:2507.15073 , 2025.\\n- Abhinav Rastogi, Albert Q Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910 , 2025.\\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\\n- Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\\n- Max W. Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet training. ArXiv , abs/2305.07170, 2023. URL https://api.semanticscholar.org/CorpusID:258676487 .\\n- Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 , 2024.\\n- Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460-9471, 2022.\\n- Richard S Sutton, Andrew G Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience , 11(1):126-134, 1999a.\\n- Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. Müller, editors, Advances in Neural Information Processing Systems , volume 12. MIT Press, 1999b. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0b ed98e80ade0a5c43b0f-Paper.pdf .\\n- Qwen Team. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.g ithub.io/blog/qwen2.5/ .\\n\\n- Daniil Tiapkin, Nikita Morozov, Alexey Naumov, and Dmitry P Vetrov. Generative flow networks as entropy-regularized rl. In International Conference on Artificial Intelligence and Statistics , pages 4213-4221. PMLR, 2024.\\n- Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939 , 2025.\\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824-24837, 2022.\\n- Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Training llms for divergent reasoning with minimal examples. In Forty-second International Conference on Machine Learning , 2025a.\\n- Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 , 2025b.\\n- Taeyoung Yun, Dinghuai Zhang, Jinkyoo Park, and Ling Pan. Learning to sample effective and diverse prompts for text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 23625-23635, 2025.\\n- Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems , 35:15476-15488, 2022.\\n- David W. Zhang, Corrado Rainone, Markus F. Peschl, and Roberto Bondesan. Robust scheduling with gflownets. ArXiv , abs/2302.05446, 2023a. URL https://api.semanticscholar.org/Corp usID:256827133 .\\n- Dinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models with GFlowNets and beyond. arXiv preprint arXiv:2209.02606v2 , 2022a.\\n- Dinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML) , 2022b.\\n- Dinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C. Courville, Yoshua Bengio, and Ling Pan. Let the flows tell: Solving graph combinatorial optimization problems with gflownets. ArXiv , abs/2305.17010, 2023b.\\n- Dinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization, 2024a.\\n- Dinghuai Zhang, Ling Pan, Ricky T. Q. Chen, Aaron Courville, and Yoshua Bengio. Distributional gflownets with quantile flows, 2024b.\\n- Dinghuai Zhang, Yizhe Zhang, Jiatao Gu, Ruixiang ZHANG, Joshua M. Susskind, Navdeep Jaitly, and Shuangfei Zhai. Improving GFlownets for text-to-image diffusion alignment. Transactions on Machine Learning Research , 2025a. ISSN 2835-8856. URL https://openreview.net/forum ?id=XDbY3qhM42 .\\n\\n- Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. A survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827 , 2025b.\\n- Mingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, and Yoshua Bengio. Phylogfn: Phylogenetic inference with generative flow networks, 2024.\\n- Heiko Zimmermann, Fredrik Lindsten, J.-W. van de Meent, and Christian Andersson Naesseth. A variational perspective on generative flow networks. ArXiv , abs/2210.07992, 2022. URL https: //api.semanticscholar.org/CorpusID:252907672 .\\n\\n## A. Proof of Proposition 1\\n\\nWe begin by analyzing the gradient of the Kullback-Leibler (KL) divergence between the policy 𝜋𝜃 ( y | x ) and the target reward distribution exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) :\\n\\n<!-- formula-not-decoded -->\\n\\nNext, consider the trajectory balance objective used in GFlowNets learning [Bartoldson et al., 2025, Bengio et al., 2023b, Lee et al., 2024], defined as:\\n\\n<!-- formula-not-decoded -->\\n\\nTaking the gradient of this objective with respect to 𝜃 yields:\\n\\n<!-- formula-not-decoded -->\\n\\nThus, minimizing the KL divergence is equivalent (up to a constant) to minimizing the trajectory balance loss, confirming Proposition 1.\\n\\n## B. Theoretical Analysis\\n\\nWe conduct an interpretation of FlowRL that clarifies the role of each component in the objective.\\n\\nProposition 5. Minimizing the KL divergence in Eq. 5 is equivalent (in terms of gradients) to jointly maximizing reward and policy entropy:\\n\\n<!-- formula-not-decoded -->\\n\\nRemark 6 ( FlowRL beyond reward maximization ) . Proposition 5 reveals that FlowRL can be interpreted as jointly maximizing expected reward and policy entropy. This shift encourages the policy to explore a broader set of high-quality solutions, enabling more diverse and generalizable behaviors on reasoning tasks. Our interpretation also aligns with prior work that views GFlowNets training as a form of maximum entropy RL [Deleu et al., 2024, Mohammadpour et al., 2024].\\n\\nThe proof of Proposition 5 is provided as below.\\n\\nRecall from Eq. 3 and Eq. 5 that the FlowRL objective is sourced from the minimization of a KL divergence:\\n\\n<!-- formula-not-decoded -->\\n\\nRearranging the terms, we obtain:\\n\\n<!-- formula-not-decoded -->\\n\\nFinally, we express the FlowRL objective in its compact form:\\n\\n<!-- formula-not-decoded -->\\n\\n\\uf8f0\\n\\n\\uf8fb\\n\\nTherefore, minimizing the FlowRL objective can be interpreted as jointly maximizing reward and entropy, while also aligning the policy with a structured prior. The reward term drives task performance, while the normalization term 𝑍𝜙 ( x ) ensures consistency with a properly normalized target distribution. This encourages the policy 𝜋𝜃 to cover the entire reward-weighted distribution rather than collapsing to a few high-reward modes. The reference policy 𝜋 ref provides inductive bias that regularizes the policy toward desirable structures, and the entropy term H( 𝜋𝜃 ) encourages diversity in sampled solutions. Together, these components promote better generalization of FlowRL.\\n\\n## C. GFlowNets\\n\\nWe follow the notation of [He et al., 2025, Madan et al., 2023] to introduce the fundamentals of GFlowNets. Let X denote the compositional objects and 𝑅 be a reward function that assigns nonnegative values to each object 𝑥 ∈ X . GFlowNets aim to learn a sequential, constructive sampling policy 𝜋 that generates objects 𝑥 with probabilities proportional to their rewards, i.e., 𝜋 ( 𝑥 ) ∝ 𝑅 ( 𝑥 ) . This process can be represented as a directed acyclic graph (DAG) G = (S , A) , where the vertices 𝑠 ∈ S are referred to as states , and the directed edges ( 𝑢 → 𝑣 ) ∈ A are called actions . The generation of an object 𝑥 ∈ X corresponds to a complete trajectory 𝜏 = ( 𝑠 0 → · · · → 𝑠 𝑛 ) ∈ T within the DAG, beginning at the initial state 𝑠 0 and ending at a terminal state 𝑠 𝑛 ∈ X . The state flow 𝐹 ( 𝑠 ) is defined as a non-negative weight assigned to each state 𝑠 ∈ S . The forward policy 𝑃𝐹 ( 𝑠 ′ | 𝑠 ) specifies the transition probability to a child state 𝑠 ′ , while the backward policy 𝑃𝐵 ( 𝑠 | 𝑠 ′ ) specifies the transition probability to a parent state 𝑠 . To this end, detailed balance objective enforces local flow consistency across every edge ( 𝑠 → 𝑠 ′ ) ∈ A :\\n\\n<!-- formula-not-decoded -->\\n\\nTo achieve this flow consistency, GFlowNets employ training objectives at different levels of granularity, including detailed balance [Bengio et al., 2023b], trajectory balance [Malkin et al., 2022], and subtrajectory balance [Madan et al., 2023]. Leveraging their diversity-seeking behavior, GFlowNets have been successfully applied across a range of domains, including molecule generation [Cretu et al., 2024], diffusion fine-tuning [Liu et al., 2025b, Zhang et al., 2025a], and amortized reasoning [Hu et al., 2024, Yu et al., 2025a]. Among various training objective in GFlowNets, trajectory balance maintains flow consistency at the trajectory level, defined as:\\n\\n<!-- formula-not-decoded -->\\n\\nFurthermore, sub-trajectory balance achieves local balance on arbitrary subpaths 𝜏𝑖 : 𝑗 = { 𝑠 𝑖 → · · ·→ 𝑠 𝑗 } , offering a more stable and less biased learning signal. We build on trajectory balance to extend our KL-based objective through a gradient-equivalence formulation (Prop. 1), and further improve it to better support long CoT reasoning in RL.\\n\\nTable 5 | Math reasoning performance (Avg@64) at temperature = 0 . 6. Relative improvements are shown as subscripts, with positive gains in green and negative changes in red. FlowRL consistently outperforms all baselines and achieves the best average score under this low-temperature setting.\\n\\n| Models                | AIME 2024             | AIME 2025             | AMC 2023              | MATH-500              | Minerva               | Olympiad              | Avg                   |\\n|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\\n| Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model |\\n| Backbone              | 4.37                  | 2.08                  | 30.78                 | 54.48                 | 22.38                 | 24.02                 | 23.02                 |\\n| R++                   | 10 . 57 + 6 . 20      | 5 . 10 + 3 . 02       | 66 . 02 + 35 . 24     | 54 . 29 - 0 . 19      | 24 . 47 + 2 . 09      | 27 . 30 + 3 . 28      | 31.29                 |\\n| PPO                   | 9 . 95 + 5 . 58       | 7 . 34 + 5 . 26       | 63 . 63 + 32 . 85     | 57 . 72 + 3 . 24      | 26 . 22 + 3 . 84      | 27 . 35 + 3 . 33      | 32.03                 |\\n| GRPO                  | 14 . 01 + 9 . 64      | 10 . 73 + 8 . 65      | 64 . 10 + 33 . 32     | 57 . 41 + 2 . 93      | 23 . 17 + 0 . 79      | 27 . 11 + 3 . 09      | 32.76                 |\\n| FlowRL                | 14 . 32 + 9 . 95      | 10 . 05 + 7 . 97      | 55 . 08 + 24 . 30     | 66 . 78 + 12 . 30     | 31 . 52 + 9 . 14      | 34 . 60 + 10 . 58     | 35.39                 |\\n\\nTable 6 | Math reasoning performance (Avg@64) at temperature = 1 . 0. Relative improvements are shown as subscripts, with positive gains in green. FlowRL maintains robust performance under higher generation randomness and continues to outperform all baselines on average.\\n\\n| Models                | AIME 2024             | AIME 2025             | AMC 2023              | MATH-500              | Minerva               | Olympiad              | Avg                   |\\n|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\\n| Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model |\\n| Backbone              | 3.39                  | 1.51                  | 23.90                 | 45.18                 | 16.98                 | 18.27                 | 18.20                 |\\n| R++                   | 10 . 63 + 7 . 24      | 4 . 63 + 3 . 12       | 66 . 99 + 43 . 09     | 54 . 36 + 9 . 18      | 23 . 89 + 6 . 91      | 26 . 65 + 8 . 38      | 31.19                 |\\n| PPO                   | 10 . 52 + 7 . 13      | 6 . 51 + 5 . 00       | 63 . 04 + 39 . 14     | 57 . 46 + 12 . 28     | 25 . 91 + 8 . 93      | 27 . 16 + 8 . 89      | 31.77                 |\\n| GRPO                  | 12 . 50 + 9 . 11      | 10 . 10 + 8 . 59      | 64 . 72 + 40 . 82     | 57 . 15 + 11 . 97     | 23 . 28 + 6 . 30      | 26 . 90 + 8 . 63      | 32.44                 |\\n| FlowRL                | 14 . 22 + 10 . 83     | 9 . 58 + 8 . 07       | 52 . 92 + 29 . 02     | 66 . 20 + 21 . 02     | 30 . 32 + 13 . 34     | 34 . 47 + 16 . 20     | 34.62                 |\\n\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\n\\n| Models   |   AIME 2024 |   AIME 2025 |   AMC 2023 |   MATH-500 |   Minerva |   Olympiad |   Avg |\\n|----------|-------------|-------------|------------|------------|-----------|------------|-------|\\n| 𝛽 = 5    |       13.54 |       10    |      56.09 |      58.91 |     20.79 |      28.72 | 31.34 |\\n| 𝛽 = 10   |       14.79 |       10.2  |      59.53 |      64.3  |     25.27 |      32.39 | 34.41 |\\n| 𝛽 = 15   |       15.41 |       10.83 |      54.53 |      66.96 |     31.41 |      34.61 | 35.63 |\\n| 𝛽 = 30   |       15    |       10.83 |      50.62 |      69.02 |     30.03 |      35.03 | 35.09 |\\n\\nTable 7 | Ablation study on the effect of the 𝛽 parameter in FlowRL. We report Avg@16 accuracy across six math reasoning benchmarks for different values of 𝛽 .\\n\\n## Diversity Evaluation Prompt\\n\\nSystem: You are evaluating the DIVERSITY of solution approaches for a mathematics competition problem. Focus on detecting even SUBTLE differences in methodology that indicate different problemsolving strategies.\\n\\n## PROBLEM:\\n\\n{problem}\\n\\n## 16 SOLUTION ATTEMPTS:\\n\\n{formatted_responses}\\n\\n## EVALUATION CRITERIA - Rate diversity from 1 to 5:\\n\\n## Score 1 - Minimal Diversity:\\n\\n- Same mathematical setup, same variable choices, same solution path\\n- 14+ responses use essentially identical approaches\\n- Only trivial differences (arithmetic, notation, wording)\\n- Indicates very low exploration/diversity in the generation process\\n\\n## Score 2 - Low Diversity:\\n\\n- 1-2 alternative approaches appear but are rare\\n- 11-13 responses use the same main approach\\n- Minor variations within the dominant method (different substitutions, orderings)\\n- Some exploration but heavily biased toward one strategy\\n\\n## Score 3 - Moderate Diversity:\\n\\n- 2-3 distinct alternative approaches present\\n- 7-10 responses use the most common approach\\n- Noticeable variation in problem setup or mathematical techniques\\n- Balanced mix showing reasonable exploration\\n\\n## Score 4 - High Diversity:\\n\\n- 3-4 distinct solution strategies well-represented\\n- 4-6 responses use the most common approach\\n- Multiple mathematical techniques and problem framings\\n- Strong evidence of diverse exploration strategies\\n\\n## Score 5 - Maximum Diversity:\\n\\n- 4+ distinctly different solution strategies\\n- No single approach dominates ( ≤ 3 responses use same method)\\n- Wide variety of mathematical techniques and creative approaches\\n\\nIMPORTANT: Focusing on the DIVERSITY of the attempted approaches. Return ONLY a number from 1 to 5.\\n\\n- Excellent exploration and generation diversity\",\n",
       " 'sections': [{'title': 'Content',\n",
       "   'content': 'arXiv:2509.15207v1  [cs.LG]  18 Sep 2025\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\n2025-09-17'},\n",
       "  {'title': 'FlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       "   'content': 'Xuekai Zhu 1 , Daixuan Cheng 6 , Dinghuai Zhang 3 , Hengli Li 5 , Kaiyan Zhang 4 , Che Jiang 4 , Youbang Sun 4 , Ermo Hua 4 , Yuxin Zuo 4 , Xingtai Lv 4 , Qizheng Zhang 7 , Lin Chen 1 , Fanghao Shao 1 , Bo Xue 1 , Yunchong Song 1 , Zhenjie Yang 1 , Ganqu Cui 2 , Ning Ding 4 , 2 , Jianfeng Gao 3 , Xiaodong Liu 3 , Bowen Zhou 4 , 2 ‡ , Hongyuan Mei 8 ‡ , Zhouhan Lin 1 , 2 ‡\\n1 Shanghai Jiao Tong University 2 Shanghai AI Laboratory 3 Microsoft Research 4 Tsinghua University 5 Peking University 6 Renmin University of China 7 Stanford University 8 Toyota Technological Institute at Chicago\\n/envelope hongyuanmei@gmail.com /envelope xuekaizhu0@gmail.com /github FlowRL ‡ Corresponding Authors.\\nAbstract | We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods ( e.g. , PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10 . 0% over GRPO and 5 . 1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.\\nFigure 1 | Top : Comparison between distribution-matching and reward-maximizing approaches. FlowRL (left) learns to match the full reward distribution, maintaining diversity across multiple modes with low KL divergence. In contrast, reward-maximizing methods like GRPO (right) concentrate on a single high-reward peak, leading to mode collapse and higher KL divergence. Bottom : Performance comparison. FlowRL consistently outperforms GRPO across math and code domains.\\nDistribution-matching: FlowRL\\nKL = 0.11\\nKL = 8.68\\nReward-maximizing\\n∶\\nR++, PPO and GRPO\\nMath Average Score\\nCodeForces Rating\\nFlowRL: Matching Reward Distributions for LLM Reasoning'},\n",
       "  {'title': '1. Introduction',\n",
       "   'content': \"Reinforcement learning (RL) plays a crucial role in the post-training of large language models (LLMs) [Zhang et al., 2025b]. A series of powerful reasoning models [Guo et al., 2025, Kavukcuoglu, 2025, Rastogi et al., 2025] have employed large-scale reinforcement learning to achieve strong performance on highly challenging benchmarks [He et al., 2024]. The evolution of RL algorithms for LLM reasoning has progressed through several key stages: REINFORCE [Sutton et al., 1999a] provides a solid baseline that is easy to implement and efficient in simple settings; PPO [Schulman et al., 2017] improves upon REINFORCE with better stability and efficiency in complex settings; GRPO [Shao et al., 2024] simplifies PPO training by eliminating value functions and relying on group comparisons, though at the cost of requiring more rollouts per update. However, all these methods share a fundamental limitation in their reward-maximizing objective.\\nReward-maximizing RL methods tend to overfit to the dominant mode of the reward distribution [Gao et al., 2023, Pan et al., 2022, Skalse et al., 2022, Zelikman et al., 2022]. This often results in limited diversity among generated reasoning paths and reduces generalization to less frequent yet valid logical outcomes [Hu et al., 2023]. As illustrated in Figure 1, GRPO neglects other meaningful modes. These drawbacks become especially pronounced in complex long chain-of-thought (CoT; Wei et al., 2022) reasoning, where capturing a diverse distribution of plausible solutions is essential for effective generalization [Liu et al., 2025a]. Recent approaches adjust the clip ratio [Yu et al., 2025b], augment the advantage function with an entropy-based term [Cheng et al., 2025], or selectively promote high-entropy tokens [Wang et al., 2025], thereby dynamically adapting the training data distribution and implicitly increasing diversity during training. This raises a fundamental question: How can we promote diverse exploration to prevent convergence to dominant solution patterns in RL training?\\nIn this paper, we propose FlowRL , a policy optimization algorithm that aligns the policy model with the full reward distribution, encouraging mode coverage. FlowRL achieves more efficient exploration by fundamentally shifting from reward maximization to reward distribution matching, thereby addressing the inherent mode-collapse limitations of previous RL approaches. As illustrated in Figure 1, the core idea of FlowRL is to introduce a learnable partition function that normalizes scalar rewards into a target distribution, and to minimize the reverse KL divergence between the policy and this reward-induced distribution. We develop this KL objective based on the trajectory balance formulation from GFlowNets [Bengio et al., 2023b], providing a gradient equivalence proof that bridges generative modeling and policy optimization. To address the challenges of long CoT training, we introduce two key technical solutions: length normalization to tackle gradient explosion issues that occur with variable-length CoT reasoning, and importance sampling to correct for the distribution mismatch between generated rollouts and the current policy.\\nWe compare FlowRL with mainstream RL algorithms including REINFORCE++, PPO, and GRPO across math and code domains, using both base and distilled LLMs (7B, 32B). In math domain, FlowRL outperforms GRPO and PPO by 10 . 0% and 5 . 1%, respectively, demonstrating consistent improvements across six challenging math benchmarks. Furthermore, FlowRL surpasses both PPO and GRPO on three challenging coding benchmarks, highlighting its strong generalization capabilities in code reasoning tasks. To understand what drives these performance gains, we analyze the diversity of generated reasoning paths. This diversity analysis confirms that FlowRL generates substantially more diverse rollouts than baseline methods, validating our approach's effectiveness in exploring multiple solution strategies.\\nContributions. We summarize the key contributions of this work as follows:\\nWe propose FlowRL, a policy optimization algorithm that shifts from reward maximization to\\n2\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nreward distribution matching via flow balance, encouraging diverse reasoning path exploration while addressing the inherent mode-collapse limitations of existing RL methods.\\nFlowRL outperforms GRPO and PPO by 10.0% and 5.1% respectively across math benchmarks and demonstrates strong generalization on code reasoning tasks, with diversity analysis confirming substantially more diverse solution exploration.\\nWe introduce length normalization and importance sampling to enable effective training on variablelength CoT reasoning, addressing gradient explosion and sampling mismatch issues.\"},\n",
       "  {'title': '2. Preliminaries',\n",
       "   'content': 'Reinforcement Learning for Reasoning. Weformulate reasoning as a conditional generation problem, where the policy model receives a question x ∈ X and generates an answer y ∈ Y . The objective is to learn a policy 𝜋𝜃 ( y | x ) that produces high-quality answers under task-specific reward signals 𝑟 . To better illustrate the policy optimization procedure, we provide a detailed formulation of GRPO below. For each question x , GRPO samples a group of answers { y 1 , y 2 , . . . , y 𝐺 } from old policy 𝜋𝜃𝑜𝑙𝑑 and updates the model by maximizing the following objective:\\nwhere 𝜖 and 𝜆 are hyper-parameters. Here, 𝐴𝑖 denotes the advantage, computed by normalizing the group reward values { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } as 𝐴𝑖 = 𝑟 𝑖 -mean ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) std ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) . Compared to GRPO, REINFORCE applies the policy gradient directly, without advantage normalization, clipping, or KL regularization. PPO uses a critic model to estimate the advantage and employs importance sampling to stabilize policy updates.\\nGFlowNets. Generative Flow Networks [Bengio et al., 2023a] are a probabilistic framework for training stochastic policies to sample discrete, compositional objects ( e.g. , graphs, sequences) in proportion to a given reward. As shown in Figure 2, the core principle of GFlowNets is to balance the forward and backward probability flows at each state, inspired by flow matching [Bengio et al., 2021]. The initial flow is estimated by 𝑍𝜙 ( 𝑠 0 ) at the initial state 𝑠 0 . The output flow is equal to the outcome reward 𝑟 ( 𝑠 𝑛 ) conditioned at the final state 𝑠 𝑛 . Following Lee et al. [2024], we use a 3-layer MLP to parameterize 𝑍𝜙 . This flow-balancing mechanism facilitates the discovery of diverse,\\nFigure 2 | GFlowNets [Bengio et al., 2023a], a flow-balance perspective on reinforcement learning. The initial flow 𝑍𝜙 ( 𝑠 0 ) injects probability mass into the environment, which is transported through intermediate states by the policy 𝜋𝜃 and accumulated at terminal states in proportion to the scalar rewards.\\n𝑠\\n!\\n𝑠\\n\"\\n𝑠\\n#\\n𝑠\\n$\\n𝑠\\n%\\n𝑠\\n&\\n𝑠\\n\\'\\n𝑠\\n(\\n𝑠\\n)\\n𝑠\\n*\\n𝑠\\n\"!\\n𝑠\\n+\\n𝑠\\n+\\n𝑆\\n!\\n𝑠\\n+\\n𝑠\\n+\\n𝑠\\n+\\nIn Flow\\nZ\\n\"\\n𝑠\\n#\\nOut Flow r\\n(𝜏)\\nhigh-reward solutions by ensuring proper exploration of the solution space. See Appendix C for detailed GFlowNets background.\\n3\\nFlowRL: Matching Reward Distributions for LLM Reasoning'},\n",
       "  {'title': '3. Methodology',\n",
       "   'content': 'In this section, we first formulate distribution matching in reinforcement learning through reverse KL divergence and establish its connection to trajectory balance from GFlowNets. To address the challenges of gradient explosion and sampling mismatch encountered during long CoT training, we further incorporate length normalization and importance sampling. Using this enhanced framework, we derive a flow-balanced objective, termed FlowRL .'},\n",
       "  {'title': '3.1. From Reward Maximization to Distribution Matching',\n",
       "   'content': 'As illustrated in Figure 1, recent powerful large reasoning models typically employ reward-maximizing RL algorithms, such as PPO or GRPO. However, these methods tend to optimize toward the dominant reward mode, frequently resulting in mode collapse and the neglect of other plausible, high-quality reasoning paths. To address this fundamental limitation, we propose optimizing the policy by aligning its output distribution to a target reward distribution. A simple yet effective way to achieve this is to minimize the reverse KL divergence 1 between the policy and this target. However, in long CoT reasoning tasks, the available supervision in RL is a scalar reward, rather than a full distribution. Moreover, enumerating or sampling all valid trajectories to recover the true reward distribution is computationally intractable.\\nInspired by energy-based modeling [Du and Mordatch, 2019, Hinton et al., 1995], we introduce a learnable partition function 𝑍𝜙 ( x ) to normalize scalar rewards into a valid target distribution. This allows us to minimize the reverse KL divergence between the policy and the reward-weighted distribution, formalized as:\\nwhere 𝑟 ( x , y ) is the reward function, 𝛽 is a hyperparameter, 𝑍𝜙 ( x ) is the learned partition function, and the resulting target distribution is defined as ˜ 𝜋 ( y | x ) = exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) . This objective encourages the policy to sample diverse, high-reward trajectories in proportion to their rewards, rather than collapsing to dominant modes as in standard reward maximization.\\nWhile the KL-based formulation provides a principled target distribution, we derive a more practical, RL-style objective that facilitates efficient policy optimization.\\nProposition 1. In terms of expected gradients, minimizing the KL objective in Eq. 2 is equivalent to minimizing the trajectory balance loss used in GFlowNet [Bartoldson et al., 2025, Lee et al., 2024, Malkin et al., 2022, 2023]:\\nRemark 2 ( Trajectory balance as a practical surrogate for KL minimization ) . Given the equivalence established in Proposition 1, the KL-based distribution matching objective can be reformulated as the trajectory balance loss. This reformulation provides a practical optimization approach by using a stable squared loss form rather than direct KL optimization, and by treating 𝑍𝜙 ( x ) as a learnable parameter rather than requiring explicit computation of the intractable partition function. The trajectory balance objective thus serves as a tractable surrogate for reward-guided KL minimization that can be directly integrated into existing RL frameworks.\\n1 We use reverse KL since we can only sample from the policy model, not the target reward distribution.\\n4\\nFlowRL: Matching Reward Distributions for LLM Reasoning'},\n",
       "  {'title': '3.2. FlowRL',\n",
       "   'content': 'As established in Proposition 1, the target reward distribution can be approximated by optimizing the trajectory balance objective. However, applying this objective directly to long CoT reasoning introduces two key challenges:\\nProblem I: Exploding gradients from long trajectories. Trajectory balance is a sequence-level objective, and applying it to long CoT reasoning with up to 8K tokens leads to exploding gradients and unstable updates. This issue is not observed in prior GFlowNets works, which typically operate on short trajectories in small discrete spaces. Specifically, the log-probability term log 𝜋𝜃 ( y | x ) decomposes into a token-wise sum, ˝ 𝑡 log 𝜋𝜃 ( y 𝑡 | y <𝑡 , x ) , causing the gradient norm to potentially scale with sequence length.\\nProblem II: Sampling mismatch. Mainstream RL algorithms such as PPO and GRPO commonly perform micro-batch updates and reuse trajectories collected from an old policy 𝜋𝜃 old , enabling data-efficient training. In contrast, the KL-based trajectory balance objective assumes fully onpolicy sampling, where responses are drawn from the current policy. This mismatch poses practical limitations when integrating trajectory balance into existing RL pipelines.\\nThese limitations motivate our reformulation that retains the benefits of distribution matching while addressing key practical challenges. To enable this reformulation, we first redefine the reward function following established practices in GFlowNets literature [Bartoldson et al., 2025, Lee et al., 2024, Yu et al., 2025a] by incorporating a reference model as a prior constraint on the reward distribution. Specifically , we modify the original exp ( 𝛽𝑟 ( x , y )) to include the reference model:\\nwhere 𝑟 ( x , y ) denotes the outcome reward commonly used in reinforcement learning and 𝜋 ref is the initial pre-trained model. We follow Guo et al. [2025] to use outcome-based reward signals, and apply group normalization to 𝑟 ( x , y ) as ˆ 𝑟 𝑖 = ( 𝑟 𝑖 -mean ( r ))/ std ( r ) , where r = { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } denotes the set of rewards within a sampled group. By substituting the redefined reward formulation Eq. 4 into Eq. 3, we derive the following objective 2 :\\nRemark 3 ( Reward shaping via length normalization ) . Trajectory balance treats both the initial flow and the outcome reward as sequence-level quantities. In contrast, standard policy optimization methods such as PPO or GRPO assign rewards at the token level and compute gradients at each step. However, for trajectories of varying lengths ( e.g. , CoT responses), this mismatch can cause the log-probability term log 𝜋𝜃 ( y | x ) = ˝ | y | 𝑡 = 1 log 𝜋𝜃 ( 𝑦 𝑡 | 𝑦 <𝑡 , x ) to scale with sequence length. To address this, we apply a form of reward shaping by normalizing log-probabilities with respect to sequence length. Specifically, we rescale the term as 1 | y | log 𝜋𝜃 ( y | x ) , balancing the contributions of long and short sequences and stabilizing the learning signal.\\nRemark 4 ( Importance sampling for data-efficient training ) . To mitigate sampling mismatch, we employ importance sampling inspired by PPO to stabilize policy updates with off-policy data. We re-weight stale trajectories using the importance ratio 𝑤 = 𝜋𝜃 ( y | x )/ 𝜋 old ( y | x ) , which serves as a coefficient in the surrogate loss. Since our objective focuses on optimizing trajectory balance rather than expected return, we detach the gradient from the current policy to prevent excessive policy drift: 𝑤 = detach [ 𝜋𝜃 ( y | x )]/ 𝜋 old ( y | x ) . For additional stability, we incorporate PPO-style clipping to detach\\n.\\n2 The substitution replaces 𝛽𝑟 ( x , y ) in trajectory balance objective Eq. 3 with 𝛽𝑟 ( x , y ) + log 𝜋 ref ( y | x ) to incorporate the reference model constraint.\\n5\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nIncorporating these improvements into Eq. 5, we arrive at the following FlowRL objective:\\nFlowRL\\nwhere the clipped importance weight 𝑤 and normalized reward ˆ 𝑟 ( x , y ) are defined as:\\nWe use this objective to update the policy parameters 𝜃 during training, and refer to this strategy as FlowRL . Implementation details and theoretical analysis are provided in § 5 and § B, respectively.'},\n",
       "  {'title': '4.1. Reinforcement Learning for Reasoning',\n",
       "   'content': \"Reinforcement learning has emerged as a powerful approach for large language models post-training on reasoning tasks [Guo et al., 2025, Lightman et al., 2023b, Schulman et al., 2017, Shao et al., 2024, Sutton et al., 1999b]. Most approaches employ reward-maximizing RL to optimize expected cumulative returns. Entropy regularization [Ahmed et al., 2019, Cheng et al., 2025, Haarnoja et al., 2018] is a classical technique for mitigating mode collapse by promoting diversity in the policy's output distribution, and has also been shown to enhance reasoning capabilities in various settings [Chao et al., 2024, Eysenbach and Levine, 2021]. However, for long CoT reasoning, the extended trajectory length (e.g., 8k-16k tokens) makes it difficult for the regularization signal to effectively influence reward-maximizing learning. Recent work [Cheng et al., 2025, Cui et al., 2025, Dong et al., 2025, Wang et al., 2025] has discovered that training with more diverse or high-entropy training data can further enhance training effectiveness. Compared to traditional entropy regularization, the above methods explicitly increase the proportion of low-probability (i.e., high-entropy) tokens in the training data. In our work, we address the mode-collapse problem by fundamentally shifting from reward maximization to reward distribution matching in our RL formulation.\"},\n",
       "  {'title': '4.2. GFlowNets',\n",
       "   'content': 'GFlowNets [Bengio et al., 2023a] represent a class of diversity-driven algorithms designed to balance probability flows across states. They have rich connections to probabilistic modeling methods [Ma et al., Malkin et al., 2023, Zhang et al., 2022a,b, 2024a, Zimmermann et al., 2022], and control methods [Pan et al., 2023b,c,d, Tiapkin et al., 2024, Zhang et al., 2024b]. This advantage has enabled GFlowNets to achieve successful applications in multiple downstream tasks, such as molecular drug discovery [Jain et al., 2022, 2023a,b, Kim et al., 2023, 2024, Liu et al., 2022, Pan et al., 2023a, Shen et al., 2023], phylogenetic inference [Zhou et al., 2024], and combinatorial optimization [Zhang et al., 2023a,b]. For generative AI, GFlowNets provide a powerful approach to align pretrained models in scenarios such as image generation [Yun et al., 2025, Zhang et al., 2025a] and language model fine-tuning [Hu et al., 2024, Lee et al., 2024, Yu et al., 2025a]. Another line of work primarily focuses on the theoretical aspects of GFlowNets. Recent theoretical studies have interpreted GFlowNets as solving a maximum entropy reinforcement learning problem within a modified Markov Decision Process (MDP) [Deleu et al., 2024, Mohammadpour et al., 2024, Tiapkin et al., 2024]. These theoretical contributions have\\n6\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\ninspired us to enhance reinforcement learning from a more foundational standpoint using GFlowNets principles. A comprehensive overview of GFlowNets theory can be found in Appendix C.'},\n",
       "  {'title': '4.3. Flow-Matching Policies',\n",
       "   'content': 'Flow matching simplifies diffusion-based approaches by learning vector fields that transport samples from prior to target distributions [Lipman et al., 2023]. Recent work has explored flow matching for policy optimization. McAllister et al. [2025] reformulates policy optimization using advantageweighted ratios from conditional flow matching loss, enabling flow-based policy training without expensive likelihood computations. Pfrommer et al. [2025] explored reward-weighted flow matching for improving policies beyond demonstration performance. Park et al. [2025] uses a separate one-step policy to avoid unstable backpropagation through time when training flow policies with RL. Zhang et al. [2025a] proposed a combined loss function integrating PPO and GFlowNets to optimize diffusion model alignment. However, these approaches focus on continuous control, image generation, or vision-action models, rather than addressing mode-collapse limitations in reward-maximizing RL. Inspired by flow matching principles, our work improves upon RL training to enhance training stability while promoting diverse solution exploration.'},\n",
       "  {'title': '5. Experiment Settings',\n",
       "   'content': 'Backbone Models. There are two learnable modules in Eq. 6: the policy model 𝜋𝜃 and the partition function 𝑍𝜙 . For the policy model 𝜋𝜃 , we use Qwen-2.5-7B/32B [Team, 2024] for math tasks and DeepSeek-R1-Distill-Qwen-7B [DeepSeek-AI, 2025] for code tasks, respectively. For partition function 𝑍𝜙 , following Lee et al. [2024], we use a randomly initialized 3-layer MLP with hidden dimensions matching those of the base model. The reference model 𝜋 ref is the corresponding fixed pretrained model. All training scripts are based on the veRL [Sheng et al., 2024]. For the reward function, following Lee et al. [2024], we set the hyperparameter 𝛽 = 15.\\nBaselines. We compare our method against three representative reward-maximization RL baselines: REINFORCE++ (R++; Hu et al., 2025, Sutton et al., 1999b), PPO [Schulman et al., 2017], and GRPO [Shao et al., 2024]. All baselines follow the official veRL recipes, with consistent training configurations. For fair comparison, all methods use the same learning rate, batch size, and training steps, and are evaluated at convergence using identical step counts.\\nTraining Configuration. We experiment on both math and code domains. For the math domain, we use the training set collected from DAPO [Yu et al., 2025b]. For the code domain, we follow the setup of DeepCoder [Luo et al., 2025], using their training set. For 7B model training, we use a single node equipped with 8 NVIDIA H800 GPUs (80GB memory each). For 32B model training, we scale to 4 nodes with 32 GPUs to accommodate the larger memory requirements. All experiments use max_prompt_length = 2048 and max_response_length = 8192 across both model sizes. We use a batch size of 512 for math reasoning tasks and 64 for code reasoning tasks. We set the learning rate to 1e-6 and enable dynamic batch sizing in veRL for efficient training. For GRPO and FlowRL, we configure rollout_n = 8, meaning each prompt generates 8 response rollouts as the group size.\\nEvaluation Configuration. For the math domain, we evaluate on six challenging benchmarks: AIME 2024/2025 [MAA, 2025], AMC 2023 [MAA, 2023], MATH-500 [Lightman et al., 2023a], Minerva [Lewkowycz et al., 2022], and Olympiad [He et al., 2024]. For the code domain, we evaluate on LiveCodeBench [Jain et al., 2024], CodeForces [Penedo et al., 2025], and HumanEval+ [Chen et al., 2021]. For all evaluation datasets, we perform 16 rollouts and report the average accuracy, denoted as Avg@16. We further report rating and percentile for Codeforces. During generation, we\\n7\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 1 | Results on math benchmarks. We report Avg@16 accuracy with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL outperforms all baselines across both 7B and 32B model scales.\\nTable 2 | Results on code benchmarks. We report metrics with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL achieves the strongest performance across all three benchmarks, demonstrating its effectiveness in code reasoning tasks.\\nuse sampling parameters of temperature =0.6 and top_p =0.95 for all evaluations. The response length for evaluation is set to 8,192, consistent with the training configuration.'},\n",
       "  {'title': '6.1. Main Results',\n",
       "   'content': \"Our experimental results, summarized in Table 1 and Table 2, demonstrate that FlowRL consistently outperforms all reward-maximization baselines across both math and code reasoning domains. Table 1 reports results on math reasoning benchmarks using both 7B and 32B base models, while Table 2 presents the corresponding results on code reasoning tasks. On math reasoning tasks, FlowRL achieves the highest average accuracy of 35.6% with the 7B model and 48.4% with the 32B model, surpassing PPO by 5.1% and GRPO by 10.1% on the 32B model. FlowRL shows strong improvements on challenging benchmarks like MATH-500 and Olympiad problems, demonstrating consistent gains\\n8\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 3 | Ablation study on FlowRL with Qwen2.5-7B as the base model. Avg@16 accuracy is reported across six math reasoning benchmarks. IS denotes importance sampling.\\nacross diverse mathematical domains. On code generation tasks, FlowRL achieves compelling improvements with the highest Avg@16 score of 37.43% on LiveCodeBench, a Codeforces rating of 1549.47 with 83.3% percentile ranking, and 83.28% accuracy on HumanEval+, outperforming all baselines across the board. These consistent performance gains across both domains and model scales provide strong empirical evidence that FlowRL's flow-balanced optimization successfully enhances generalization. This improvement comes from promoting diverse solution exploration compared to previous reward-maximizing RL approaches.\"},\n",
       "  {'title': '6.2. Ablation Studies',\n",
       "   'content': 'We conduct ablation studies on importance sampling and the 𝛽 hyperparameter. For importance sampling, we compared the performance with and without it, and implemented a combined loss approach proposed by Zhang et al. [2025a] that simultaneously optimizes both GFlowNets and PPO objectives. This combined loss focuses on optimizing diffusion models, and we adapt it to long CoT reasoning tasks for comparison. Table 3 demonstrates that importance sampling substantially improves FlowRL performance across all math reasoning benchmarks. Compared to Zhang et al. [2025a], using importance sampling as a trajectory-level ratio is more suitable than the combined loss of GFlowNets and PPO. The performance drop without importance sampling (from 35.63% to 26.71%) highlights the critical role of correcting for distribution mismatch between rollout generation and policy training. For the hyperparam-\\nFigure 3 | Ablation study on the 𝛽 in FlowRL. 𝛽 = 15 (highlighted in blue) achieves the best performance.\\n=5\\n=10\\n=15\\n=30\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\nAverage Score (%)\\n31.34\\n34.41\\n35.63\\n35.09\\neter 𝛽 , we conduct a series of parameter ablation studies, and Figure 3 shows that 𝛽 = 15 achieves optimal performance, with detailed results shown in Table 7.'},\n",
       "  {'title': '7.1. Diversity Analysis',\n",
       "   'content': 'To assess solution diversity , we follow the approach of Yu et al. [2025a] and employ GPT-4o-mini [OpenAI, 2024] to evaluate all responses generated by each method on AIME 24/25. The evaluation prompt is shown in Appendix C. As shown in Figure 4, FlowRL achieves higher diversity scores compared to baseline methods. This demonstrates that FlowRL improves sample diversity compared to baselines, which tend to exhibit repetitive solution patterns. This diversity evaluation reveals\\n9\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 4 | Case study comparing GRPO and FlowRL rollouts on an AIME problem. GRPO exhibits repetitive patterns (AM-GM × 3, identity loops × 2), while FlowRL follows a more diverse solution path.'},\n",
       "  {'title': 'Question',\n",
       "   'content': \"Let B be the set of rectangular boxes with surface area 54 and volume 23. Let 𝑟 be the radius of the smallest sphere that can contain each box in B . If 𝑟 2 = 𝑝 𝑞 with gcd ( 𝑝, 𝑞 ) = 1, find 𝑝 + 𝑞 .\\nGRPO\\n'. . .\\ndenote\\n𝑎, 𝑏, 𝑐\\n. . .\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n=\\n54,\\n𝑎𝑏𝑐\\n=\\n23 ...\\n𝑑\\n=\\n√\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2 ,\\n𝑟\\n=\\n𝑑\\n/\\n2\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n=\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n+\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n. . . AM-GM\\n×\\n3 :\\nAM-GM (1)\\n. . .\\nAM-GM (2)\\n. . .\\nAM-GM (3)\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n3\\nidentity loop\\n×\\n2 :\\nloop (1)\\n. . .\\nloop (2)\\n. . .\\n𝑎\\n=\\n𝑏\\n=\\n𝑐\\n(contradiction) .. .\\nback to\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n. . .\\nno factorization . . . '\\nFlowRL\\n'. . .\\nlet\\n𝑎, 𝑏, 𝑐\\nwith\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n=\\n54,\\n𝑎𝑏𝑐\\n=\\n23\\n. . .\\n𝑑\\n=\\n√\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2 ,\\n𝑟\\n=\\n𝑑\\n/\\n2\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n⇒\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n=\\n𝑠\\n2\\n-\\n54\\n. . .\\n𝑎\\n=\\n𝑏\\n. . .\\n𝑎\\n3\\n-\\n27\\n𝑎\\n+\\n46\\n=\\n0\\n. . .\\nrational root\\n𝑎\\n=\\n2 ...\\nfactor\\n(\\n𝑎\\n-\\n2\\n)(\\n𝑎\\n2\\n+\\n2\\n𝑎\\n-\\n23\\n)\\n. . .\\nbranch\\n𝑎\\n=\\n-\\n1\\n+\\n2\\n√\\n6 ...\\nback-sub\\n𝑐\\n=\\n23\\n/\\n𝑎\\n2\\n. . .\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n=\\n657\\n16\\n. . .\\n𝑟\\n2\\n=\\n657\\n64\\n. . .\\nAnswer 721 ...'\\nsignificant differences in exploration patterns across methods. This nearly doubling of diversity score compared to the strongest baseline (PPO) indicates that FlowRL generates qualitatively different solution approaches rather than minor variations of the same strategy. The diversity analysis provides empirical validation of our core hypothesis that flow-balanced optimization promotes mode coverage in complex reasoning tasks.\"},\n",
       "  {'title': '7.2. Case Study',\n",
       "   'content': \"Table 4 illustrates the behavioral differences between GRPO and FlowRL on a representative AIME problem. GRPO exhibits repetitive patterns, applying AMGM three times and getting stuck in identity loops, failing to solve the problem. FlowRL explores more diverse actions: it sets 𝑎 = 𝑏 , derives a cubic equation, finds the rational root, and reaches the correct answer. This shows that FlowRL successfully avoids the repetitive exploration patterns. The contrast reveals fundamental differences in exploration strategies: GRPO's reward-maximizing approach leads to exploitation of familiar techniques (AM-GM inequality) without exploring alternatives, eventually reaching contradictory conclusions like 𝑎 = 𝑏 = 𝑐 . In contrast, FlowRL's distribution-matching enables strategic decisions such as the symmetry assumption 𝑎 = 𝑏 , which\\nFigure 4 | GPT-judged diversity scores on rollouts of AIME 24/25 problems. FlowRL generates more diverse solutions than R++, GRPO, and PPO.\\nR++\\nGRPO\\nPPO\\nFlowRL\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nDiversity Score\\n1.11\\n1.23\\n1.31\\n2.28\\ntransforms the problem into a tractable cubic equation 𝑎 3 -27 𝑎 + 46 = 0, allowing systematic solution through rational root testing and polynomial factorization.\\n10\\nFlowRL: Matching Reward Distributions for LLM Reasoning\"},\n",
       "  {'title': '8. Conclusion',\n",
       "   'content': 'In this work, we introduce FlowRL, which transforms scalar rewards into normalized target distributions using a learnable partition function and minimizes the reverse KL divergence between the policy and target distribution. We demonstrate that this approach is theoretically equivalent to trajectory balance objectives from GFlowNets and implicitly maximizes both reward and entropy, thereby promoting diverse reasoning trajectories. To further address gradient explosion and sampling mismatch issues in long CoT reasoning, we incorporate importance sampling and length normalization. Through experiments on math and code reasoning benchmarks, FlowRL achieves consistent improvements across all tasks compared to GRPO and PPO. Our diversity analysis and case studies confirm that FlowRL generates more varied solution approaches while avoiding repetitive patterns.'},\n",
       "  {'title': 'Acknowledgments',\n",
       "   'content': 'We are grateful to Mingqian Feng and Yuetai Li for their valuable discussions and feedback, which helped improve the quality of this work.'},\n",
       "  {'title': 'References',\n",
       "   'content': \"Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of entropy on policy optimization. In International conference on machine learning , pages 151-160. PMLR, 2019.\\nBrian R Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, and Bhavya Kailkhura. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable llm post-training. arXiv preprint arXiv:2503.18929 , 2025.\\nEmmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS) , 2021.\\nYoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research , 24(210):1-55, 2023a. URL http: //jmlr.org/papers/v24/22-0364.html .\\nYoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. The Journal of Machine Learning Research , 24(1):10006-10060, 2023b.\\nChen-Hao Chao, Chien Feng, Wei-Fang Sun, Cheng-Kuang Lee, Simon See, and Chun-Yi Lee. Maximum entropy reinforcement learning via energy-based normalizing flow. arXiv preprint arXiv:2405.13629 , 2024.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\\n11\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.\\nDaixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758 , 2025.\\nMiruna Cretu, Charles Harris, Ilia Igashov, Arne Schneuing, Marwin Segler, Bruno Correia, Julien Roy, Emmanuel Bengio, and Pietro Liò. Synflownet: Design of diverse and novel molecules with synthesis constraints. arXiv preprint arXiv:2405.01155 , 2024.\\nGanqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617 , 2025.\\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 .\\nTristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, and Yoshua Bengio. Discrete probabilistic inference as control in multi-path environments. arXiv preprint arXiv:2402.10309 , 2024.\\nGuanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849 , 2025.\\nYilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in neural information processing systems , 32, 2019.\\nBenjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257 , 2021.\\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning , pages 10835-10866. PMLR, 2023.\\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning , pages 1861-1870. Pmlr, 2018.\\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 , 2024.\\nHaoran He, Can Chang, Huazhe Xu, and Ling Pan. Looking backward: Retrospective backward synthesis for goal-conditioned GFlownets. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?id=fNMKqyvuZT .\\nGeoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and R M Neal. The 'wake-sleep' algorithm for unsupervised neural networks. Science , 268 5214:1158-61, 1995.\\nEdward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. arXiv preprint arXiv:2310.04363 , 2023.\\n12\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nEdward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/f orum?id=Ouj6p4ca60 .\\nJian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv. org/abs/2501 , 3262:32-33, 2025.\\nMoksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena Simine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. International Conference on Machine Learning (ICML) , 2022.\\nMoksh Jain, Tristan Deleu, Jason S. Hartford, Cheng-Hao Liu, Alex Hernández-García, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. ArXiv , abs/2302.00615, 2023a. URL https: //api.semanticscholar.org/CorpusID:256459319 .\\nMoksh Jain, Sharath Chandra Raparthy, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective GFlowNets. International Conference on Machine Learning (ICML) , 2023b.\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.\\nKoray Kavukcuoglu. Gemini 2.5: Our most intelligent AI model, 2025. URL https://blog.goo gle/technology/google-deepmind/gemini-model-thinking-updates-march-2025/ . Google Blog (The Keyword), Published Mar. 25, 2025.\\nMinsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search gflownets. ArXiv , abs/2310.02710, 2023.\\nMinsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan, Woochang Kim, Jinkyoo Park, Emmanuel Bengio, and Yoshua Bengio. Learning to scale logits for temperature-conditional gflownets, 2024.\\nSeanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et al. Learning diverse attacks on large language models for robust red-teaming and safety tuning. arXiv preprint arXiv:2405.18540 , 2024.\\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 3843-3857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/18abb eef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf .\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050 , 2023a.\\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. In The Twelfth International Conference on Learning Representations , 2023b.\\n13\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nYaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t .\\nDianbo Liu, Moksh Jain, Bonaventure F. P. Dossou, Qianli Shen, Salem Lahlou, Anirudh Goyal, Nikolay Malkin, Chris C. Emezue, Dinghuai Zhang, Nadhir Hassen, Xu Ji, Kenji Kawaguchi, and Yoshua Bengio. Gflowout: Dropout with generative flow networks. In International Conference on Machine Learning , 2022.\\nMingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507 , 2025a.\\nZhen Liu, Tim Z Xiao, , Weiyang Liu, Yoshua Bengio, and Dinghuai Zhang. Efficient diversity-preserving diffusion alignment via gradient-informed gflownets. In ICLR , 2025b.\\nMichael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, Ion Stoica, and Tianjun Zhang. Deepcoder: A fully open-source 14b coder at o3-mini level, 2025. Notion Blog.\\nJiangyan Ma, Emmanuel Bengio, Yoshua Bengio, and Dinghuai Zhang. Baking symmetry into gflownets.\\nMAA. American mathematics competitions - amc. https://maa.org/ , 2023.\\nMAA. American invitational mathematics examination - aime. https://maa.org/ , 2025.\\nKanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning , pages 23467-23483. PMLR, 2023.\\nNikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. Advances in Neural Information Processing Systems , 35: 5955-5967, 2022.\\nNikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. International Conference on Learning Representations (ICLR) , 2023.\\nDavid McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053 , 2025.\\nSobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropy gflownets with soft q-learning. In International Conference on Artificial Intelligence and Statistics , pages 2593-2601. PMLR, 2024.\\nOpenAI. Gpt-4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-effic ient-intelligence/ , 2024. Accessed: 2024.\\nAlexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544 , 2022.\\nLing Pan, Moksh Jain, Kanika Madan, and Yoshua Bengio. Pre-training and fine-tuning generative flow networks, 2023a.\\n14\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nLing Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. Better training of GFlowNets with local credit and incomplete trajectories. International Conference on Machine Learning (ICML) , 2023b.\\nLing Pan, Dinghuai Zhang, Aaron Courville, Longbo Huang, and Yoshua Bengio. Generative augmented flow networks. International Conference on Learning Representations (ICLR) , 2023c.\\nLing Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic generative flow networks. Uncertainty in Artificial Intelligence (UAI) , 2023d.\\nSeohong Park, Qiyang Li, and Sergey Levine. Flow q-learning. In Forty-second International Conference on Machine Learning , 2025. URL https://openreview.net/forum?id=KVf2SFL1pi .\\nGuilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/open-r1/codeforces , 2025.\\nSamuel Pfrommer, Yixiao Huang, and Somayeh Sojoudi. Reinforcement learning for flow-matching policies. arXiv preprint arXiv:2507.15073 , 2025.\\nAbhinav Rastogi, Albert Q Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910 , 2025.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\\nMax W. Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet training. ArXiv , abs/2305.07170, 2023. URL https://api.semanticscholar.org/CorpusID:258676487 .\\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 , 2024.\\nJoar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460-9471, 2022.\\nRichard S Sutton, Andrew G Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience , 11(1):126-134, 1999a.\\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. Müller, editors, Advances in Neural Information Processing Systems , volume 12. MIT Press, 1999b. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0b ed98e80ade0a5c43b0f-Paper.pdf .\\nQwen Team. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.g ithub.io/blog/qwen2.5/ .\\n15\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nDaniil Tiapkin, Nikita Morozov, Alexey Naumov, and Dmitry P Vetrov. Generative flow networks as entropy-regularized rl. In International Conference on Artificial Intelligence and Statistics , pages 4213-4221. PMLR, 2024.\\nShenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939 , 2025.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824-24837, 2022.\\nFangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Training llms for divergent reasoning with minimal examples. In Forty-second International Conference on Machine Learning , 2025a.\\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 , 2025b.\\nTaeyoung Yun, Dinghuai Zhang, Jinkyoo Park, and Ling Pan. Learning to sample effective and diverse prompts for text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 23625-23635, 2025.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems , 35:15476-15488, 2022.\\nDavid W. Zhang, Corrado Rainone, Markus F. Peschl, and Roberto Bondesan. Robust scheduling with gflownets. ArXiv , abs/2302.05446, 2023a. URL https://api.semanticscholar.org/Corp usID:256827133 .\\nDinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models with GFlowNets and beyond. arXiv preprint arXiv:2209.02606v2 , 2022a.\\nDinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML) , 2022b.\\nDinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C. Courville, Yoshua Bengio, and Ling Pan. Let the flows tell: Solving graph combinatorial optimization problems with gflownets. ArXiv , abs/2305.17010, 2023b.\\nDinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization, 2024a.\\nDinghuai Zhang, Ling Pan, Ricky T. Q. Chen, Aaron Courville, and Yoshua Bengio. Distributional gflownets with quantile flows, 2024b.\\nDinghuai Zhang, Yizhe Zhang, Jiatao Gu, Ruixiang ZHANG, Joshua M. Susskind, Navdeep Jaitly, and Shuangfei Zhai. Improving GFlownets for text-to-image diffusion alignment. Transactions on Machine Learning Research , 2025a. ISSN 2835-8856. URL https://openreview.net/forum ?id=XDbY3qhM42 .\\n16\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nKaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. A survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827 , 2025b.\\nMingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, and Yoshua Bengio. Phylogfn: Phylogenetic inference with generative flow networks, 2024.\\nHeiko Zimmermann, Fredrik Lindsten, J.-W. van de Meent, and Christian Andersson Naesseth. A variational perspective on generative flow networks. ArXiv , abs/2210.07992, 2022. URL https: //api.semanticscholar.org/CorpusID:252907672 .\\n17\\nFlowRL: Matching Reward Distributions for LLM Reasoning\"},\n",
       "  {'title': 'A. Proof of Proposition 1',\n",
       "   'content': 'We begin by analyzing the gradient of the Kullback-Leibler (KL) divergence between the policy 𝜋𝜃 ( y | x ) and the target reward distribution exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) :\\nNext, consider the trajectory balance objective used in GFlowNets learning [Bartoldson et al., 2025, Bengio et al., 2023b, Lee et al., 2024], defined as:\\nTaking the gradient of this objective with respect to 𝜃 yields:\\nThus, minimizing the KL divergence is equivalent (up to a constant) to minimizing the trajectory balance loss, confirming Proposition 1.'},\n",
       "  {'title': 'B. Theoretical Analysis',\n",
       "   'content': 'We conduct an interpretation of FlowRL that clarifies the role of each component in the objective.\\nProposition 5. Minimizing the KL divergence in Eq. 5 is equivalent (in terms of gradients) to jointly maximizing reward and policy entropy:\\nRemark 6 ( FlowRL beyond reward maximization ) . Proposition 5 reveals that FlowRL can be interpreted as jointly maximizing expected reward and policy entropy. This shift encourages the policy to explore a broader set of high-quality solutions, enabling more diverse and generalizable behaviors on reasoning tasks. Our interpretation also aligns with prior work that views GFlowNets training as a form of maximum entropy RL [Deleu et al., 2024, Mohammadpour et al., 2024].\\n18\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nThe proof of Proposition 5 is provided as below.\\nRecall from Eq. 3 and Eq. 5 that the FlowRL objective is sourced from the minimization of a KL divergence:\\nRearranging the terms, we obtain:\\nFinally, we express the FlowRL objective in its compact form:\\n\\uf8f0\\n\\uf8fb\\nTherefore, minimizing the FlowRL objective can be interpreted as jointly maximizing reward and entropy, while also aligning the policy with a structured prior. The reward term drives task performance, while the normalization term 𝑍𝜙 ( x ) ensures consistency with a properly normalized target distribution. This encourages the policy 𝜋𝜃 to cover the entire reward-weighted distribution rather than collapsing to a few high-reward modes. The reference policy 𝜋 ref provides inductive bias that regularizes the policy toward desirable structures, and the entropy term H( 𝜋𝜃 ) encourages diversity in sampled solutions. Together, these components promote better generalization of FlowRL.'},\n",
       "  {'title': 'C. GFlowNets',\n",
       "   'content': 'We follow the notation of [He et al., 2025, Madan et al., 2023] to introduce the fundamentals of GFlowNets. Let X denote the compositional objects and 𝑅 be a reward function that assigns nonnegative values to each object 𝑥 ∈ X . GFlowNets aim to learn a sequential, constructive sampling policy 𝜋 that generates objects 𝑥 with probabilities proportional to their rewards, i.e., 𝜋 ( 𝑥 ) ∝ 𝑅 ( 𝑥 ) . This process can be represented as a directed acyclic graph (DAG) G = (S , A) , where the vertices 𝑠 ∈ S are referred to as states , and the directed edges ( 𝑢 → 𝑣 ) ∈ A are called actions . The generation of an object 𝑥 ∈ X corresponds to a complete trajectory 𝜏 = ( 𝑠 0 → · · · → 𝑠 𝑛 ) ∈ T within the DAG, beginning at the initial state 𝑠 0 and ending at a terminal state 𝑠 𝑛 ∈ X . The state flow 𝐹 ( 𝑠 ) is defined as a non-negative weight assigned to each state 𝑠 ∈ S . The forward policy 𝑃𝐹 ( 𝑠 ′ | 𝑠 ) specifies the transition probability to a child state 𝑠 ′ , while the backward policy 𝑃𝐵 ( 𝑠 | 𝑠 ′ ) specifies the transition probability to a parent state 𝑠 . To this end, detailed balance objective enforces local flow consistency across every edge ( 𝑠 → 𝑠 ′ ) ∈ A :\\n19\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTo achieve this flow consistency, GFlowNets employ training objectives at different levels of granularity, including detailed balance [Bengio et al., 2023b], trajectory balance [Malkin et al., 2022], and subtrajectory balance [Madan et al., 2023]. Leveraging their diversity-seeking behavior, GFlowNets have been successfully applied across a range of domains, including molecule generation [Cretu et al., 2024], diffusion fine-tuning [Liu et al., 2025b, Zhang et al., 2025a], and amortized reasoning [Hu et al., 2024, Yu et al., 2025a]. Among various training objective in GFlowNets, trajectory balance maintains flow consistency at the trajectory level, defined as:\\nFurthermore, sub-trajectory balance achieves local balance on arbitrary subpaths 𝜏𝑖 : 𝑗 = { 𝑠 𝑖 → · · ·→ 𝑠 𝑗 } , offering a more stable and less biased learning signal. We build on trajectory balance to extend our KL-based objective through a gradient-equivalence formulation (Prop. 1), and further improve it to better support long CoT reasoning in RL.\\nTable 5 | Math reasoning performance (Avg@64) at temperature = 0 . 6. Relative improvements are shown as subscripts, with positive gains in green and negative changes in red. FlowRL consistently outperforms all baselines and achieves the best average score under this low-temperature setting.\\nTable 6 | Math reasoning performance (Avg@64) at temperature = 1 . 0. Relative improvements are shown as subscripts, with positive gains in green. FlowRL maintains robust performance under higher generation randomness and continues to outperform all baselines on average.\\n20\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 7 | Ablation study on the effect of the 𝛽 parameter in FlowRL. We report Avg@16 accuracy across six math reasoning benchmarks for different values of 𝛽 .'},\n",
       "  {'title': 'Diversity Evaluation Prompt',\n",
       "   'content': 'System: You are evaluating the DIVERSITY of solution approaches for a mathematics competition problem. Focus on detecting even SUBTLE differences in methodology that indicate different problemsolving strategies.'},\n",
       "  {'title': 'PROBLEM:', 'content': '{problem}'},\n",
       "  {'title': '16 SOLUTION ATTEMPTS:', 'content': '{formatted_responses}'},\n",
       "  {'title': 'Score 1 - Minimal Diversity:',\n",
       "   'content': 'Same mathematical setup, same variable choices, same solution path\\n14+ responses use essentially identical approaches\\nOnly trivial differences (arithmetic, notation, wording)\\nIndicates very low exploration/diversity in the generation process'},\n",
       "  {'title': 'Score 2 - Low Diversity:',\n",
       "   'content': '1-2 alternative approaches appear but are rare\\n11-13 responses use the same main approach\\nMinor variations within the dominant method (different substitutions, orderings)\\nSome exploration but heavily biased toward one strategy'},\n",
       "  {'title': 'Score 3 - Moderate Diversity:',\n",
       "   'content': '2-3 distinct alternative approaches present\\n7-10 responses use the most common approach\\nNoticeable variation in problem setup or mathematical techniques\\nBalanced mix showing reasonable exploration'},\n",
       "  {'title': 'Score 4 - High Diversity:',\n",
       "   'content': '3-4 distinct solution strategies well-represented\\n4-6 responses use the most common approach\\nMultiple mathematical techniques and problem framings\\nStrong evidence of diverse exploration strategies'},\n",
       "  {'title': 'Score 5 - Maximum Diversity:',\n",
       "   'content': '4+ distinctly different solution strategies\\nNo single approach dominates ( ≤ 3 responses use same method)\\nWide variety of mathematical techniques and creative approaches\\nIMPORTANT: Focusing on the DIVERSITY of the attempted approaches. Return ONLY a number from 1 to 5.\\nExcellent exploration and generation diversity\\n21'}],\n",
       " 'is_processed': True,\n",
       " 'metrics': {'accuracy': 35.6},\n",
       " 'research_area': 'Artificial Intelligence',\n",
       " 'research_areas_all': ['Artificial Intelligence',\n",
       "  'Computer Vision and Pattern Recognition',\n",
       "  'Machine Learning'],\n",
       " 'word_count': 10103,\n",
       " 'author_count': 8,\n",
       " 'institutions': ['Tsinghua University',\n",
       "  'Shanghai Jiao Tong University',\n",
       "  'Peking University',\n",
       "  'Toyota Technological Institute',\n",
       "  'University of China']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_with_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 5 : Chunker test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.services.chunking.chunker import PaperChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\arxiv-ai-explorer\\backend\\src\\services\\chunking\\chunker.py:47: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=self.config.embedding_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-21 14:28:45,530 ] [sentence_transformers.SentenceTransformer] | Module: SentenceTransformer |Function: __init__ | Line: 219 - INFO - Use pytorch device_name: cpu\n",
      "[ 2025-09-21 14:28:45,531 ] [sentence_transformers.SentenceTransformer] | Module: SentenceTransformer |Function: __init__ | Line: 227 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "paper_chunker = PaperChunker()\n",
    "\n",
    "chunks = paper_chunker.chunk_paper(enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'Content',\n",
       "  'section_type': 'Content',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'arXiv:2509.15207v1  [cs.LG]  18 Sep 2025\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\n2025-09-17',\n",
       "  'start_char': 0,\n",
       "  'end_char': 107,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 13},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'FlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       "  'section_type': 'FlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'Xuekai Zhu 1 , Daixuan Cheng 6 , Dinghuai Zhang 3 , Hengli Li 5 , Kaiyan Zhang 4 , Che Jiang 4 , Youbang Sun 4 , Ermo Hua 4 , Yuxin Zuo 4 , Xingtai Lv 4 , Qizheng Zhang 7 , Lin Chen 1 , Fanghao Shao 1 , Bo Xue 1 , Yunchong Song 1 , Zhenjie Yang 1 , Ganqu Cui 2 , Ning Ding 4 , 2 , Jianfeng Gao 3 , Xiaodong Liu 3 , Bowen Zhou 4 , 2 ‡ , Hongyuan Mei 8 ‡ , Zhouhan Lin 1 , 2 ‡\\n1 Shanghai Jiao Tong University 2 Shanghai AI Laboratory 3 Microsoft Research 4 Tsinghua University 5 Peking University 6 Renmin University of China 7 Stanford University 8 Toyota Technological Institute at Chicago\\n/envelope hongyuanmei@gmail.com /envelope xuekaizhu0@gmail.com /github FlowRL ‡ Corresponding Authors.\\nAbstract | We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods ( e.g. , PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10 . 0% over GRPO and 5 . 1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.\\nFigure 1 | Top : Comparison between distribution-matching and reward-maximizing approaches. FlowRL (left) learns to match the full reward distribution, maintaining diversity across multiple modes with low KL divergence. In contrast, reward-maximizing methods like GRPO (right) concentrate on a single high-reward peak, leading to mode collapse and higher KL divergence. Bottom : Performance comparison. FlowRL consistently outperforms GRPO across math and code domains.\\nDistribution-matching: FlowRL\\nKL = 0.11\\nKL = 8.68\\nReward-maximizing\\n∶\\nR++, PPO and GRPO\\nMath Average Score\\nCodeForces Rating\\nFlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       "  'start_char': 0,\n",
       "  'end_char': 2465,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 390},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '1. Introduction',\n",
       "  'section_type': '1. Introduction',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': \"Reinforcement learning (RL) plays a crucial role in the post-training of large language models (LLMs) [Zhang et al., 2025b]. A series of powerful reasoning models [Guo et al., 2025, Kavukcuoglu, 2025, Rastogi et al., 2025] have employed large-scale reinforcement learning to achieve strong performance on highly challenging benchmarks [He et al., 2024]. The evolution of RL algorithms for LLM reasoning has progressed through several key stages: REINFORCE [Sutton et al., 1999a] provides a solid baseline that is easy to implement and efficient in simple settings; PPO [Schulman et al., 2017] improves upon REINFORCE with better stability and efficiency in complex settings; GRPO [Shao et al., 2024] simplifies PPO training by eliminating value functions and relying on group comparisons, though at the cost of requiring more rollouts per update. However, all these methods share a fundamental limitation in their reward-maximizing objective.\\nReward-maximizing RL methods tend to overfit to the dominant mode of the reward distribution [Gao et al., 2023, Pan et al., 2022, Skalse et al., 2022, Zelikman et al., 2022]. This often results in limited diversity among generated reasoning paths and reduces generalization to less frequent yet valid logical outcomes [Hu et al., 2023]. As illustrated in Figure 1, GRPO neglects other meaningful modes. These drawbacks become especially pronounced in complex long chain-of-thought (CoT; Wei et al., 2022) reasoning, where capturing a diverse distribution of plausible solutions is essential for effective generalization [Liu et al., 2025a]. Recent approaches adjust the clip ratio [Yu et al., 2025b], augment the advantage function with an entropy-based term [Cheng et al., 2025], or selectively promote high-entropy tokens [Wang et al., 2025], thereby dynamically adapting the training data distribution and implicitly increasing diversity during training. This raises a fundamental question: How can we promote diverse exploration to prevent convergence to dominant solution patterns in RL training?\\nIn this paper, we propose FlowRL , a policy optimization algorithm that aligns the policy model with the full reward distribution, encouraging mode coverage. FlowRL achieves more efficient exploration by fundamentally shifting from reward maximization to reward distribution matching, thereby addressing the inherent mode-collapse limitations of previous RL approaches. As illustrated in Figure 1, the core idea of FlowRL is to introduce a learnable partition function that normalizes scalar rewards into a target distribution, and to minimize the reverse KL divergence between the policy and this reward-induced distribution. We develop this KL objective based on the trajectory balance formulation from GFlowNets [Bengio et al., 2023b], providing a gradient equivalence proof that bridges generative modeling and policy optimization. To address the challenges of long CoT training, we introduce two key technical solutions: length normalization to tackle gradient explosion issues that occur with variable-length CoT reasoning, and importance sampling to correct for the distribution mismatch between generated rollouts and the current policy.\\nWe compare FlowRL with mainstream RL algorithms including REINFORCE++, PPO, and GRPO across math and code domains, using both base and distilled LLMs (7B, 32B). In math domain, FlowRL outperforms GRPO and PPO by 10 . 0% and 5 . 1%, respectively, demonstrating consistent improvements across six challenging math benchmarks. Furthermore, FlowRL surpasses both PPO and GRPO on three challenging coding benchmarks, highlighting its strong generalization capabilities in code reasoning tasks. To understand what drives these performance gains, we analyze the diversity of generated reasoning paths. This diversity analysis confirms that FlowRL generates substantially more diverse rollouts than baseline methods, validating our approach's effectiveness in exploring multiple solution strategies.\\nContributions. We summarize the key contributions of this work as follows:\\nWe propose FlowRL, a policy optimization algorithm that shifts from reward maximization to\\n2\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nreward distribution matching via flow balance, encouraging diverse reasoning path exploration while addressing the inherent mode-collapse limitations of existing RL methods.\\nFlowRL outperforms GRPO and PPO by 10.0% and 5.1% respectively across math benchmarks and demonstrates strong generalization on code reasoning tasks, with diversity analysis confirming substantially more diverse solution exploration.\\nWe introduce length normalization and importance sampling to enable effective training on variablelength CoT reasoning, addressing gradient explosion and sampling mismatch issues.\",\n",
       "  'start_char': 0,\n",
       "  'end_char': 4794,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 675},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '2. Preliminaries',\n",
       "  'section_type': '2. Preliminaries',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'Reinforcement Learning for Reasoning. Weformulate reasoning as a conditional generation problem, where the policy model receives a question x ∈ X and generates an answer y ∈ Y . The objective is to learn a policy 𝜋𝜃 ( y | x ) that produces high-quality answers under task-specific reward signals 𝑟 . To better illustrate the policy optimization procedure, we provide a detailed formulation of GRPO below. For each question x , GRPO samples a group of answers { y 1 , y 2 , . . . , y 𝐺 } from old policy 𝜋𝜃𝑜𝑙𝑑 and updates the model by maximizing the following objective:\\nwhere 𝜖 and 𝜆 are hyper-parameters. Here, 𝐴𝑖 denotes the advantage, computed by normalizing the group reward values { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } as 𝐴𝑖 = 𝑟 𝑖 -mean ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) std ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) . Compared to GRPO, REINFORCE applies the policy gradient directly, without advantage normalization, clipping, or KL regularization. PPO uses a critic model to estimate the advantage and employs importance sampling to stabilize policy updates.\\nGFlowNets. Generative Flow Networks [Bengio et al., 2023a] are a probabilistic framework for training stochastic policies to sample discrete, compositional objects ( e.g. , graphs, sequences) in proportion to a given reward. As shown in Figure 2, the core principle of GFlowNets is to balance the forward and backward probability flows at each state, inspired by flow matching [Bengio et al., 2021]. The initial flow is estimated by 𝑍𝜙 ( 𝑠 0 ) at the initial state 𝑠 0 . The output flow is equal to the outcome reward 𝑟 ( 𝑠 𝑛 ) conditioned at the final state 𝑠 𝑛 . Following Lee et al. [2024], we use a 3-layer MLP to parameterize 𝑍𝜙 . This flow-balancing mechanism facilitates the discovery of diverse,\\nFigure 2 | GFlowNets [Bengio et al., 2023a], a flow-balance perspective on reinforcement learning. The initial flow 𝑍𝜙 ( 𝑠 0 ) injects probability mass into the environment, which is transported through intermediate states by the policy 𝜋𝜃 and accumulated at terminal states in proportion to the scalar rewards.\\n𝑠\\n!\\n𝑠\\n\"\\n𝑠\\n#\\n𝑠\\n$\\n𝑠\\n%\\n𝑠\\n&\\n𝑠\\n\\'\\n𝑠\\n(\\n𝑠\\n)\\n𝑠\\n*\\n𝑠\\n\"!\\n𝑠\\n+\\n𝑠\\n+\\n𝑆\\n!\\n𝑠\\n+\\n𝑠\\n+\\n𝑠\\n+\\nIn Flow\\nZ\\n\"\\n𝑠\\n#\\nOut Flow r\\n(𝜏)\\nhigh-reward solutions by ensuring proper exploration of the solution space. See Appendix C for detailed GFlowNets background.\\n3\\nFlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       "  'start_char': 0,\n",
       "  'end_char': 2333,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 442},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '3. Methodology',\n",
       "  'section_type': '3. Methodology',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'In this section, we first formulate distribution matching in reinforcement learning through reverse KL divergence and establish its connection to trajectory balance from GFlowNets. To address the challenges of gradient explosion and sampling mismatch encountered during long CoT training, we further incorporate length normalization and importance sampling. Using this enhanced framework, we derive a flow-balanced objective, termed FlowRL .',\n",
       "  'start_char': 0,\n",
       "  'end_char': 441,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 59},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '3.1. From Reward Maximization to Distribution Matching',\n",
       "  'section_type': '3.1. From Reward Maximization to Distribution Matching',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'As illustrated in Figure 1, recent powerful large reasoning models typically employ reward-maximizing RL algorithms, such as PPO or GRPO. However, these methods tend to optimize toward the dominant reward mode, frequently resulting in mode collapse and the neglect of other plausible, high-quality reasoning paths. To address this fundamental limitation, we propose optimizing the policy by aligning its output distribution to a target reward distribution. A simple yet effective way to achieve this is to minimize the reverse KL divergence 1 between the policy and this target. However, in long CoT reasoning tasks, the available supervision in RL is a scalar reward, rather than a full distribution. Moreover, enumerating or sampling all valid trajectories to recover the true reward distribution is computationally intractable.\\nInspired by energy-based modeling [Du and Mordatch, 2019, Hinton et al., 1995], we introduce a learnable partition function 𝑍𝜙 ( x ) to normalize scalar rewards into a valid target distribution. This allows us to minimize the reverse KL divergence between the policy and the reward-weighted distribution, formalized as:\\nwhere 𝑟 ( x , y ) is the reward function, 𝛽 is a hyperparameter, 𝑍𝜙 ( x ) is the learned partition function, and the resulting target distribution is defined as ˜ 𝜋 ( y | x ) = exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) . This objective encourages the policy to sample diverse, high-reward trajectories in proportion to their rewards, rather than collapsing to dominant modes as in standard reward maximization.\\nWhile the KL-based formulation provides a principled target distribution, we derive a more practical, RL-style objective that facilitates efficient policy optimization.\\nProposition 1. In terms of expected gradients, minimizing the KL objective in Eq. 2 is equivalent to minimizing the trajectory balance loss used in GFlowNet [Bartoldson et al., 2025, Lee et al., 2024, Malkin et al., 2022, 2023]:\\nRemark 2 ( Trajectory balance as a practical surrogate for KL minimization ) . Given the equivalence established in Proposition 1, the KL-based distribution matching objective can be reformulated as the trajectory balance loss. This reformulation provides a practical optimization approach by using a stable squared loss form rather than direct KL optimization, and by treating 𝑍𝜙 ( x ) as a learnable parameter rather than requiring explicit computation of the intractable partition function. The trajectory balance objective thus serves as a tractable surrogate for reward-guided KL minimization that can be directly integrated into existing RL frameworks.\\n1 We use reverse KL since we can only sample from the policy model, not the target reward distribution.\\n4\\nFlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       "  'start_char': 0,\n",
       "  'end_char': 2768,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 435},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '3.2. FlowRL',\n",
       "  'section_type': '3.2. FlowRL',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'As established in Proposition 1, the target reward distribution can be approximated by optimizing the trajectory balance objective. However, applying this objective directly to long CoT reasoning introduces two key challenges:\\nProblem I: Exploding gradients from long trajectories. Trajectory balance is a sequence-level objective, and applying it to long CoT reasoning with up to 8K tokens leads to exploding gradients and unstable updates. This issue is not observed in prior GFlowNets works, which typically operate on short trajectories in small discrete spaces. Specifically, the log-probability term log 𝜋𝜃 ( y | x ) decomposes into a token-wise sum, ˝ 𝑡 log 𝜋𝜃 ( y 𝑡 | y <𝑡 , x ) , causing the gradient norm to potentially scale with sequence length.\\nProblem II: Sampling mismatch. Mainstream RL algorithms such as PPO and GRPO commonly perform micro-batch updates and reuse trajectories collected from an old policy 𝜋𝜃 old , enabling data-efficient training. In contrast, the KL-based trajectory balance objective assumes fully onpolicy sampling, where responses are drawn from the current policy. This mismatch poses practical limitations when integrating trajectory balance into existing RL pipelines.\\nThese limitations motivate our reformulation that retains the benefits of distribution matching while addressing key practical challenges. To enable this reformulation, we first redefine the reward function following established practices in GFlowNets literature [Bartoldson et al., 2025, Lee et al., 2024, Yu et al., 2025a] by incorporating a reference model as a prior constraint on the reward distribution. Specifically , we modify the original exp ( 𝛽𝑟 ( x , y )) to include the reference model:\\nwhere 𝑟 ( x , y ) denotes the outcome reward commonly used in reinforcement learning and 𝜋 ref is the initial pre-trained model. We follow Guo et al. [2025] to use outcome-based reward signals, and apply group normalization to 𝑟 ( x , y ) as ˆ 𝑟 𝑖 = ( 𝑟 𝑖 -mean ( r ))/ std ( r ) , where r = { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } denotes the set of rewards within a sampled group. By substituting the redefined reward formulation Eq. 4 into Eq. 3, we derive the following objective 2 :\\nRemark 3 ( Reward shaping via length normalization ) . Trajectory balance treats both the initial flow and the outcome reward as sequence-level quantities. In contrast, standard policy optimization methods such as PPO or GRPO assign rewards at the token level and compute gradients at each step. However, for trajectories of varying lengths ( e.g. , CoT responses), this mismatch can cause the log-probability term log 𝜋𝜃 ( y | x ) = ˝ | y | 𝑡 = 1 log 𝜋𝜃 ( 𝑦 𝑡 | 𝑦 <𝑡 , x ) to scale with sequence length. To address this, we apply a form of reward shaping by normalizing log-probabilities with respect to sequence length. Specifically, we rescale the term as 1 | y | log 𝜋𝜃 ( y | x ) , balancing the contributions of long and short sequences and stabilizing the learning signal.\\nRemark 4 ( Importance sampling for data-efficient training ) . To mitigate sampling mismatch, we employ importance sampling inspired by PPO to stabilize policy updates with off-policy data. We re-weight stale trajectories using the importance ratio 𝑤 = 𝜋𝜃 ( y | x )/ 𝜋 old ( y | x ) , which serves as a coefficient in the surrogate loss. Since our objective focuses on optimizing trajectory balance rather than expected return, we detach the gradient from the current policy to prevent excessive policy drift: 𝑤 = detach [ 𝜋𝜃 ( y | x )]/ 𝜋 old ( y | x ) . For additional stability, we incorporate PPO-style clipping to detach\\n.\\n2 The substitution replaces 𝛽𝑟 ( x , y ) in trajectory balance objective Eq. 3 with 𝛽𝑟 ( x , y ) + log 𝜋 ref ( y | x ) to incorporate the reference model constraint.\\n5\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nIncorporating these improvements into Eq. 5, we arrive at the following FlowRL objective:\\nFlowRL\\nwhere the clipped importance weight 𝑤 and normalized reward ˆ 𝑟 ( x , y ) are defined as:\\nWe use this objective to update the policy parameters 𝜃 during training, and refer to this strategy as FlowRL . Implementation details and theoretical analysis are provided in § 5 and § B, respectively.',\n",
       "  'start_char': 0,\n",
       "  'end_char': 4208,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 740},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '4.1. Reinforcement Learning for Reasoning',\n",
       "  'section_type': '4.1. Reinforcement Learning for Reasoning',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': \"Reinforcement learning has emerged as a powerful approach for large language models post-training on reasoning tasks [Guo et al., 2025, Lightman et al., 2023b, Schulman et al., 2017, Shao et al., 2024, Sutton et al., 1999b]. Most approaches employ reward-maximizing RL to optimize expected cumulative returns. Entropy regularization [Ahmed et al., 2019, Cheng et al., 2025, Haarnoja et al., 2018] is a classical technique for mitigating mode collapse by promoting diversity in the policy's output distribution, and has also been shown to enhance reasoning capabilities in various settings [Chao et al., 2024, Eysenbach and Levine, 2021]. However, for long CoT reasoning, the extended trajectory length (e.g., 8k-16k tokens) makes it difficult for the regularization signal to effectively influence reward-maximizing learning. Recent work [Cheng et al., 2025, Cui et al., 2025, Dong et al., 2025, Wang et al., 2025] has discovered that training with more diverse or high-entropy training data can further enhance training effectiveness. Compared to traditional entropy regularization, the above methods explicitly increase the proportion of low-probability (i.e., high-entropy) tokens in the training data. In our work, we address the mode-collapse problem by fundamentally shifting from reward maximization to reward distribution matching in our RL formulation.\",\n",
       "  'start_char': 0,\n",
       "  'end_char': 1361,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 197},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '4.2. GFlowNets',\n",
       "  'section_type': '4.2. GFlowNets',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'GFlowNets [Bengio et al., 2023a] represent a class of diversity-driven algorithms designed to balance probability flows across states. They have rich connections to probabilistic modeling methods [Ma et al., Malkin et al., 2023, Zhang et al., 2022a,b, 2024a, Zimmermann et al., 2022], and control methods [Pan et al., 2023b,c,d, Tiapkin et al., 2024, Zhang et al., 2024b]. This advantage has enabled GFlowNets to achieve successful applications in multiple downstream tasks, such as molecular drug discovery [Jain et al., 2022, 2023a,b, Kim et al., 2023, 2024, Liu et al., 2022, Pan et al., 2023a, Shen et al., 2023], phylogenetic inference [Zhou et al., 2024], and combinatorial optimization [Zhang et al., 2023a,b]. For generative AI, GFlowNets provide a powerful approach to align pretrained models in scenarios such as image generation [Yun et al., 2025, Zhang et al., 2025a] and language model fine-tuning [Hu et al., 2024, Lee et al., 2024, Yu et al., 2025a]. Another line of work primarily focuses on the theoretical aspects of GFlowNets. Recent theoretical studies have interpreted GFlowNets as solving a maximum entropy reinforcement learning problem within a modified Markov Decision Process (MDP) [Deleu et al., 2024, Mohammadpour et al., 2024, Tiapkin et al., 2024]. These theoretical contributions have\\n6\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\ninspired us to enhance reinforcement learning from a more foundational standpoint using GFlowNets principles. A comprehensive overview of GFlowNets theory can be found in Appendix C.',\n",
       "  'start_char': 0,\n",
       "  'end_char': 1556,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 235},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '4.3. Flow-Matching Policies',\n",
       "  'section_type': '4.3. Flow-Matching Policies',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'Flow matching simplifies diffusion-based approaches by learning vector fields that transport samples from prior to target distributions [Lipman et al., 2023]. Recent work has explored flow matching for policy optimization. McAllister et al. [2025] reformulates policy optimization using advantageweighted ratios from conditional flow matching loss, enabling flow-based policy training without expensive likelihood computations. Pfrommer et al. [2025] explored reward-weighted flow matching for improving policies beyond demonstration performance. Park et al. [2025] uses a separate one-step policy to avoid unstable backpropagation through time when training flow policies with RL. Zhang et al. [2025a] proposed a combined loss function integrating PPO and GFlowNets to optimize diffusion model alignment. However, these approaches focus on continuous control, image generation, or vision-action models, rather than addressing mode-collapse limitations in reward-maximizing RL. Inspired by flow matching principles, our work improves upon RL training to enhance training stability while promoting diverse solution exploration.',\n",
       "  'start_char': 0,\n",
       "  'end_char': 1126,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 146},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '5. Experiment Settings',\n",
       "  'section_type': '5. Experiment Settings',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'Backbone Models. There are two learnable modules in Eq. 6: the policy model 𝜋𝜃 and the partition function 𝑍𝜙 . For the policy model 𝜋𝜃 , we use Qwen-2.5-7B/32B [Team, 2024] for math tasks and DeepSeek-R1-Distill-Qwen-7B [DeepSeek-AI, 2025] for code tasks, respectively. For partition function 𝑍𝜙 , following Lee et al. [2024], we use a randomly initialized 3-layer MLP with hidden dimensions matching those of the base model. The reference model 𝜋 ref is the corresponding fixed pretrained model. All training scripts are based on the veRL [Sheng et al., 2024]. For the reward function, following Lee et al. [2024], we set the hyperparameter 𝛽 = 15.\\nBaselines. We compare our method against three representative reward-maximization RL baselines: REINFORCE++ (R++; Hu et al., 2025, Sutton et al., 1999b), PPO [Schulman et al., 2017], and GRPO [Shao et al., 2024]. All baselines follow the official veRL recipes, with consistent training configurations. For fair comparison, all methods use the same learning rate, batch size, and training steps, and are evaluated at convergence using identical step counts.\\nTraining Configuration. We experiment on both math and code domains. For the math domain, we use the training set collected from DAPO [Yu et al., 2025b]. For the code domain, we follow the setup of DeepCoder [Luo et al., 2025], using their training set. For 7B model training, we use a single node equipped with 8 NVIDIA H800 GPUs (80GB memory each). For 32B model training, we scale to 4 nodes with 32 GPUs to accommodate the larger memory requirements. All experiments use max_prompt_length = 2048 and max_response_length = 8192 across both model sizes. We use a batch size of 512 for math reasoning tasks and 64 for code reasoning tasks. We set the learning rate to 1e-6 and enable dynamic batch sizing in veRL for efficient training. For GRPO and FlowRL, we configure rollout_n = 8, meaning each prompt generates 8 response rollouts as the group size.\\nEvaluation Configuration. For the math domain, we evaluate on six challenging benchmarks: AIME 2024/2025 [MAA, 2025], AMC 2023 [MAA, 2023], MATH-500 [Lightman et al., 2023a], Minerva [Lewkowycz et al., 2022], and Olympiad [He et al., 2024]. For the code domain, we evaluate on LiveCodeBench [Jain et al., 2024], CodeForces [Penedo et al., 2025], and HumanEval+ [Chen et al., 2021]. For all evaluation datasets, we perform 16 rollouts and report the average accuracy, denoted as Avg@16. We further report rating and percentile for Codeforces. During generation, we\\n7\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 1 | Results on math benchmarks. We report Avg@16 accuracy with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL outperforms all baselines across both 7B and 32B model scales.\\nTable 2 | Results on code benchmarks. We report metrics with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL achieves the strongest performance across all three benchmarks, demonstrating its effectiveness in code reasoning tasks.\\nuse sampling parameters of temperature =0.6 and top_p =0.95 for all evaluations. The response length for evaluation is set to 8,192, consistent with the training configuration.',\n",
       "  'start_char': 0,\n",
       "  'end_char': 3301,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 524},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '6.1. Main Results',\n",
       "  'section_type': '6.1. Main Results',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': \"Our experimental results, summarized in Table 1 and Table 2, demonstrate that FlowRL consistently outperforms all reward-maximization baselines across both math and code reasoning domains. Table 1 reports results on math reasoning benchmarks using both 7B and 32B base models, while Table 2 presents the corresponding results on code reasoning tasks. On math reasoning tasks, FlowRL achieves the highest average accuracy of 35.6% with the 7B model and 48.4% with the 32B model, surpassing PPO by 5.1% and GRPO by 10.1% on the 32B model. FlowRL shows strong improvements on challenging benchmarks like MATH-500 and Olympiad problems, demonstrating consistent gains\\n8\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 3 | Ablation study on FlowRL with Qwen2.5-7B as the base model. Avg@16 accuracy is reported across six math reasoning benchmarks. IS denotes importance sampling.\\nacross diverse mathematical domains. On code generation tasks, FlowRL achieves compelling improvements with the highest Avg@16 score of 37.43% on LiveCodeBench, a Codeforces rating of 1549.47 with 83.3% percentile ranking, and 83.28% accuracy on HumanEval+, outperforming all baselines across the board. These consistent performance gains across both domains and model scales provide strong empirical evidence that FlowRL's flow-balanced optimization successfully enhances generalization. This improvement comes from promoting diverse solution exploration compared to previous reward-maximizing RL approaches.\",\n",
       "  'start_char': 0,\n",
       "  'end_char': 1499,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 210},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '6.2. Ablation Studies',\n",
       "  'section_type': '6.2. Ablation Studies',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'We conduct ablation studies on importance sampling and the 𝛽 hyperparameter. For importance sampling, we compared the performance with and without it, and implemented a combined loss approach proposed by Zhang et al. [2025a] that simultaneously optimizes both GFlowNets and PPO objectives. This combined loss focuses on optimizing diffusion models, and we adapt it to long CoT reasoning tasks for comparison. Table 3 demonstrates that importance sampling substantially improves FlowRL performance across all math reasoning benchmarks. Compared to Zhang et al. [2025a], using importance sampling as a trajectory-level ratio is more suitable than the combined loss of GFlowNets and PPO. The performance drop without importance sampling (from 35.63% to 26.71%) highlights the critical role of correcting for distribution mismatch between rollout generation and policy training. For the hyperparam-\\nFigure 3 | Ablation study on the 𝛽 in FlowRL. 𝛽 = 15 (highlighted in blue) achieves the best performance.\\n=5\\n=10\\n=15\\n=30\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\nAverage Score (%)\\n31.34\\n34.41\\n35.63\\n35.09\\neter 𝛽 , we conduct a series of parameter ablation studies, and Figure 3 shows that 𝛽 = 15 achieves optimal performance, with detailed results shown in Table 7.',\n",
       "  'start_char': 0,\n",
       "  'end_char': 1242,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 196},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '7.1. Diversity Analysis',\n",
       "  'section_type': '7.1. Diversity Analysis',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'To assess solution diversity , we follow the approach of Yu et al. [2025a] and employ GPT-4o-mini [OpenAI, 2024] to evaluate all responses generated by each method on AIME 24/25. The evaluation prompt is shown in Appendix C. As shown in Figure 4, FlowRL achieves higher diversity scores compared to baseline methods. This demonstrates that FlowRL improves sample diversity compared to baselines, which tend to exhibit repetitive solution patterns. This diversity evaluation reveals\\n9\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 4 | Case study comparing GRPO and FlowRL rollouts on an AIME problem. GRPO exhibits repetitive patterns (AM-GM × 3, identity loops × 2), while FlowRL follows a more diverse solution path.',\n",
       "  'start_char': 0,\n",
       "  'end_char': 733,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 114},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'Question',\n",
       "  'section_type': 'Question',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': \"Let B be the set of rectangular boxes with surface area 54 and volume 23. Let 𝑟 be the radius of the smallest sphere that can contain each box in B . If 𝑟 2 = 𝑝 𝑞 with gcd ( 𝑝, 𝑞 ) = 1, find 𝑝 + 𝑞 .\\nGRPO\\n'. . .\\ndenote\\n𝑎, 𝑏, 𝑐\\n. . .\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n=\\n54,\\n𝑎𝑏𝑐\\n=\\n23 ...\\n𝑑\\n=\\n√\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2 ,\\n𝑟\\n=\\n𝑑\\n/\\n2\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n=\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n+\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n. . . AM-GM\\n×\\n3 :\\nAM-GM (1)\\n. . .\\nAM-GM (2)\\n. . .\\nAM-GM (3)\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n3\\nidentity loop\\n×\\n2 :\\nloop (1)\\n. . .\\nloop (2)\\n. . .\\n𝑎\\n=\\n𝑏\\n=\\n𝑐\\n(contradiction) .. .\\nback to\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n. . .\\nno factorization . . . '\\nFlowRL\\n'. . .\\nlet\\n𝑎, 𝑏, 𝑐\\nwith\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n=\\n54,\\n𝑎𝑏𝑐\\n=\\n23\\n. . .\\n𝑑\\n=\\n√\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2 ,\\n𝑟\\n=\\n𝑑\\n/\\n2\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n⇒\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n=\\n𝑠\\n2\\n-\\n54\\n. . .\\n𝑎\\n=\\n𝑏\\n. . .\\n𝑎\\n3\\n-\\n27\\n𝑎\\n+\\n46\\n=\\n0\\n. . .\\nrational root\\n𝑎\\n=\\n2 ...\\nfactor\\n(\\n𝑎\\n-\\n2\\n)(\\n𝑎\\n2\\n+\\n2\\n𝑎\\n-\\n23\\n)\\n. . .\\nbranch\\n𝑎\\n=\\n-\\n1\\n+\\n2\\n√\\n6 ...\\nback-sub\\n𝑐\\n=\\n23\\n/\\n𝑎\\n2\\n. . .\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n=\\n657\\n16\\n. . .\\n𝑟\\n2\\n=\\n657\\n64\\n. . .\\nAnswer 721 ...'\\nsignificant differences in exploration patterns across methods. This nearly doubling of diversity score compared to the strongest baseline (PPO) indicates that FlowRL generates qualitatively different solution approaches rather than minor variations of the same strategy. The diversity analysis provides empirical validation of our core hypothesis that flow-balanced optimization promotes mode coverage in complex reasoning tasks.\",\n",
       "  'start_char': 0,\n",
       "  'end_char': 1399,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 405},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '7.2. Case Study',\n",
       "  'section_type': '7.2. Case Study',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': \"Table 4 illustrates the behavioral differences between GRPO and FlowRL on a representative AIME problem. GRPO exhibits repetitive patterns, applying AMGM three times and getting stuck in identity loops, failing to solve the problem. FlowRL explores more diverse actions: it sets 𝑎 = 𝑏 , derives a cubic equation, finds the rational root, and reaches the correct answer. This shows that FlowRL successfully avoids the repetitive exploration patterns. The contrast reveals fundamental differences in exploration strategies: GRPO's reward-maximizing approach leads to exploitation of familiar techniques (AM-GM inequality) without exploring alternatives, eventually reaching contradictory conclusions like 𝑎 = 𝑏 = 𝑐 . In contrast, FlowRL's distribution-matching enables strategic decisions such as the symmetry assumption 𝑎 = 𝑏 , which\\nFigure 4 | GPT-judged diversity scores on rollouts of AIME 24/25 problems. FlowRL generates more diverse solutions than R++, GRPO, and PPO.\\nR++\\nGRPO\\nPPO\\nFlowRL\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nDiversity Score\\n1.11\\n1.23\\n1.31\\n2.28\\ntransforms the problem into a tractable cubic equation 𝑎 3 -27 𝑎 + 46 = 0, allowing systematic solution through rational root testing and polynomial factorization.\\n10\\nFlowRL: Matching Reward Distributions for LLM Reasoning\",\n",
       "  'start_char': 0,\n",
       "  'end_char': 1275,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 190},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '8. Conclusion',\n",
       "  'section_type': '8. Conclusion',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'In this work, we introduce FlowRL, which transforms scalar rewards into normalized target distributions using a learnable partition function and minimizes the reverse KL divergence between the policy and target distribution. We demonstrate that this approach is theoretically equivalent to trajectory balance objectives from GFlowNets and implicitly maximizes both reward and entropy, thereby promoting diverse reasoning trajectories. To further address gradient explosion and sampling mismatch issues in long CoT reasoning, we incorporate importance sampling and length normalization. Through experiments on math and code reasoning benchmarks, FlowRL achieves consistent improvements across all tasks compared to GRPO and PPO. Our diversity analysis and case studies confirm that FlowRL generates more varied solution approaches while avoiding repetitive patterns.',\n",
       "  'start_char': 0,\n",
       "  'end_char': 865,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 115},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'Acknowledgments',\n",
       "  'section_type': 'Acknowledgments',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'We are grateful to Mingqian Feng and Yuetai Li for their valuable discussions and feedback, which helped improve the quality of this work.',\n",
       "  'start_char': 0,\n",
       "  'end_char': 138,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 23},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': \"Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of entropy on policy optimization. In International conference on machine learning , pages 151-160. PMLR, 2019. Brian R Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, and Bhavya Kailkhura. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable llm post-training. arXiv preprint arXiv:2503.18929 , 2025. Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS) , 2021. Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research , 24(210):1-55, 2023a. URL http: //jmlr.org/papers/v24/22-0364.html . Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. The Journal of Machine Learning Research , 24(1):10006-10060, 2023b. Chen-Hao Chao, Chien Feng, Wei-Fang Sun, Cheng-Kuang Lee, Simon See, and Chun-Yi Lee. Maximum entropy reinforcement learning via energy-based normalizing flow. arXiv preprint arXiv:2405.13629 , 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\\n11\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758 , 2025. Miruna Cretu, Charles Harris, Ilia Igashov, Arne Schneuing, Marwin Segler, Bruno Correia, Julien Roy, Emmanuel Bengio, and Pietro Liò. Synflownet: Design of diverse and novel molecules with synthesis constraints. arXiv preprint arXiv:2405.01155 , 2024. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617 , 2025. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 . Tristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, and Yoshua Bengio. Discrete probabilistic inference as control in multi-path environments. arXiv preprint arXiv:2402.10309 , 2024. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849 , 2025. Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in neural information processing systems , 32, 2019. Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257 , 2021. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning , pages 10835-10866. PMLR, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning , pages 1861-1870. Pmlr, 2018. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 , 2024. Haoran He, Can Chang, Huazhe Xu, and Ling Pan. Looking backward: Retrospective backward synthesis for goal-conditioned GFlownets. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?id=fNMKqyvuZT . Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and R M Neal. The 'wake-sleep' algorithm for unsupervised neural networks. Science , 268 5214:1158-61, 1995. Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. arXiv preprint arXiv:2310.04363 , 2023. 12\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nEdward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/f orum?id=Ouj6p4ca60 . Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv.\",\n",
       "  'start_char': 0,\n",
       "  'end_char': 5977,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 807},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 1,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'org/abs/2501 , 3262:32-33, 2025. Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena Simine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. International Conference on Machine Learning (ICML) , 2022. Moksh Jain, Tristan Deleu, Jason S. Hartford, Cheng-Hao Liu, Alex Hernández-García, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. ArXiv , abs/2302.00615, 2023a. URL https: //api.semanticscholar.org/CorpusID:256459319 . Moksh Jain, Sharath Chandra Raparthy, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective GFlowNets. International Conference on Machine Learning (ICML) , 2023b. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024. Koray Kavukcuoglu. Gemini 2.5: Our most intelligent AI model, 2025. URL https://blog.goo gle/technology/google-deepmind/gemini-model-thinking-updates-march-2025/ . Google Blog (The Keyword), Published Mar. 25, 2025. Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search gflownets. ArXiv , abs/2310.02710, 2023. Minsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan, Woochang Kim, Jinkyoo Park, Emmanuel Bengio, and Yoshua Bengio. Learning to scale logits for temperature-conditional gflownets, 2024. Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et al. Learning diverse attacks on large language models for robust red-teaming and safety tuning. arXiv preprint arXiv:2405.18540 , 2024. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In S.',\n",
       "  'start_char': 1187,\n",
       "  'end_char': 3394,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 286},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 2,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K.',\n",
       "  'start_char': 8186,\n",
       "  'end_char': 8233,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 8},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 3,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': \"Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 3843-3857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/18abb eef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf . Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050 , 2023a. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. In The Twelfth International Conference on Learning Representations , 2023b. 13\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nYaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t . Dianbo Liu, Moksh Jain, Bonaventure F. P. Dossou, Qianli Shen, Salem Lahlou, Anirudh Goyal, Nikolay Malkin, Chris C. Emezue, Dinghuai Zhang, Nadhir Hassen, Xu Ji, Kenji Kawaguchi, and Yoshua Bengio. Gflowout: Dropout with generative flow networks. In International Conference on Machine Learning , 2022. Mingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507 , 2025a. Zhen Liu, Tim Z Xiao, , Weiyang Liu, Yoshua Bengio, and Dinghuai Zhang. Efficient diversity-preserving diffusion alignment via gradient-informed gflownets. In ICLR , 2025b.\",\n",
       "  'start_char': 8234,\n",
       "  'end_char': 9998,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 237},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 4,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, Ion Stoica, and Tianjun Zhang. Deepcoder: A fully open-source 14b coder at o3-mini level, 2025. Notion Blog.',\n",
       "  'start_char': 9999,\n",
       "  'end_char': 10260,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 43},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 5,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Jiangyan Ma, Emmanuel Bengio, Yoshua Bengio, and Dinghuai Zhang. Baking symmetry into gflownets. MAA. American mathematics competitions - amc. https://maa.org/ , 2023. MAA. American invitational mathematics examination - aime. https://maa.org/ , 2025. Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning , pages 23467-23483. PMLR, 2023. Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. Advances in Neural Information Processing Systems , 35: 5955-5967, 2022. Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. International Conference on Learning Representations (ICLR) , 2023. David McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053 , 2025. Sobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropy gflownets with soft q-learning. In International Conference on Artificial Intelligence and Statistics , pages 2593-2601. PMLR, 2024. OpenAI. Gpt-4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-effic ient-intelligence/ , 2024. Accessed: 2024.',\n",
       "  'start_char': 10261,\n",
       "  'end_char': 11785,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 194},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 6,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544 , 2022. Ling Pan, Moksh Jain, Kanika Madan, and Yoshua Bengio. Pre-training and fine-tuning generative flow networks, 2023a. 14\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nLing Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. Better training of GFlowNets with local credit and incomplete trajectories. International Conference on Machine Learning (ICML) , 2023b. Ling Pan, Dinghuai Zhang, Aaron Courville, Longbo Huang, and Yoshua Bengio. Generative augmented flow networks. International Conference on Learning Representations (ICLR) , 2023c. Ling Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic generative flow networks. Uncertainty in Artificial Intelligence (UAI) , 2023d. Seohong Park, Qiyang Li, and Sergey Levine. Flow q-learning. In Forty-second International Conference on Machine Learning , 2025. URL https://openreview.net/forum?id=KVf2SFL1pi . Guilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/open-r1/codeforces , 2025. Samuel Pfrommer, Yixiao Huang, and Somayeh Sojoudi. Reinforcement learning for flow-matching policies. arXiv preprint arXiv:2507.15073 , 2025. Abhinav Rastogi, Albert Q Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910 , 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024. Max W. Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet training. ArXiv , abs/2305.07170, 2023. URL https://api.semanticscholar.org/CorpusID:258676487 . Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 , 2024.',\n",
       "  'start_char': 11786,\n",
       "  'end_char': 14337,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 331},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 7,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460-9471, 2022. Richard S Sutton, Andrew G Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience , 11(1):126-134, 1999a. Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S.',\n",
       "  'start_char': 14338,\n",
       "  'end_char': 14804,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 61},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 8,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Solla, T. Leen, and K. Müller, editors, Advances in Neural Information Processing Systems , volume 12. MIT Press, 1999b. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0b ed98e80ade0a5c43b0f-Paper.pdf . Qwen Team. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.g ithub.io/blog/qwen2.5/ . 15\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nDaniil Tiapkin, Nikita Morozov, Alexey Naumov, and Dmitry P Vetrov. Generative flow networks as entropy-regularized rl. In International Conference on Artificial Intelligence and Statistics , pages 4213-4221. PMLR, 2024. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939 , 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824-24837, 2022. Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Training llms for divergent reasoning with minimal examples. In Forty-second International Conference on Machine Learning , 2025a. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 , 2025b. Taeyoung Yun, Dinghuai Zhang, Jinkyoo Park, and Ling Pan. Learning to sample effective and diverse prompts for text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 23625-23635, 2025. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems , 35:15476-15488, 2022. David W.',\n",
       "  'start_char': 14805,\n",
       "  'end_char': 16819,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 276},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 9,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Zhang, Corrado Rainone, Markus F.',\n",
       "  'start_char': 16820,\n",
       "  'end_char': 16853,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 5},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 10,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Peschl, and Roberto Bondesan. Robust scheduling with gflownets. ArXiv , abs/2302.05446, 2023a. URL https://api.semanticscholar.org/Corp usID:256827133 . Dinghuai Zhang, Ricky T. Q.',\n",
       "  'start_char': 16854,\n",
       "  'end_char': 17034,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 21},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 11,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models with GFlowNets and beyond. arXiv preprint arXiv:2209.02606v2 , 2022a. Dinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML) , 2022b. Dinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C. Courville, Yoshua Bengio, and Ling Pan. Let the flows tell: Solving graph combinatorial optimization problems with gflownets. ArXiv , abs/2305.17010, 2023b.',\n",
       "  'start_char': 17035,\n",
       "  'end_char': 17604,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 75},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 12,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Dinghuai Zhang, Ricky T. Q.',\n",
       "  'start_char': 17007,\n",
       "  'end_char': 17034,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 5},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 13,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization, 2024a. Dinghuai Zhang, Ling Pan, Ricky T.',\n",
       "  'start_char': 17633,\n",
       "  'end_char': 17835,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 26},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 14,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Q.',\n",
       "  'start_char': 9033,\n",
       "  'end_char': 9035,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 1},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'References',\n",
       "  'section_type': 'References',\n",
       "  'chunk_index': 15,\n",
       "  'total_chunks': 16,\n",
       "  'chunk_text': 'Chen, Aaron Courville, and Yoshua Bengio. Distributional gflownets with quantile flows, 2024b. Dinghuai Zhang, Yizhe Zhang, Jiatao Gu, Ruixiang ZHANG, Joshua M. Susskind, Navdeep Jaitly, and Shuangfei Zhai. Improving GFlownets for text-to-image diffusion alignment. Transactions on Machine Learning Research , 2025a. ISSN 2835-8856. URL https://openreview.net/forum ?id=XDbY3qhM42 . 16\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nKaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. A survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827 , 2025b. Mingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, and Yoshua Bengio. Phylogfn: Phylogenetic inference with generative flow networks, 2024. Heiko Zimmermann, Fredrik Lindsten, J.-W. van de Meent, and Christian Andersson Naesseth. A variational perspective on generative flow networks. ArXiv , abs/2210.07992, 2022. URL https: //api.semanticscholar.org/CorpusID:252907672 . 17\\nFlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       "  'start_char': 17839,\n",
       "  'end_char': 18996,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 151},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'A. Proof of Proposition 1',\n",
       "  'section_type': 'A. Proof of Proposition 1',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'We begin by analyzing the gradient of the Kullback-Leibler (KL) divergence between the policy 𝜋𝜃 ( y | x ) and the target reward distribution exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) :\\nNext, consider the trajectory balance objective used in GFlowNets learning [Bartoldson et al., 2025, Bengio et al., 2023b, Lee et al., 2024], defined as:\\nTaking the gradient of this objective with respect to 𝜃 yields:\\nThus, minimizing the KL divergence is equivalent (up to a constant) to minimizing the trajectory balance loss, confirming Proposition 1.',\n",
       "  'start_char': 0,\n",
       "  'end_char': 528,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 94},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'B. Theoretical Analysis',\n",
       "  'section_type': 'B. Theoretical Analysis',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'We conduct an interpretation of FlowRL that clarifies the role of each component in the objective.\\nProposition 5. Minimizing the KL divergence in Eq. 5 is equivalent (in terms of gradients) to jointly maximizing reward and policy entropy:\\nRemark 6 ( FlowRL beyond reward maximization ) . Proposition 5 reveals that FlowRL can be interpreted as jointly maximizing expected reward and policy entropy. This shift encourages the policy to explore a broader set of high-quality solutions, enabling more diverse and generalizable behaviors on reasoning tasks. Our interpretation also aligns with prior work that views GFlowNets training as a form of maximum entropy RL [Deleu et al., 2024, Mohammadpour et al., 2024].\\n18\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nThe proof of Proposition 5 is provided as below.\\nRecall from Eq. 3 and Eq. 5 that the FlowRL objective is sourced from the minimization of a KL divergence:\\nRearranging the terms, we obtain:\\nFinally, we express the FlowRL objective in its compact form:\\n\\uf8f0\\n\\uf8fb\\nTherefore, minimizing the FlowRL objective can be interpreted as jointly maximizing reward and entropy, while also aligning the policy with a structured prior. The reward term drives task performance, while the normalization term 𝑍𝜙 ( x ) ensures consistency with a properly normalized target distribution. This encourages the policy 𝜋𝜃 to cover the entire reward-weighted distribution rather than collapsing to a few high-reward modes. The reference policy 𝜋 ref provides inductive bias that regularizes the policy toward desirable structures, and the entropy term H( 𝜋𝜃 ) encourages diversity in sampled solutions. Together, these components promote better generalization of FlowRL.',\n",
       "  'start_char': 0,\n",
       "  'end_char': 1711,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 264},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'C. GFlowNets',\n",
       "  'section_type': 'C. GFlowNets',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'We follow the notation of [He et al., 2025, Madan et al., 2023] to introduce the fundamentals of GFlowNets. Let X denote the compositional objects and 𝑅 be a reward function that assigns nonnegative values to each object 𝑥 ∈ X . GFlowNets aim to learn a sequential, constructive sampling policy 𝜋 that generates objects 𝑥 with probabilities proportional to their rewards, i.e., 𝜋 ( 𝑥 ) ∝ 𝑅 ( 𝑥 ) . This process can be represented as a directed acyclic graph (DAG) G = (S , A) , where the vertices 𝑠 ∈ S are referred to as states , and the directed edges ( 𝑢 → 𝑣 ) ∈ A are called actions . The generation of an object 𝑥 ∈ X corresponds to a complete trajectory 𝜏 = ( 𝑠 0 → · · · → 𝑠 𝑛 ) ∈ T within the DAG, beginning at the initial state 𝑠 0 and ending at a terminal state 𝑠 𝑛 ∈ X . The state flow 𝐹 ( 𝑠 ) is defined as a non-negative weight assigned to each state 𝑠 ∈ S . The forward policy 𝑃𝐹 ( 𝑠 ′ | 𝑠 ) specifies the transition probability to a child state 𝑠 ′ , while the backward policy 𝑃𝐵 ( 𝑠 | 𝑠 ′ ) specifies the transition probability to a parent state 𝑠 . To this end, detailed balance objective enforces local flow consistency across every edge ( 𝑠 → 𝑠 ′ ) ∈ A :\\n19\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTo achieve this flow consistency, GFlowNets employ training objectives at different levels of granularity, including detailed balance [Bengio et al., 2023b], trajectory balance [Malkin et al., 2022], and subtrajectory balance [Madan et al., 2023]. Leveraging their diversity-seeking behavior, GFlowNets have been successfully applied across a range of domains, including molecule generation [Cretu et al., 2024], diffusion fine-tuning [Liu et al., 2025b, Zhang et al., 2025a], and amortized reasoning [Hu et al., 2024, Yu et al., 2025a]. Among various training objective in GFlowNets, trajectory balance maintains flow consistency at the trajectory level, defined as:\\nFurthermore, sub-trajectory balance achieves local balance on arbitrary subpaths 𝜏𝑖 : 𝑗 = { 𝑠 𝑖 → · · ·→ 𝑠 𝑗 } , offering a more stable and less biased learning signal. We build on trajectory balance to extend our KL-based objective through a gradient-equivalence formulation (Prop. 1), and further improve it to better support long CoT reasoning in RL.\\nTable 5 | Math reasoning performance (Avg@64) at temperature = 0 . 6. Relative improvements are shown as subscripts, with positive gains in green and negative changes in red. FlowRL consistently outperforms all baselines and achieves the best average score under this low-temperature setting.\\nTable 6 | Math reasoning performance (Avg@64) at temperature = 1 . 0. Relative improvements are shown as subscripts, with positive gains in green. FlowRL maintains robust performance under higher generation randomness and continues to outperform all baselines on average.\\n20\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 7 | Ablation study on the effect of the 𝛽 parameter in FlowRL. We report Avg@16 accuracy across six math reasoning benchmarks for different values of 𝛽 .',\n",
       "  'start_char': 0,\n",
       "  'end_char': 3038,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 534},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'Diversity Evaluation Prompt',\n",
       "  'section_type': 'Diversity Evaluation Prompt',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'System: You are evaluating the DIVERSITY of solution approaches for a mathematics competition problem. Focus on detecting even SUBTLE differences in methodology that indicate different problemsolving strategies.',\n",
       "  'start_char': 0,\n",
       "  'end_char': 211,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 27},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'PROBLEM:',\n",
       "  'section_type': 'PROBLEM:',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': '{problem}',\n",
       "  'start_char': 0,\n",
       "  'end_char': 9,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 1},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': '16 SOLUTION ATTEMPTS:',\n",
       "  'section_type': '16 SOLUTION ATTEMPTS:',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': '{formatted_responses}',\n",
       "  'start_char': 0,\n",
       "  'end_char': 21,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 1},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'Score 1 - Minimal Diversity:',\n",
       "  'section_type': 'Score 1 - Minimal Diversity:',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': 'Same mathematical setup, same variable choices, same solution path\\n14+ responses use essentially identical approaches\\nOnly trivial differences (arithmetic, notation, wording)\\nIndicates very low exploration/diversity in the generation process',\n",
       "  'start_char': 0,\n",
       "  'end_char': 241,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 29},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'Score 2 - Low Diversity:',\n",
       "  'section_type': 'Score 2 - Low Diversity:',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': '1-2 alternative approaches appear but are rare\\n11-13 responses use the same main approach\\nMinor variations within the dominant method (different substitutions, orderings)\\nSome exploration but heavily biased toward one strategy',\n",
       "  'start_char': 0,\n",
       "  'end_char': 226,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 31},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'Score 3 - Moderate Diversity:',\n",
       "  'section_type': 'Score 3 - Moderate Diversity:',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': '2-3 distinct alternative approaches present\\n7-10 responses use the most common approach\\nNoticeable variation in problem setup or mathematical techniques\\nBalanced mix showing reasonable exploration',\n",
       "  'start_char': 0,\n",
       "  'end_char': 196,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 25},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'Score 4 - High Diversity:',\n",
       "  'section_type': 'Score 4 - High Diversity:',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': '3-4 distinct solution strategies well-represented\\n4-6 responses use the most common approach\\nMultiple mathematical techniques and problem framings\\nStrong evidence of diverse exploration strategies',\n",
       "  'start_char': 0,\n",
       "  'end_char': 196,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 24},\n",
       " {'arxiv_id': '2509.15217v1',\n",
       "  'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       "  'primary_category': 'cs.AI',\n",
       "  'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       "  'section_title': 'Score 5 - Maximum Diversity:',\n",
       "  'section_type': 'Score 5 - Maximum Diversity:',\n",
       "  'chunk_index': 0,\n",
       "  'total_chunks': 1,\n",
       "  'chunk_text': '4+ distinctly different solution strategies\\nNo single approach dominates ( ≤ 3 responses use same method)\\nWide variety of mathematical techniques and creative approaches\\nIMPORTANT: Focusing on the DIVERSITY of the attempted approaches. Return ONLY a number from 1 to 5.\\nExcellent exploration and generation diversity\\n21',\n",
       "  'start_char': 0,\n",
       "  'end_char': 319,\n",
       "  'published_date': '2025-09-18T17:59:11Z',\n",
       "  'authors': ['Yue Xin',\n",
       "   'Wenyuan Wang',\n",
       "   'Rui Pan',\n",
       "   'Ruida Wang',\n",
       "   'Howard Meng',\n",
       "   'Renjie Pi',\n",
       "   'Shizhe Diao',\n",
       "   'Tong Zhang'],\n",
       "  'word_count': 47}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['arxiv_id', 'title', 'abstract', 'authors', 'categories', 'primary_category', 'published', 'updated', 'pdf_url', 'arxiv_url', 'doi', 'journal_ref', 'content', 'sections', 'is_processed', 'metrics', 'research_area', 'research_areas_all', 'word_count', 'author_count', 'institutions'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict_keys(['arxiv_id', 'title', 'summary', 'authors', 'categories', 'primary_category', 'published', 'updated', 'pdf_url', 'abs_url', 'doi', 'journal_ref', 'content', 'sections', 'is_processed', 'datasets', 'metrics', 'research_area', 'research_areas_all', 'word_count', 'author_count', 'institutions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arxiv_id': '2509.15217v1',\n",
       " 'title': 'Generalizable Geometric Image Caption Synthesis',\n",
       " 'abstract': 'Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\\\\%\\\\text{-}4.8\\\\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\\\\%\\\\text{-}3.9\\\\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.',\n",
       " 'authors': ['Yue Xin',\n",
       "  'Wenyuan Wang',\n",
       "  'Rui Pan',\n",
       "  'Ruida Wang',\n",
       "  'Howard Meng',\n",
       "  'Renjie Pi',\n",
       "  'Shizhe Diao',\n",
       "  'Tong Zhang'],\n",
       " 'categories': ['cs.AI', 'cs.CV', 'cs.LG'],\n",
       " 'primary_category': 'cs.AI',\n",
       " 'published': '2025-09-18T17:59:11Z',\n",
       " 'updated': '2025-09-18T17:59:11Z',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2509.15217v1',\n",
       " 'arxiv_url': 'http://arxiv.org/abs/2509.15217v1',\n",
       " 'doi': '',\n",
       " 'journal_ref': '',\n",
       " 'content': \"## FlowRL: Matching Reward Distributions for LLM Reasoning\\n\\nXuekai Zhu 1 , Daixuan Cheng 6 , Dinghuai Zhang 3 , Hengli Li 5 , Kaiyan Zhang 4 , Che Jiang 4 , Youbang Sun 4 , Ermo Hua 4 , Yuxin Zuo 4 , Xingtai Lv 4 , Qizheng Zhang 7 , Lin Chen 1 , Fanghao Shao 1 , Bo Xue 1 , Yunchong Song 1 , Zhenjie Yang 1 , Ganqu Cui 2 , Ning Ding 4 , 2 , Jianfeng Gao 3 , Xiaodong Liu 3 , Bowen Zhou 4 , 2 ‡ , Hongyuan Mei 8 ‡ , Zhouhan Lin 1 , 2 ‡\\n\\n1 Shanghai Jiao Tong University 2 Shanghai AI Laboratory 3 Microsoft Research 4 Tsinghua University 5 Peking University 6 Renmin University of China 7 Stanford University 8 Toyota Technological Institute at Chicago\\n\\n/envelope hongyuanmei@gmail.com /envelope xuekaizhu0@gmail.com /github FlowRL ‡ Corresponding Authors.\\n\\nAbstract | We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods ( e.g. , PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10 . 0% over GRPO and 5 . 1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.\\n\\nFigure 1 | Top : Comparison between distribution-matching and reward-maximizing approaches. FlowRL (left) learns to match the full reward distribution, maintaining diversity across multiple modes with low KL divergence. In contrast, reward-maximizing methods like GRPO (right) concentrate on a single high-reward peak, leading to mode collapse and higher KL divergence. Bottom : Performance comparison. FlowRL consistently outperforms GRPO across math and code domains.\\n\\n## 1. Introduction\\n\\nReinforcement learning (RL) plays a crucial role in the post-training of large language models (LLMs) [Zhang et al., 2025b]. A series of powerful reasoning models [Guo et al., 2025, Kavukcuoglu, 2025, Rastogi et al., 2025] have employed large-scale reinforcement learning to achieve strong performance on highly challenging benchmarks [He et al., 2024]. The evolution of RL algorithms for LLM reasoning has progressed through several key stages: REINFORCE [Sutton et al., 1999a] provides a solid baseline that is easy to implement and efficient in simple settings; PPO [Schulman et al., 2017] improves upon REINFORCE with better stability and efficiency in complex settings; GRPO [Shao et al., 2024] simplifies PPO training by eliminating value functions and relying on group comparisons, though at the cost of requiring more rollouts per update. However, all these methods share a fundamental limitation in their reward-maximizing objective.\\n\\nReward-maximizing RL methods tend to overfit to the dominant mode of the reward distribution [Gao et al., 2023, Pan et al., 2022, Skalse et al., 2022, Zelikman et al., 2022]. This often results in limited diversity among generated reasoning paths and reduces generalization to less frequent yet valid logical outcomes [Hu et al., 2023]. As illustrated in Figure 1, GRPO neglects other meaningful modes. These drawbacks become especially pronounced in complex long chain-of-thought (CoT; Wei et al., 2022) reasoning, where capturing a diverse distribution of plausible solutions is essential for effective generalization [Liu et al., 2025a]. Recent approaches adjust the clip ratio [Yu et al., 2025b], augment the advantage function with an entropy-based term [Cheng et al., 2025], or selectively promote high-entropy tokens [Wang et al., 2025], thereby dynamically adapting the training data distribution and implicitly increasing diversity during training. This raises a fundamental question: How can we promote diverse exploration to prevent convergence to dominant solution patterns in RL training?\\n\\nIn this paper, we propose FlowRL , a policy optimization algorithm that aligns the policy model with the full reward distribution, encouraging mode coverage. FlowRL achieves more efficient exploration by fundamentally shifting from reward maximization to reward distribution matching, thereby addressing the inherent mode-collapse limitations of previous RL approaches. As illustrated in Figure 1, the core idea of FlowRL is to introduce a learnable partition function that normalizes scalar rewards into a target distribution, and to minimize the reverse KL divergence between the policy and this reward-induced distribution. We develop this KL objective based on the trajectory balance formulation from GFlowNets [Bengio et al., 2023b], providing a gradient equivalence proof that bridges generative modeling and policy optimization. To address the challenges of long CoT training, we introduce two key technical solutions: length normalization to tackle gradient explosion issues that occur with variable-length CoT reasoning, and importance sampling to correct for the distribution mismatch between generated rollouts and the current policy.\\n\\nWe compare FlowRL with mainstream RL algorithms including REINFORCE++, PPO, and GRPO across math and code domains, using both base and distilled LLMs (7B, 32B). In math domain, FlowRL outperforms GRPO and PPO by 10 . 0% and 5 . 1%, respectively, demonstrating consistent improvements across six challenging math benchmarks. Furthermore, FlowRL surpasses both PPO and GRPO on three challenging coding benchmarks, highlighting its strong generalization capabilities in code reasoning tasks. To understand what drives these performance gains, we analyze the diversity of generated reasoning paths. This diversity analysis confirms that FlowRL generates substantially more diverse rollouts than baseline methods, validating our approach's effectiveness in exploring multiple solution strategies.\\n\\nContributions. We summarize the key contributions of this work as follows:\\n\\n- We propose FlowRL, a policy optimization algorithm that shifts from reward maximization to\\n\\n- reward distribution matching via flow balance, encouraging diverse reasoning path exploration while addressing the inherent mode-collapse limitations of existing RL methods.\\n- FlowRL outperforms GRPO and PPO by 10.0% and 5.1% respectively across math benchmarks and demonstrates strong generalization on code reasoning tasks, with diversity analysis confirming substantially more diverse solution exploration.\\n- We introduce length normalization and importance sampling to enable effective training on variablelength CoT reasoning, addressing gradient explosion and sampling mismatch issues.\\n\\n## 2. Preliminaries\\n\\nReinforcement Learning for Reasoning. Weformulate reasoning as a conditional generation problem, where the policy model receives a question x ∈ X and generates an answer y ∈ Y . The objective is to learn a policy 𝜋𝜃 ( y | x ) that produces high-quality answers under task-specific reward signals 𝑟 . To better illustrate the policy optimization procedure, we provide a detailed formulation of GRPO below. For each question x , GRPO samples a group of answers { y 1 , y 2 , . . . , y 𝐺 } from old policy 𝜋𝜃𝑜𝑙𝑑 and updates the model by maximizing the following objective:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝜖 and 𝜆 are hyper-parameters. Here, 𝐴𝑖 denotes the advantage, computed by normalizing the group reward values { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } as 𝐴𝑖 = 𝑟 𝑖 -mean ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) std ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) . Compared to GRPO, REINFORCE applies the policy gradient directly, without advantage normalization, clipping, or KL regularization. PPO uses a critic model to estimate the advantage and employs importance sampling to stabilize policy updates.\\n\\nGFlowNets. Generative Flow Networks [Bengio et al., 2023a] are a probabilistic framework for training stochastic policies to sample discrete, compositional objects ( e.g. , graphs, sequences) in proportion to a given reward. As shown in Figure 2, the core principle of GFlowNets is to balance the forward and backward probability flows at each state, inspired by flow matching [Bengio et al., 2021]. The initial flow is estimated by 𝑍𝜙 ( 𝑠 0 ) at the initial state 𝑠 0 . The output flow is equal to the outcome reward 𝑟 ( 𝑠 𝑛 ) conditioned at the final state 𝑠 𝑛 . Following Lee et al. [2024], we use a 3-layer MLP to parameterize 𝑍𝜙 . This flow-balancing mechanism facilitates the discovery of diverse,\\n\\nFigure 2 | GFlowNets [Bengio et al., 2023a], a flow-balance perspective on reinforcement learning. The initial flow 𝑍𝜙 ( 𝑠 0 ) injects probability mass into the environment, which is transported through intermediate states by the policy 𝜋𝜃 and accumulated at terminal states in proportion to the scalar rewards.\\n\\nhigh-reward solutions by ensuring proper exploration of the solution space. See Appendix C for detailed GFlowNets background.\\n\\n## 3. Methodology\\n\\nIn this section, we first formulate distribution matching in reinforcement learning through reverse KL divergence and establish its connection to trajectory balance from GFlowNets. To address the challenges of gradient explosion and sampling mismatch encountered during long CoT training, we further incorporate length normalization and importance sampling. Using this enhanced framework, we derive a flow-balanced objective, termed FlowRL .\\n\\n## 3.1. From Reward Maximization to Distribution Matching\\n\\nAs illustrated in Figure 1, recent powerful large reasoning models typically employ reward-maximizing RL algorithms, such as PPO or GRPO. However, these methods tend to optimize toward the dominant reward mode, frequently resulting in mode collapse and the neglect of other plausible, high-quality reasoning paths. To address this fundamental limitation, we propose optimizing the policy by aligning its output distribution to a target reward distribution. A simple yet effective way to achieve this is to minimize the reverse KL divergence 1 between the policy and this target. However, in long CoT reasoning tasks, the available supervision in RL is a scalar reward, rather than a full distribution. Moreover, enumerating or sampling all valid trajectories to recover the true reward distribution is computationally intractable.\\n\\nInspired by energy-based modeling [Du and Mordatch, 2019, Hinton et al., 1995], we introduce a learnable partition function 𝑍𝜙 ( x ) to normalize scalar rewards into a valid target distribution. This allows us to minimize the reverse KL divergence between the policy and the reward-weighted distribution, formalized as:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝑟 ( x , y ) is the reward function, 𝛽 is a hyperparameter, 𝑍𝜙 ( x ) is the learned partition function, and the resulting target distribution is defined as ˜ 𝜋 ( y | x ) = exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) . This objective encourages the policy to sample diverse, high-reward trajectories in proportion to their rewards, rather than collapsing to dominant modes as in standard reward maximization.\\n\\nWhile the KL-based formulation provides a principled target distribution, we derive a more practical, RL-style objective that facilitates efficient policy optimization.\\n\\nProposition 1. In terms of expected gradients, minimizing the KL objective in Eq. 2 is equivalent to minimizing the trajectory balance loss used in GFlowNet [Bartoldson et al., 2025, Lee et al., 2024, Malkin et al., 2022, 2023]:\\n\\n<!-- formula-not-decoded -->\\n\\nRemark 2 ( Trajectory balance as a practical surrogate for KL minimization ) . Given the equivalence established in Proposition 1, the KL-based distribution matching objective can be reformulated as the trajectory balance loss. This reformulation provides a practical optimization approach by using a stable squared loss form rather than direct KL optimization, and by treating 𝑍𝜙 ( x ) as a learnable parameter rather than requiring explicit computation of the intractable partition function. The trajectory balance objective thus serves as a tractable surrogate for reward-guided KL minimization that can be directly integrated into existing RL frameworks.\\n\\n1 We use reverse KL since we can only sample from the policy model, not the target reward distribution.\\n\\n## 3.2. FlowRL\\n\\nAs established in Proposition 1, the target reward distribution can be approximated by optimizing the trajectory balance objective. However, applying this objective directly to long CoT reasoning introduces two key challenges:\\n\\nProblem I: Exploding gradients from long trajectories. Trajectory balance is a sequence-level objective, and applying it to long CoT reasoning with up to 8K tokens leads to exploding gradients and unstable updates. This issue is not observed in prior GFlowNets works, which typically operate on short trajectories in small discrete spaces. Specifically, the log-probability term log 𝜋𝜃 ( y | x ) decomposes into a token-wise sum, ˝ 𝑡 log 𝜋𝜃 ( y 𝑡 | y &lt;𝑡 , x ) , causing the gradient norm to potentially scale with sequence length.\\n\\nProblem II: Sampling mismatch. Mainstream RL algorithms such as PPO and GRPO commonly perform micro-batch updates and reuse trajectories collected from an old policy 𝜋𝜃 old , enabling data-efficient training. In contrast, the KL-based trajectory balance objective assumes fully onpolicy sampling, where responses are drawn from the current policy. This mismatch poses practical limitations when integrating trajectory balance into existing RL pipelines.\\n\\nThese limitations motivate our reformulation that retains the benefits of distribution matching while addressing key practical challenges. To enable this reformulation, we first redefine the reward function following established practices in GFlowNets literature [Bartoldson et al., 2025, Lee et al., 2024, Yu et al., 2025a] by incorporating a reference model as a prior constraint on the reward distribution. Specifically , we modify the original exp ( 𝛽𝑟 ( x , y )) to include the reference model:\\n\\n<!-- formula-not-decoded -->\\n\\nwhere 𝑟 ( x , y ) denotes the outcome reward commonly used in reinforcement learning and 𝜋 ref is the initial pre-trained model. We follow Guo et al. [2025] to use outcome-based reward signals, and apply group normalization to 𝑟 ( x , y ) as ˆ 𝑟 𝑖 = ( 𝑟 𝑖 -mean ( r ))/ std ( r ) , where r = { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } denotes the set of rewards within a sampled group. By substituting the redefined reward formulation Eq. 4 into Eq. 3, we derive the following objective 2 :\\n\\n<!-- formula-not-decoded -->\\n\\nRemark 3 ( Reward shaping via length normalization ) . Trajectory balance treats both the initial flow and the outcome reward as sequence-level quantities. In contrast, standard policy optimization methods such as PPO or GRPO assign rewards at the token level and compute gradients at each step. However, for trajectories of varying lengths ( e.g. , CoT responses), this mismatch can cause the log-probability term log 𝜋𝜃 ( y | x ) = ˝ | y | 𝑡 = 1 log 𝜋𝜃 ( 𝑦 𝑡 | 𝑦 &lt;𝑡 , x ) to scale with sequence length. To address this, we apply a form of reward shaping by normalizing log-probabilities with respect to sequence length. Specifically, we rescale the term as 1 | y | log 𝜋𝜃 ( y | x ) , balancing the contributions of long and short sequences and stabilizing the learning signal.\\n\\nRemark 4 ( Importance sampling for data-efficient training ) . To mitigate sampling mismatch, we employ importance sampling inspired by PPO to stabilize policy updates with off-policy data. We re-weight stale trajectories using the importance ratio 𝑤 = 𝜋𝜃 ( y | x )/ 𝜋 old ( y | x ) , which serves as a coefficient in the surrogate loss. Since our objective focuses on optimizing trajectory balance rather than expected return, we detach the gradient from the current policy to prevent excessive policy drift: 𝑤 = detach [ 𝜋𝜃 ( y | x )]/ 𝜋 old ( y | x ) . For additional stability, we incorporate PPO-style clipping to detach\\n\\n.\\n\\n<!-- formula-not-decoded -->\\n\\n2 The substitution replaces 𝛽𝑟 ( x , y ) in trajectory balance objective Eq. 3 with 𝛽𝑟 ( x , y ) + log 𝜋 ref ( y | x ) to incorporate the reference model constraint.\\n\\nIncorporating these improvements into Eq. 5, we arrive at the following FlowRL objective:\\n\\nFlowRL\\n\\n<!-- formula-not-decoded -->\\n\\nwhere the clipped importance weight 𝑤 and normalized reward ˆ 𝑟 ( x , y ) are defined as:\\n\\n<!-- formula-not-decoded -->\\n\\nWe use this objective to update the policy parameters 𝜃 during training, and refer to this strategy as FlowRL . Implementation details and theoretical analysis are provided in § 5 and § B, respectively.\\n\\n## 4. Related Work\\n\\n## 4.1. Reinforcement Learning for Reasoning\\n\\nReinforcement learning has emerged as a powerful approach for large language models post-training on reasoning tasks [Guo et al., 2025, Lightman et al., 2023b, Schulman et al., 2017, Shao et al., 2024, Sutton et al., 1999b]. Most approaches employ reward-maximizing RL to optimize expected cumulative returns. Entropy regularization [Ahmed et al., 2019, Cheng et al., 2025, Haarnoja et al., 2018] is a classical technique for mitigating mode collapse by promoting diversity in the policy's output distribution, and has also been shown to enhance reasoning capabilities in various settings [Chao et al., 2024, Eysenbach and Levine, 2021]. However, for long CoT reasoning, the extended trajectory length (e.g., 8k-16k tokens) makes it difficult for the regularization signal to effectively influence reward-maximizing learning. Recent work [Cheng et al., 2025, Cui et al., 2025, Dong et al., 2025, Wang et al., 2025] has discovered that training with more diverse or high-entropy training data can further enhance training effectiveness. Compared to traditional entropy regularization, the above methods explicitly increase the proportion of low-probability (i.e., high-entropy) tokens in the training data. In our work, we address the mode-collapse problem by fundamentally shifting from reward maximization to reward distribution matching in our RL formulation.\\n\\n## 4.2. GFlowNets\\n\\nGFlowNets [Bengio et al., 2023a] represent a class of diversity-driven algorithms designed to balance probability flows across states. They have rich connections to probabilistic modeling methods [Ma et al., Malkin et al., 2023, Zhang et al., 2022a,b, 2024a, Zimmermann et al., 2022], and control methods [Pan et al., 2023b,c,d, Tiapkin et al., 2024, Zhang et al., 2024b]. This advantage has enabled GFlowNets to achieve successful applications in multiple downstream tasks, such as molecular drug discovery [Jain et al., 2022, 2023a,b, Kim et al., 2023, 2024, Liu et al., 2022, Pan et al., 2023a, Shen et al., 2023], phylogenetic inference [Zhou et al., 2024], and combinatorial optimization [Zhang et al., 2023a,b]. For generative AI, GFlowNets provide a powerful approach to align pretrained models in scenarios such as image generation [Yun et al., 2025, Zhang et al., 2025a] and language model fine-tuning [Hu et al., 2024, Lee et al., 2024, Yu et al., 2025a]. Another line of work primarily focuses on the theoretical aspects of GFlowNets. Recent theoretical studies have interpreted GFlowNets as solving a maximum entropy reinforcement learning problem within a modified Markov Decision Process (MDP) [Deleu et al., 2024, Mohammadpour et al., 2024, Tiapkin et al., 2024]. These theoretical contributions have\\n\\ninspired us to enhance reinforcement learning from a more foundational standpoint using GFlowNets principles. A comprehensive overview of GFlowNets theory can be found in Appendix C.\\n\\n## 4.3. Flow-Matching Policies\\n\\nFlow matching simplifies diffusion-based approaches by learning vector fields that transport samples from prior to target distributions [Lipman et al., 2023]. Recent work has explored flow matching for policy optimization. McAllister et al. [2025] reformulates policy optimization using advantageweighted ratios from conditional flow matching loss, enabling flow-based policy training without expensive likelihood computations. Pfrommer et al. [2025] explored reward-weighted flow matching for improving policies beyond demonstration performance. Park et al. [2025] uses a separate one-step policy to avoid unstable backpropagation through time when training flow policies with RL. Zhang et al. [2025a] proposed a combined loss function integrating PPO and GFlowNets to optimize diffusion model alignment. However, these approaches focus on continuous control, image generation, or vision-action models, rather than addressing mode-collapse limitations in reward-maximizing RL. Inspired by flow matching principles, our work improves upon RL training to enhance training stability while promoting diverse solution exploration.\\n\\n## 5. Experiment Settings\\n\\nBackbone Models. There are two learnable modules in Eq. 6: the policy model 𝜋𝜃 and the partition function 𝑍𝜙 . For the policy model 𝜋𝜃 , we use Qwen-2.5-7B/32B [Team, 2024] for math tasks and DeepSeek-R1-Distill-Qwen-7B [DeepSeek-AI, 2025] for code tasks, respectively. For partition function 𝑍𝜙 , following Lee et al. [2024], we use a randomly initialized 3-layer MLP with hidden dimensions matching those of the base model. The reference model 𝜋 ref is the corresponding fixed pretrained model. All training scripts are based on the veRL [Sheng et al., 2024]. For the reward function, following Lee et al. [2024], we set the hyperparameter 𝛽 = 15.\\n\\nBaselines. We compare our method against three representative reward-maximization RL baselines: REINFORCE++ (R++; Hu et al., 2025, Sutton et al., 1999b), PPO [Schulman et al., 2017], and GRPO [Shao et al., 2024]. All baselines follow the official veRL recipes, with consistent training configurations. For fair comparison, all methods use the same learning rate, batch size, and training steps, and are evaluated at convergence using identical step counts.\\n\\nTraining Configuration. We experiment on both math and code domains. For the math domain, we use the training set collected from DAPO [Yu et al., 2025b]. For the code domain, we follow the setup of DeepCoder [Luo et al., 2025], using their training set. For 7B model training, we use a single node equipped with 8 NVIDIA H800 GPUs (80GB memory each). For 32B model training, we scale to 4 nodes with 32 GPUs to accommodate the larger memory requirements. All experiments use max_prompt_length = 2048 and max_response_length = 8192 across both model sizes. We use a batch size of 512 for math reasoning tasks and 64 for code reasoning tasks. We set the learning rate to 1e-6 and enable dynamic batch sizing in veRL for efficient training. For GRPO and FlowRL, we configure rollout_n = 8, meaning each prompt generates 8 response rollouts as the group size.\\n\\nEvaluation Configuration. For the math domain, we evaluate on six challenging benchmarks: AIME 2024/2025 [MAA, 2025], AMC 2023 [MAA, 2023], MATH-500 [Lightman et al., 2023a], Minerva [Lewkowycz et al., 2022], and Olympiad [He et al., 2024]. For the code domain, we evaluate on LiveCodeBench [Jain et al., 2024], CodeForces [Penedo et al., 2025], and HumanEval+ [Chen et al., 2021]. For all evaluation datasets, we perform 16 rollouts and report the average accuracy, denoted as Avg@16. We further report rating and percentile for Codeforces. During generation, we\\n\\nTable 1 | Results on math benchmarks. We report Avg@16 accuracy with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL outperforms all baselines across both 7B and 32B model scales.\\n\\n|                                       | AIME24                                | AIME25                                | AMC23                                 | MATH500                               | Minerva                               | Olympiad                              | Avg                                   |\\n|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|---------------------------------------|\\n| Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K | Qwen2.5-32B-Base, Max Response Len=8K |\\n| Backbone                              | 4.6                                   | 2.1                                   | 28.6                                  | 52.5                                  | 27.0                                  | 21.4                                  | 22.7                                  |\\n| R++                                   | 14 . 8 + 10 . 2                       | 9 . 2 + 7 . 1                         | 52 . 7 + 24 . 1                       | 44 . 4 - 8 . 1                        | 17 . 4 - 9 . 6                        | 24 . 5 + 3 . 1                        | 27.1                                  |\\n| PPO                                   | 26 . 9 + 22 . 3                       | 20 . 4 + 18 . 3                       | 76 . 4 + 47 . 8                       | 69 . 2 + 16 . 7                       | 28 . 8 + 1 . 8                        | 37 . 9 + 16 . 5                       | 43.3                                  |\\n| GRPO                                  | 23 . 1 + 18 . 5                       | 14 . 6 + 12 . 5                       | 76 . 9 + 48 . 3                       | 61 . 6 + 9 . 1                        | 19 . 0 - 8 . 0                        | 34 . 9 + 13 . 5                       | 38.3                                  |\\n| FlowRL                                | 24 . 0 + 19 . 4                       | 21 . 9 + 19 . 8                       | 73 . 8 + 45 . 2                       | 80 . 8 + 28 . 3                       | 38 . 2 + 11 . 2                       | 51 . 8 + 30 . 4                       | 48.4                                  |\\n| Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  | Qwen2.5-7B-Base, Max Response Len=8K  |\\n| Backbone                              | 4.4                                   | 2.1                                   | 30.8                                  | 54.5                                  | 22.4                                  | 24.0                                  | 23.0                                  |\\n| R++                                   | 11 . 0 + 6 . 6                        | 5 . 4 + 3 . 3                         | 66 . 7 + 35 . 9                       | 54 . 3 - 0 . 2                        | 24 . 4 + 2 . 0                        | 27 . 3 + 3 . 3                        | 31.5                                  |\\n| PPO                                   | 9 . 4 + 5 . 0                         | 7 . 3 + 5 . 2                         | 63 . 4 + 32 . 6                       | 58 . 0 + 3 . 5                        | 26 . 5 + 4 . 1                        | 27 . 3 + 3 . 3                        | 32.0                                  |\\n| GRPO                                  | 13 . 5 + 9 . 1                        | 9 . 8 + 7 . 7                         | 64 . 5 + 33 . 7                       | 57 . 1 + 2 . 6                        | 23 . 1 + 0 . 7                        | 26 . 9 + 2 . 9                        | 32.5                                  |\\n| FlowRL                                | 15 . 4 + 11 . 0                       | 10 . 8 + 8 . 7                        | 54 . 5 + 23 . 7                       | 67 . 0 + 12 . 5                       | 31 . 4 + 9 . 0                        | 34 . 6 + 10 . 6                       | 35.6                                  |\\n\\nTable 2 | Results on code benchmarks. We report metrics with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL achieves the strongest performance across all three benchmarks, demonstrating its effectiveness in code reasoning tasks.\\n\\n| Models                           | LiveCodeBench                    | LiveCodeBench                    | CodeForces                       | CodeForces      | HumanEval+     |\\n|----------------------------------|----------------------------------|----------------------------------|----------------------------------|-----------------|----------------|\\n|                                  | Avg@16                           | Pass@16                          | Rating                           | Percentile      | Avg@16         |\\n| DeepSeek-R1-Distill-Qwen-7B, Max | DeepSeek-R1-Distill-Qwen-7B, Max | DeepSeek-R1-Distill-Qwen-7B, Max | DeepSeek-R1-Distill-Qwen-7B, Max | Response        | Len=8K         |\\n| Backbone                         | 30.7                             | 49.5                             | 886.7                            | 19.4            | 80.9           |\\n| R++                              | 30 . 5 - 0 . 2                   | 52 . 7 + 3 . 2                   | 1208 . 0 + 321 . 3               | 56 . 8 + 37 . 4 | 76 . 6 - 4 . 3 |\\n| PPO                              | 35 . 1 + 4 . 4                   | 54 . 5 + 5 . 0                   | 1403 . 1 + 516 . 4               | 73 . 7 + 54 . 3 | 82 . 3 + 1 . 4 |\\n| GRPO                             | 32 . 8 + 2 . 1                   | 52 . 3 + 2 . 8                   | 1313 . 8 + 427 . 1               | 67 . 1 + 47 . 7 | 80 . 1 - 0 . 8 |\\n| FlowRL                           | 37 . 4 + 6 . 7                   | 56 . 3 + 6 . 8                   | 1549 . 5 + 662 . 8               | 83 . 3 + 63 . 9 | 83 . 3 + 2 . 4 |\\n\\nuse sampling parameters of temperature =0.6 and top_p =0.95 for all evaluations. The response length for evaluation is set to 8,192, consistent with the training configuration.\\n\\n## 6. Results\\n\\n## 6.1. Main Results\\n\\nOur experimental results, summarized in Table 1 and Table 2, demonstrate that FlowRL consistently outperforms all reward-maximization baselines across both math and code reasoning domains. Table 1 reports results on math reasoning benchmarks using both 7B and 32B base models, while Table 2 presents the corresponding results on code reasoning tasks. On math reasoning tasks, FlowRL achieves the highest average accuracy of 35.6% with the 7B model and 48.4% with the 32B model, surpassing PPO by 5.1% and GRPO by 10.1% on the 32B model. FlowRL shows strong improvements on challenging benchmarks like MATH-500 and Olympiad problems, demonstrating consistent gains\\n\\nTable 3 | Ablation study on FlowRL with Qwen2.5-7B as the base model. Avg@16 accuracy is reported across six math reasoning benchmarks. IS denotes importance sampling.\\n\\n| Method               |   AIME 2024 |   AIME 2025 |   AMC 2023 |   MATH-500 |   Minerva |   Olympiad |   Avg |\\n|----------------------|-------------|-------------|------------|------------|-----------|------------|-------|\\n| FlowRL               |       15.41 |       10.83 |      54.53 |      66.96 |     31.41 |      34.61 | 35.63 |\\n| w/o IS               |        6.25 |        7.91 |      41.4  |      56.97 |     22.19 |      25.52 | 26.71 |\\n| Zhang et al. [2025a] |       10.41 |        6.66 |      53.75 |      66.5  |     30.97 |      33.72 | 33.67 |\\n\\nacross diverse mathematical domains. On code generation tasks, FlowRL achieves compelling improvements with the highest Avg@16 score of 37.43% on LiveCodeBench, a Codeforces rating of 1549.47 with 83.3% percentile ranking, and 83.28% accuracy on HumanEval+, outperforming all baselines across the board. These consistent performance gains across both domains and model scales provide strong empirical evidence that FlowRL's flow-balanced optimization successfully enhances generalization. This improvement comes from promoting diverse solution exploration compared to previous reward-maximizing RL approaches.\\n\\n## 6.2. Ablation Studies\\n\\nWe conduct ablation studies on importance sampling and the 𝛽 hyperparameter. For importance sampling, we compared the performance with and without it, and implemented a combined loss approach proposed by Zhang et al. [2025a] that simultaneously optimizes both GFlowNets and PPO objectives. This combined loss focuses on optimizing diffusion models, and we adapt it to long CoT reasoning tasks for comparison. Table 3 demonstrates that importance sampling substantially improves FlowRL performance across all math reasoning benchmarks. Compared to Zhang et al. [2025a], using importance sampling as a trajectory-level ratio is more suitable than the combined loss of GFlowNets and PPO. The performance drop without importance sampling (from 35.63% to 26.71%) highlights the critical role of correcting for distribution mismatch between rollout generation and policy training. For the hyperparam-\\n\\nFigure 3 | Ablation study on the 𝛽 in FlowRL. 𝛽 = 15 (highlighted in blue) achieves the best performance.\\n\\neter 𝛽 , we conduct a series of parameter ablation studies, and Figure 3 shows that 𝛽 = 15 achieves optimal performance, with detailed results shown in Table 7.\\n\\n## 7. Analysis\\n\\n## 7.1. Diversity Analysis\\n\\nTo assess solution diversity , we follow the approach of Yu et al. [2025a] and employ GPT-4o-mini [OpenAI, 2024] to evaluate all responses generated by each method on AIME 24/25. The evaluation prompt is shown in Appendix C. As shown in Figure 4, FlowRL achieves higher diversity scores compared to baseline methods. This demonstrates that FlowRL improves sample diversity compared to baselines, which tend to exhibit repetitive solution patterns. This diversity evaluation reveals\\n\\nTable 4 | Case study comparing GRPO and FlowRL rollouts on an AIME problem. GRPO exhibits repetitive patterns (AM-GM × 3, identity loops × 2), while FlowRL follows a more diverse solution path.\\n\\nsignificant differences in exploration patterns across methods. This nearly doubling of diversity score compared to the strongest baseline (PPO) indicates that FlowRL generates qualitatively different solution approaches rather than minor variations of the same strategy. The diversity analysis provides empirical validation of our core hypothesis that flow-balanced optimization promotes mode coverage in complex reasoning tasks.\\n\\n## 7.2. Case Study\\n\\nTable 4 illustrates the behavioral differences between GRPO and FlowRL on a representative AIME problem. GRPO exhibits repetitive patterns, applying AMGM three times and getting stuck in identity loops, failing to solve the problem. FlowRL explores more diverse actions: it sets 𝑎 = 𝑏 , derives a cubic equation, finds the rational root, and reaches the correct answer. This shows that FlowRL successfully avoids the repetitive exploration patterns. The contrast reveals fundamental differences in exploration strategies: GRPO's reward-maximizing approach leads to exploitation of familiar techniques (AM-GM inequality) without exploring alternatives, eventually reaching contradictory conclusions like 𝑎 = 𝑏 = 𝑐 . In contrast, FlowRL's distribution-matching enables strategic decisions such as the symmetry assumption 𝑎 = 𝑏 , which\\n\\nFigure 4 | GPT-judged diversity scores on rollouts of AIME 24/25 problems. FlowRL generates more diverse solutions than R++, GRPO, and PPO.\\n\\ntransforms the problem into a tractable cubic equation 𝑎 3 -27 𝑎 + 46 = 0, allowing systematic solution through rational root testing and polynomial factorization.\\n\\n## 8. Conclusion\\n\\nIn this work, we introduce FlowRL, which transforms scalar rewards into normalized target distributions using a learnable partition function and minimizes the reverse KL divergence between the policy and target distribution. We demonstrate that this approach is theoretically equivalent to trajectory balance objectives from GFlowNets and implicitly maximizes both reward and entropy, thereby promoting diverse reasoning trajectories. To further address gradient explosion and sampling mismatch issues in long CoT reasoning, we incorporate importance sampling and length normalization. Through experiments on math and code reasoning benchmarks, FlowRL achieves consistent improvements across all tasks compared to GRPO and PPO. Our diversity analysis and case studies confirm that FlowRL generates more varied solution approaches while avoiding repetitive patterns.\\n\\n## Acknowledgments\\n\\nWe are grateful to Mingqian Feng and Yuetai Li for their valuable discussions and feedback, which helped improve the quality of this work.\\n\\n## References\\n\\n- Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of entropy on policy optimization. In International conference on machine learning , pages 151-160. PMLR, 2019.\\n- Brian R Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, and Bhavya Kailkhura. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable llm post-training. arXiv preprint arXiv:2503.18929 , 2025.\\n- Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS) , 2021.\\n- Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research , 24(210):1-55, 2023a. URL http: //jmlr.org/papers/v24/22-0364.html .\\n- Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. The Journal of Machine Learning Research , 24(1):10006-10060, 2023b.\\n- Chen-Hao Chao, Chien Feng, Wei-Fang Sun, Cheng-Kuang Lee, Simon See, and Chun-Yi Lee. Maximum entropy reinforcement learning via energy-based normalizing flow. arXiv preprint arXiv:2405.13629 , 2024.\\n- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\\n\\n- Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.\\n- Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758 , 2025.\\n- Miruna Cretu, Charles Harris, Ilia Igashov, Arne Schneuing, Marwin Segler, Bruno Correia, Julien Roy, Emmanuel Bengio, and Pietro Liò. Synflownet: Design of diverse and novel molecules with synthesis constraints. arXiv preprint arXiv:2405.01155 , 2024.\\n- Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617 , 2025.\\n- DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 .\\n- Tristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, and Yoshua Bengio. Discrete probabilistic inference as control in multi-path environments. arXiv preprint arXiv:2402.10309 , 2024.\\n- Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849 , 2025.\\n- Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in neural information processing systems , 32, 2019.\\n- Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257 , 2021.\\n- Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning , pages 10835-10866. PMLR, 2023.\\n- Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\\n- Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning , pages 1861-1870. Pmlr, 2018.\\n- Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 , 2024.\\n- Haoran He, Can Chang, Huazhe Xu, and Ling Pan. Looking backward: Retrospective backward synthesis for goal-conditioned GFlownets. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?id=fNMKqyvuZT .\\n- Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and R M Neal. The 'wake-sleep' algorithm for unsupervised neural networks. Science , 268 5214:1158-61, 1995.\\n- Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. arXiv preprint arXiv:2310.04363 , 2023.\\n\\n- Edward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/f orum?id=Ouj6p4ca60 .\\n- Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv. org/abs/2501 , 3262:32-33, 2025.\\n- Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena Simine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. International Conference on Machine Learning (ICML) , 2022.\\n- Moksh Jain, Tristan Deleu, Jason S. Hartford, Cheng-Hao Liu, Alex Hernández-García, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. ArXiv , abs/2302.00615, 2023a. URL https: //api.semanticscholar.org/CorpusID:256459319 .\\n- Moksh Jain, Sharath Chandra Raparthy, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective GFlowNets. International Conference on Machine Learning (ICML) , 2023b.\\n- Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.\\n- Koray Kavukcuoglu. Gemini 2.5: Our most intelligent AI model, 2025. URL https://blog.goo gle/technology/google-deepmind/gemini-model-thinking-updates-march-2025/ . Google Blog (The Keyword), Published Mar. 25, 2025.\\n- Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search gflownets. ArXiv , abs/2310.02710, 2023.\\n- Minsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan, Woochang Kim, Jinkyoo Park, Emmanuel Bengio, and Yoshua Bengio. Learning to scale logits for temperature-conditional gflownets, 2024.\\n- Seanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et al. Learning diverse attacks on large language models for robust red-teaming and safety tuning. arXiv preprint arXiv:2405.18540 , 2024.\\n- Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 3843-3857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/18abb eef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf .\\n- Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050 , 2023a.\\n- Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. In The Twelfth International Conference on Learning Representations , 2023b.\\n\\n- Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t .\\n- Dianbo Liu, Moksh Jain, Bonaventure F. P. Dossou, Qianli Shen, Salem Lahlou, Anirudh Goyal, Nikolay Malkin, Chris C. Emezue, Dinghuai Zhang, Nadhir Hassen, Xu Ji, Kenji Kawaguchi, and Yoshua Bengio. Gflowout: Dropout with generative flow networks. In International Conference on Machine Learning , 2022.\\n- Mingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507 , 2025a.\\n- Zhen Liu, Tim Z Xiao, , Weiyang Liu, Yoshua Bengio, and Dinghuai Zhang. Efficient diversity-preserving diffusion alignment via gradient-informed gflownets. In ICLR , 2025b.\\n- Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, Ion Stoica, and Tianjun Zhang. Deepcoder: A fully open-source 14b coder at o3-mini level, 2025. Notion Blog.\\n- Jiangyan Ma, Emmanuel Bengio, Yoshua Bengio, and Dinghuai Zhang. Baking symmetry into gflownets.\\n- MAA. American mathematics competitions - amc. https://maa.org/ , 2023.\\n- MAA. American invitational mathematics examination - aime. https://maa.org/ , 2025.\\n- Kanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning , pages 23467-23483. PMLR, 2023.\\n- Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. Advances in Neural Information Processing Systems , 35: 5955-5967, 2022.\\n- Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. International Conference on Learning Representations (ICLR) , 2023.\\n- David McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053 , 2025.\\n- Sobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropy gflownets with soft q-learning. In International Conference on Artificial Intelligence and Statistics , pages 2593-2601. PMLR, 2024.\\n- OpenAI. Gpt-4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-effic ient-intelligence/ , 2024. Accessed: 2024.\\n- Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544 , 2022.\\n- Ling Pan, Moksh Jain, Kanika Madan, and Yoshua Bengio. Pre-training and fine-tuning generative flow networks, 2023a.\\n\\n- Ling Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. Better training of GFlowNets with local credit and incomplete trajectories. International Conference on Machine Learning (ICML) , 2023b.\\n- Ling Pan, Dinghuai Zhang, Aaron Courville, Longbo Huang, and Yoshua Bengio. Generative augmented flow networks. International Conference on Learning Representations (ICLR) , 2023c.\\n- Ling Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic generative flow networks. Uncertainty in Artificial Intelligence (UAI) , 2023d.\\n- Seohong Park, Qiyang Li, and Sergey Levine. Flow q-learning. In Forty-second International Conference on Machine Learning , 2025. URL https://openreview.net/forum?id=KVf2SFL1pi .\\n- Guilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/open-r1/codeforces , 2025.\\n- Samuel Pfrommer, Yixiao Huang, and Somayeh Sojoudi. Reinforcement learning for flow-matching policies. arXiv preprint arXiv:2507.15073 , 2025.\\n- Abhinav Rastogi, Albert Q Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910 , 2025.\\n- John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\\n- Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\\n- Max W. Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet training. ArXiv , abs/2305.07170, 2023. URL https://api.semanticscholar.org/CorpusID:258676487 .\\n- Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 , 2024.\\n- Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460-9471, 2022.\\n- Richard S Sutton, Andrew G Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience , 11(1):126-134, 1999a.\\n- Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. Müller, editors, Advances in Neural Information Processing Systems , volume 12. MIT Press, 1999b. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0b ed98e80ade0a5c43b0f-Paper.pdf .\\n- Qwen Team. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.g ithub.io/blog/qwen2.5/ .\\n\\n- Daniil Tiapkin, Nikita Morozov, Alexey Naumov, and Dmitry P Vetrov. Generative flow networks as entropy-regularized rl. In International Conference on Artificial Intelligence and Statistics , pages 4213-4221. PMLR, 2024.\\n- Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939 , 2025.\\n- Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824-24837, 2022.\\n- Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Training llms for divergent reasoning with minimal examples. In Forty-second International Conference on Machine Learning , 2025a.\\n- Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 , 2025b.\\n- Taeyoung Yun, Dinghuai Zhang, Jinkyoo Park, and Ling Pan. Learning to sample effective and diverse prompts for text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 23625-23635, 2025.\\n- Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems , 35:15476-15488, 2022.\\n- David W. Zhang, Corrado Rainone, Markus F. Peschl, and Roberto Bondesan. Robust scheduling with gflownets. ArXiv , abs/2302.05446, 2023a. URL https://api.semanticscholar.org/Corp usID:256827133 .\\n- Dinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models with GFlowNets and beyond. arXiv preprint arXiv:2209.02606v2 , 2022a.\\n- Dinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML) , 2022b.\\n- Dinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C. Courville, Yoshua Bengio, and Ling Pan. Let the flows tell: Solving graph combinatorial optimization problems with gflownets. ArXiv , abs/2305.17010, 2023b.\\n- Dinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization, 2024a.\\n- Dinghuai Zhang, Ling Pan, Ricky T. Q. Chen, Aaron Courville, and Yoshua Bengio. Distributional gflownets with quantile flows, 2024b.\\n- Dinghuai Zhang, Yizhe Zhang, Jiatao Gu, Ruixiang ZHANG, Joshua M. Susskind, Navdeep Jaitly, and Shuangfei Zhai. Improving GFlownets for text-to-image diffusion alignment. Transactions on Machine Learning Research , 2025a. ISSN 2835-8856. URL https://openreview.net/forum ?id=XDbY3qhM42 .\\n\\n- Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. A survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827 , 2025b.\\n- Mingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, and Yoshua Bengio. Phylogfn: Phylogenetic inference with generative flow networks, 2024.\\n- Heiko Zimmermann, Fredrik Lindsten, J.-W. van de Meent, and Christian Andersson Naesseth. A variational perspective on generative flow networks. ArXiv , abs/2210.07992, 2022. URL https: //api.semanticscholar.org/CorpusID:252907672 .\\n\\n## A. Proof of Proposition 1\\n\\nWe begin by analyzing the gradient of the Kullback-Leibler (KL) divergence between the policy 𝜋𝜃 ( y | x ) and the target reward distribution exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) :\\n\\n<!-- formula-not-decoded -->\\n\\nNext, consider the trajectory balance objective used in GFlowNets learning [Bartoldson et al., 2025, Bengio et al., 2023b, Lee et al., 2024], defined as:\\n\\n<!-- formula-not-decoded -->\\n\\nTaking the gradient of this objective with respect to 𝜃 yields:\\n\\n<!-- formula-not-decoded -->\\n\\nThus, minimizing the KL divergence is equivalent (up to a constant) to minimizing the trajectory balance loss, confirming Proposition 1.\\n\\n## B. Theoretical Analysis\\n\\nWe conduct an interpretation of FlowRL that clarifies the role of each component in the objective.\\n\\nProposition 5. Minimizing the KL divergence in Eq. 5 is equivalent (in terms of gradients) to jointly maximizing reward and policy entropy:\\n\\n<!-- formula-not-decoded -->\\n\\nRemark 6 ( FlowRL beyond reward maximization ) . Proposition 5 reveals that FlowRL can be interpreted as jointly maximizing expected reward and policy entropy. This shift encourages the policy to explore a broader set of high-quality solutions, enabling more diverse and generalizable behaviors on reasoning tasks. Our interpretation also aligns with prior work that views GFlowNets training as a form of maximum entropy RL [Deleu et al., 2024, Mohammadpour et al., 2024].\\n\\nThe proof of Proposition 5 is provided as below.\\n\\nRecall from Eq. 3 and Eq. 5 that the FlowRL objective is sourced from the minimization of a KL divergence:\\n\\n<!-- formula-not-decoded -->\\n\\nRearranging the terms, we obtain:\\n\\n<!-- formula-not-decoded -->\\n\\nFinally, we express the FlowRL objective in its compact form:\\n\\n<!-- formula-not-decoded -->\\n\\n\\uf8f0\\n\\n\\uf8fb\\n\\nTherefore, minimizing the FlowRL objective can be interpreted as jointly maximizing reward and entropy, while also aligning the policy with a structured prior. The reward term drives task performance, while the normalization term 𝑍𝜙 ( x ) ensures consistency with a properly normalized target distribution. This encourages the policy 𝜋𝜃 to cover the entire reward-weighted distribution rather than collapsing to a few high-reward modes. The reference policy 𝜋 ref provides inductive bias that regularizes the policy toward desirable structures, and the entropy term H( 𝜋𝜃 ) encourages diversity in sampled solutions. Together, these components promote better generalization of FlowRL.\\n\\n## C. GFlowNets\\n\\nWe follow the notation of [He et al., 2025, Madan et al., 2023] to introduce the fundamentals of GFlowNets. Let X denote the compositional objects and 𝑅 be a reward function that assigns nonnegative values to each object 𝑥 ∈ X . GFlowNets aim to learn a sequential, constructive sampling policy 𝜋 that generates objects 𝑥 with probabilities proportional to their rewards, i.e., 𝜋 ( 𝑥 ) ∝ 𝑅 ( 𝑥 ) . This process can be represented as a directed acyclic graph (DAG) G = (S , A) , where the vertices 𝑠 ∈ S are referred to as states , and the directed edges ( 𝑢 → 𝑣 ) ∈ A are called actions . The generation of an object 𝑥 ∈ X corresponds to a complete trajectory 𝜏 = ( 𝑠 0 → · · · → 𝑠 𝑛 ) ∈ T within the DAG, beginning at the initial state 𝑠 0 and ending at a terminal state 𝑠 𝑛 ∈ X . The state flow 𝐹 ( 𝑠 ) is defined as a non-negative weight assigned to each state 𝑠 ∈ S . The forward policy 𝑃𝐹 ( 𝑠 ′ | 𝑠 ) specifies the transition probability to a child state 𝑠 ′ , while the backward policy 𝑃𝐵 ( 𝑠 | 𝑠 ′ ) specifies the transition probability to a parent state 𝑠 . To this end, detailed balance objective enforces local flow consistency across every edge ( 𝑠 → 𝑠 ′ ) ∈ A :\\n\\n<!-- formula-not-decoded -->\\n\\nTo achieve this flow consistency, GFlowNets employ training objectives at different levels of granularity, including detailed balance [Bengio et al., 2023b], trajectory balance [Malkin et al., 2022], and subtrajectory balance [Madan et al., 2023]. Leveraging their diversity-seeking behavior, GFlowNets have been successfully applied across a range of domains, including molecule generation [Cretu et al., 2024], diffusion fine-tuning [Liu et al., 2025b, Zhang et al., 2025a], and amortized reasoning [Hu et al., 2024, Yu et al., 2025a]. Among various training objective in GFlowNets, trajectory balance maintains flow consistency at the trajectory level, defined as:\\n\\n<!-- formula-not-decoded -->\\n\\nFurthermore, sub-trajectory balance achieves local balance on arbitrary subpaths 𝜏𝑖 : 𝑗 = { 𝑠 𝑖 → · · ·→ 𝑠 𝑗 } , offering a more stable and less biased learning signal. We build on trajectory balance to extend our KL-based objective through a gradient-equivalence formulation (Prop. 1), and further improve it to better support long CoT reasoning in RL.\\n\\nTable 5 | Math reasoning performance (Avg@64) at temperature = 0 . 6. Relative improvements are shown as subscripts, with positive gains in green and negative changes in red. FlowRL consistently outperforms all baselines and achieves the best average score under this low-temperature setting.\\n\\n| Models                | AIME 2024             | AIME 2025             | AMC 2023              | MATH-500              | Minerva               | Olympiad              | Avg                   |\\n|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\\n| Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model |\\n| Backbone              | 4.37                  | 2.08                  | 30.78                 | 54.48                 | 22.38                 | 24.02                 | 23.02                 |\\n| R++                   | 10 . 57 + 6 . 20      | 5 . 10 + 3 . 02       | 66 . 02 + 35 . 24     | 54 . 29 - 0 . 19      | 24 . 47 + 2 . 09      | 27 . 30 + 3 . 28      | 31.29                 |\\n| PPO                   | 9 . 95 + 5 . 58       | 7 . 34 + 5 . 26       | 63 . 63 + 32 . 85     | 57 . 72 + 3 . 24      | 26 . 22 + 3 . 84      | 27 . 35 + 3 . 33      | 32.03                 |\\n| GRPO                  | 14 . 01 + 9 . 64      | 10 . 73 + 8 . 65      | 64 . 10 + 33 . 32     | 57 . 41 + 2 . 93      | 23 . 17 + 0 . 79      | 27 . 11 + 3 . 09      | 32.76                 |\\n| FlowRL                | 14 . 32 + 9 . 95      | 10 . 05 + 7 . 97      | 55 . 08 + 24 . 30     | 66 . 78 + 12 . 30     | 31 . 52 + 9 . 14      | 34 . 60 + 10 . 58     | 35.39                 |\\n\\nTable 6 | Math reasoning performance (Avg@64) at temperature = 1 . 0. Relative improvements are shown as subscripts, with positive gains in green. FlowRL maintains robust performance under higher generation randomness and continues to outperform all baselines on average.\\n\\n| Models                | AIME 2024             | AIME 2025             | AMC 2023              | MATH-500              | Minerva               | Olympiad              | Avg                   |\\n|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\\n| Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model | Qwen2.5-7B Base Model |\\n| Backbone              | 3.39                  | 1.51                  | 23.90                 | 45.18                 | 16.98                 | 18.27                 | 18.20                 |\\n| R++                   | 10 . 63 + 7 . 24      | 4 . 63 + 3 . 12       | 66 . 99 + 43 . 09     | 54 . 36 + 9 . 18      | 23 . 89 + 6 . 91      | 26 . 65 + 8 . 38      | 31.19                 |\\n| PPO                   | 10 . 52 + 7 . 13      | 6 . 51 + 5 . 00       | 63 . 04 + 39 . 14     | 57 . 46 + 12 . 28     | 25 . 91 + 8 . 93      | 27 . 16 + 8 . 89      | 31.77                 |\\n| GRPO                  | 12 . 50 + 9 . 11      | 10 . 10 + 8 . 59      | 64 . 72 + 40 . 82     | 57 . 15 + 11 . 97     | 23 . 28 + 6 . 30      | 26 . 90 + 8 . 63      | 32.44                 |\\n| FlowRL                | 14 . 22 + 10 . 83     | 9 . 58 + 8 . 07       | 52 . 92 + 29 . 02     | 66 . 20 + 21 . 02     | 30 . 32 + 13 . 34     | 34 . 47 + 16 . 20     | 34.62                 |\\n\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\n\\n| Models   |   AIME 2024 |   AIME 2025 |   AMC 2023 |   MATH-500 |   Minerva |   Olympiad |   Avg |\\n|----------|-------------|-------------|------------|------------|-----------|------------|-------|\\n| 𝛽 = 5    |       13.54 |       10    |      56.09 |      58.91 |     20.79 |      28.72 | 31.34 |\\n| 𝛽 = 10   |       14.79 |       10.2  |      59.53 |      64.3  |     25.27 |      32.39 | 34.41 |\\n| 𝛽 = 15   |       15.41 |       10.83 |      54.53 |      66.96 |     31.41 |      34.61 | 35.63 |\\n| 𝛽 = 30   |       15    |       10.83 |      50.62 |      69.02 |     30.03 |      35.03 | 35.09 |\\n\\nTable 7 | Ablation study on the effect of the 𝛽 parameter in FlowRL. We report Avg@16 accuracy across six math reasoning benchmarks for different values of 𝛽 .\\n\\n## Diversity Evaluation Prompt\\n\\nSystem: You are evaluating the DIVERSITY of solution approaches for a mathematics competition problem. Focus on detecting even SUBTLE differences in methodology that indicate different problemsolving strategies.\\n\\n## PROBLEM:\\n\\n{problem}\\n\\n## 16 SOLUTION ATTEMPTS:\\n\\n{formatted_responses}\\n\\n## EVALUATION CRITERIA - Rate diversity from 1 to 5:\\n\\n## Score 1 - Minimal Diversity:\\n\\n- Same mathematical setup, same variable choices, same solution path\\n- 14+ responses use essentially identical approaches\\n- Only trivial differences (arithmetic, notation, wording)\\n- Indicates very low exploration/diversity in the generation process\\n\\n## Score 2 - Low Diversity:\\n\\n- 1-2 alternative approaches appear but are rare\\n- 11-13 responses use the same main approach\\n- Minor variations within the dominant method (different substitutions, orderings)\\n- Some exploration but heavily biased toward one strategy\\n\\n## Score 3 - Moderate Diversity:\\n\\n- 2-3 distinct alternative approaches present\\n- 7-10 responses use the most common approach\\n- Noticeable variation in problem setup or mathematical techniques\\n- Balanced mix showing reasonable exploration\\n\\n## Score 4 - High Diversity:\\n\\n- 3-4 distinct solution strategies well-represented\\n- 4-6 responses use the most common approach\\n- Multiple mathematical techniques and problem framings\\n- Strong evidence of diverse exploration strategies\\n\\n## Score 5 - Maximum Diversity:\\n\\n- 4+ distinctly different solution strategies\\n- No single approach dominates ( ≤ 3 responses use same method)\\n- Wide variety of mathematical techniques and creative approaches\\n\\nIMPORTANT: Focusing on the DIVERSITY of the attempted approaches. Return ONLY a number from 1 to 5.\\n\\n- Excellent exploration and generation diversity\",\n",
       " 'sections': [{'title': 'Content',\n",
       "   'content': 'arXiv:2509.15207v1  [cs.LG]  18 Sep 2025\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\n2025-09-17'},\n",
       "  {'title': 'FlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       "   'content': 'Xuekai Zhu 1 , Daixuan Cheng 6 , Dinghuai Zhang 3 , Hengli Li 5 , Kaiyan Zhang 4 , Che Jiang 4 , Youbang Sun 4 , Ermo Hua 4 , Yuxin Zuo 4 , Xingtai Lv 4 , Qizheng Zhang 7 , Lin Chen 1 , Fanghao Shao 1 , Bo Xue 1 , Yunchong Song 1 , Zhenjie Yang 1 , Ganqu Cui 2 , Ning Ding 4 , 2 , Jianfeng Gao 3 , Xiaodong Liu 3 , Bowen Zhou 4 , 2 ‡ , Hongyuan Mei 8 ‡ , Zhouhan Lin 1 , 2 ‡\\n1 Shanghai Jiao Tong University 2 Shanghai AI Laboratory 3 Microsoft Research 4 Tsinghua University 5 Peking University 6 Renmin University of China 7 Stanford University 8 Toyota Technological Institute at Chicago\\n/envelope hongyuanmei@gmail.com /envelope xuekaizhu0@gmail.com /github FlowRL ‡ Corresponding Authors.\\nAbstract | We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods ( e.g. , PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of 10 . 0% over GRPO and 5 . 1% over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.\\nFigure 1 | Top : Comparison between distribution-matching and reward-maximizing approaches. FlowRL (left) learns to match the full reward distribution, maintaining diversity across multiple modes with low KL divergence. In contrast, reward-maximizing methods like GRPO (right) concentrate on a single high-reward peak, leading to mode collapse and higher KL divergence. Bottom : Performance comparison. FlowRL consistently outperforms GRPO across math and code domains.\\nDistribution-matching: FlowRL\\nKL = 0.11\\nKL = 8.68\\nReward-maximizing\\n∶\\nR++, PPO and GRPO\\nMath Average Score\\nCodeForces Rating\\nFlowRL: Matching Reward Distributions for LLM Reasoning'},\n",
       "  {'title': '1. Introduction',\n",
       "   'content': \"Reinforcement learning (RL) plays a crucial role in the post-training of large language models (LLMs) [Zhang et al., 2025b]. A series of powerful reasoning models [Guo et al., 2025, Kavukcuoglu, 2025, Rastogi et al., 2025] have employed large-scale reinforcement learning to achieve strong performance on highly challenging benchmarks [He et al., 2024]. The evolution of RL algorithms for LLM reasoning has progressed through several key stages: REINFORCE [Sutton et al., 1999a] provides a solid baseline that is easy to implement and efficient in simple settings; PPO [Schulman et al., 2017] improves upon REINFORCE with better stability and efficiency in complex settings; GRPO [Shao et al., 2024] simplifies PPO training by eliminating value functions and relying on group comparisons, though at the cost of requiring more rollouts per update. However, all these methods share a fundamental limitation in their reward-maximizing objective.\\nReward-maximizing RL methods tend to overfit to the dominant mode of the reward distribution [Gao et al., 2023, Pan et al., 2022, Skalse et al., 2022, Zelikman et al., 2022]. This often results in limited diversity among generated reasoning paths and reduces generalization to less frequent yet valid logical outcomes [Hu et al., 2023]. As illustrated in Figure 1, GRPO neglects other meaningful modes. These drawbacks become especially pronounced in complex long chain-of-thought (CoT; Wei et al., 2022) reasoning, where capturing a diverse distribution of plausible solutions is essential for effective generalization [Liu et al., 2025a]. Recent approaches adjust the clip ratio [Yu et al., 2025b], augment the advantage function with an entropy-based term [Cheng et al., 2025], or selectively promote high-entropy tokens [Wang et al., 2025], thereby dynamically adapting the training data distribution and implicitly increasing diversity during training. This raises a fundamental question: How can we promote diverse exploration to prevent convergence to dominant solution patterns in RL training?\\nIn this paper, we propose FlowRL , a policy optimization algorithm that aligns the policy model with the full reward distribution, encouraging mode coverage. FlowRL achieves more efficient exploration by fundamentally shifting from reward maximization to reward distribution matching, thereby addressing the inherent mode-collapse limitations of previous RL approaches. As illustrated in Figure 1, the core idea of FlowRL is to introduce a learnable partition function that normalizes scalar rewards into a target distribution, and to minimize the reverse KL divergence between the policy and this reward-induced distribution. We develop this KL objective based on the trajectory balance formulation from GFlowNets [Bengio et al., 2023b], providing a gradient equivalence proof that bridges generative modeling and policy optimization. To address the challenges of long CoT training, we introduce two key technical solutions: length normalization to tackle gradient explosion issues that occur with variable-length CoT reasoning, and importance sampling to correct for the distribution mismatch between generated rollouts and the current policy.\\nWe compare FlowRL with mainstream RL algorithms including REINFORCE++, PPO, and GRPO across math and code domains, using both base and distilled LLMs (7B, 32B). In math domain, FlowRL outperforms GRPO and PPO by 10 . 0% and 5 . 1%, respectively, demonstrating consistent improvements across six challenging math benchmarks. Furthermore, FlowRL surpasses both PPO and GRPO on three challenging coding benchmarks, highlighting its strong generalization capabilities in code reasoning tasks. To understand what drives these performance gains, we analyze the diversity of generated reasoning paths. This diversity analysis confirms that FlowRL generates substantially more diverse rollouts than baseline methods, validating our approach's effectiveness in exploring multiple solution strategies.\\nContributions. We summarize the key contributions of this work as follows:\\nWe propose FlowRL, a policy optimization algorithm that shifts from reward maximization to\\n2\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nreward distribution matching via flow balance, encouraging diverse reasoning path exploration while addressing the inherent mode-collapse limitations of existing RL methods.\\nFlowRL outperforms GRPO and PPO by 10.0% and 5.1% respectively across math benchmarks and demonstrates strong generalization on code reasoning tasks, with diversity analysis confirming substantially more diverse solution exploration.\\nWe introduce length normalization and importance sampling to enable effective training on variablelength CoT reasoning, addressing gradient explosion and sampling mismatch issues.\"},\n",
       "  {'title': '2. Preliminaries',\n",
       "   'content': 'Reinforcement Learning for Reasoning. Weformulate reasoning as a conditional generation problem, where the policy model receives a question x ∈ X and generates an answer y ∈ Y . The objective is to learn a policy 𝜋𝜃 ( y | x ) that produces high-quality answers under task-specific reward signals 𝑟 . To better illustrate the policy optimization procedure, we provide a detailed formulation of GRPO below. For each question x , GRPO samples a group of answers { y 1 , y 2 , . . . , y 𝐺 } from old policy 𝜋𝜃𝑜𝑙𝑑 and updates the model by maximizing the following objective:\\nwhere 𝜖 and 𝜆 are hyper-parameters. Here, 𝐴𝑖 denotes the advantage, computed by normalizing the group reward values { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } as 𝐴𝑖 = 𝑟 𝑖 -mean ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) std ( { 𝑟 1 ,𝑟 2 , ··· ,𝑟 𝐺 }) . Compared to GRPO, REINFORCE applies the policy gradient directly, without advantage normalization, clipping, or KL regularization. PPO uses a critic model to estimate the advantage and employs importance sampling to stabilize policy updates.\\nGFlowNets. Generative Flow Networks [Bengio et al., 2023a] are a probabilistic framework for training stochastic policies to sample discrete, compositional objects ( e.g. , graphs, sequences) in proportion to a given reward. As shown in Figure 2, the core principle of GFlowNets is to balance the forward and backward probability flows at each state, inspired by flow matching [Bengio et al., 2021]. The initial flow is estimated by 𝑍𝜙 ( 𝑠 0 ) at the initial state 𝑠 0 . The output flow is equal to the outcome reward 𝑟 ( 𝑠 𝑛 ) conditioned at the final state 𝑠 𝑛 . Following Lee et al. [2024], we use a 3-layer MLP to parameterize 𝑍𝜙 . This flow-balancing mechanism facilitates the discovery of diverse,\\nFigure 2 | GFlowNets [Bengio et al., 2023a], a flow-balance perspective on reinforcement learning. The initial flow 𝑍𝜙 ( 𝑠 0 ) injects probability mass into the environment, which is transported through intermediate states by the policy 𝜋𝜃 and accumulated at terminal states in proportion to the scalar rewards.\\n𝑠\\n!\\n𝑠\\n\"\\n𝑠\\n#\\n𝑠\\n$\\n𝑠\\n%\\n𝑠\\n&\\n𝑠\\n\\'\\n𝑠\\n(\\n𝑠\\n)\\n𝑠\\n*\\n𝑠\\n\"!\\n𝑠\\n+\\n𝑠\\n+\\n𝑆\\n!\\n𝑠\\n+\\n𝑠\\n+\\n𝑠\\n+\\nIn Flow\\nZ\\n\"\\n𝑠\\n#\\nOut Flow r\\n(𝜏)\\nhigh-reward solutions by ensuring proper exploration of the solution space. See Appendix C for detailed GFlowNets background.\\n3\\nFlowRL: Matching Reward Distributions for LLM Reasoning'},\n",
       "  {'title': '3. Methodology',\n",
       "   'content': 'In this section, we first formulate distribution matching in reinforcement learning through reverse KL divergence and establish its connection to trajectory balance from GFlowNets. To address the challenges of gradient explosion and sampling mismatch encountered during long CoT training, we further incorporate length normalization and importance sampling. Using this enhanced framework, we derive a flow-balanced objective, termed FlowRL .'},\n",
       "  {'title': '3.1. From Reward Maximization to Distribution Matching',\n",
       "   'content': 'As illustrated in Figure 1, recent powerful large reasoning models typically employ reward-maximizing RL algorithms, such as PPO or GRPO. However, these methods tend to optimize toward the dominant reward mode, frequently resulting in mode collapse and the neglect of other plausible, high-quality reasoning paths. To address this fundamental limitation, we propose optimizing the policy by aligning its output distribution to a target reward distribution. A simple yet effective way to achieve this is to minimize the reverse KL divergence 1 between the policy and this target. However, in long CoT reasoning tasks, the available supervision in RL is a scalar reward, rather than a full distribution. Moreover, enumerating or sampling all valid trajectories to recover the true reward distribution is computationally intractable.\\nInspired by energy-based modeling [Du and Mordatch, 2019, Hinton et al., 1995], we introduce a learnable partition function 𝑍𝜙 ( x ) to normalize scalar rewards into a valid target distribution. This allows us to minimize the reverse KL divergence between the policy and the reward-weighted distribution, formalized as:\\nwhere 𝑟 ( x , y ) is the reward function, 𝛽 is a hyperparameter, 𝑍𝜙 ( x ) is the learned partition function, and the resulting target distribution is defined as ˜ 𝜋 ( y | x ) = exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) . This objective encourages the policy to sample diverse, high-reward trajectories in proportion to their rewards, rather than collapsing to dominant modes as in standard reward maximization.\\nWhile the KL-based formulation provides a principled target distribution, we derive a more practical, RL-style objective that facilitates efficient policy optimization.\\nProposition 1. In terms of expected gradients, minimizing the KL objective in Eq. 2 is equivalent to minimizing the trajectory balance loss used in GFlowNet [Bartoldson et al., 2025, Lee et al., 2024, Malkin et al., 2022, 2023]:\\nRemark 2 ( Trajectory balance as a practical surrogate for KL minimization ) . Given the equivalence established in Proposition 1, the KL-based distribution matching objective can be reformulated as the trajectory balance loss. This reformulation provides a practical optimization approach by using a stable squared loss form rather than direct KL optimization, and by treating 𝑍𝜙 ( x ) as a learnable parameter rather than requiring explicit computation of the intractable partition function. The trajectory balance objective thus serves as a tractable surrogate for reward-guided KL minimization that can be directly integrated into existing RL frameworks.\\n1 We use reverse KL since we can only sample from the policy model, not the target reward distribution.\\n4\\nFlowRL: Matching Reward Distributions for LLM Reasoning'},\n",
       "  {'title': '3.2. FlowRL',\n",
       "   'content': 'As established in Proposition 1, the target reward distribution can be approximated by optimizing the trajectory balance objective. However, applying this objective directly to long CoT reasoning introduces two key challenges:\\nProblem I: Exploding gradients from long trajectories. Trajectory balance is a sequence-level objective, and applying it to long CoT reasoning with up to 8K tokens leads to exploding gradients and unstable updates. This issue is not observed in prior GFlowNets works, which typically operate on short trajectories in small discrete spaces. Specifically, the log-probability term log 𝜋𝜃 ( y | x ) decomposes into a token-wise sum, ˝ 𝑡 log 𝜋𝜃 ( y 𝑡 | y <𝑡 , x ) , causing the gradient norm to potentially scale with sequence length.\\nProblem II: Sampling mismatch. Mainstream RL algorithms such as PPO and GRPO commonly perform micro-batch updates and reuse trajectories collected from an old policy 𝜋𝜃 old , enabling data-efficient training. In contrast, the KL-based trajectory balance objective assumes fully onpolicy sampling, where responses are drawn from the current policy. This mismatch poses practical limitations when integrating trajectory balance into existing RL pipelines.\\nThese limitations motivate our reformulation that retains the benefits of distribution matching while addressing key practical challenges. To enable this reformulation, we first redefine the reward function following established practices in GFlowNets literature [Bartoldson et al., 2025, Lee et al., 2024, Yu et al., 2025a] by incorporating a reference model as a prior constraint on the reward distribution. Specifically , we modify the original exp ( 𝛽𝑟 ( x , y )) to include the reference model:\\nwhere 𝑟 ( x , y ) denotes the outcome reward commonly used in reinforcement learning and 𝜋 ref is the initial pre-trained model. We follow Guo et al. [2025] to use outcome-based reward signals, and apply group normalization to 𝑟 ( x , y ) as ˆ 𝑟 𝑖 = ( 𝑟 𝑖 -mean ( r ))/ std ( r ) , where r = { 𝑟 1 , 𝑟 2 , . . . , 𝑟 𝐺 } denotes the set of rewards within a sampled group. By substituting the redefined reward formulation Eq. 4 into Eq. 3, we derive the following objective 2 :\\nRemark 3 ( Reward shaping via length normalization ) . Trajectory balance treats both the initial flow and the outcome reward as sequence-level quantities. In contrast, standard policy optimization methods such as PPO or GRPO assign rewards at the token level and compute gradients at each step. However, for trajectories of varying lengths ( e.g. , CoT responses), this mismatch can cause the log-probability term log 𝜋𝜃 ( y | x ) = ˝ | y | 𝑡 = 1 log 𝜋𝜃 ( 𝑦 𝑡 | 𝑦 <𝑡 , x ) to scale with sequence length. To address this, we apply a form of reward shaping by normalizing log-probabilities with respect to sequence length. Specifically, we rescale the term as 1 | y | log 𝜋𝜃 ( y | x ) , balancing the contributions of long and short sequences and stabilizing the learning signal.\\nRemark 4 ( Importance sampling for data-efficient training ) . To mitigate sampling mismatch, we employ importance sampling inspired by PPO to stabilize policy updates with off-policy data. We re-weight stale trajectories using the importance ratio 𝑤 = 𝜋𝜃 ( y | x )/ 𝜋 old ( y | x ) , which serves as a coefficient in the surrogate loss. Since our objective focuses on optimizing trajectory balance rather than expected return, we detach the gradient from the current policy to prevent excessive policy drift: 𝑤 = detach [ 𝜋𝜃 ( y | x )]/ 𝜋 old ( y | x ) . For additional stability, we incorporate PPO-style clipping to detach\\n.\\n2 The substitution replaces 𝛽𝑟 ( x , y ) in trajectory balance objective Eq. 3 with 𝛽𝑟 ( x , y ) + log 𝜋 ref ( y | x ) to incorporate the reference model constraint.\\n5\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nIncorporating these improvements into Eq. 5, we arrive at the following FlowRL objective:\\nFlowRL\\nwhere the clipped importance weight 𝑤 and normalized reward ˆ 𝑟 ( x , y ) are defined as:\\nWe use this objective to update the policy parameters 𝜃 during training, and refer to this strategy as FlowRL . Implementation details and theoretical analysis are provided in § 5 and § B, respectively.'},\n",
       "  {'title': '4.1. Reinforcement Learning for Reasoning',\n",
       "   'content': \"Reinforcement learning has emerged as a powerful approach for large language models post-training on reasoning tasks [Guo et al., 2025, Lightman et al., 2023b, Schulman et al., 2017, Shao et al., 2024, Sutton et al., 1999b]. Most approaches employ reward-maximizing RL to optimize expected cumulative returns. Entropy regularization [Ahmed et al., 2019, Cheng et al., 2025, Haarnoja et al., 2018] is a classical technique for mitigating mode collapse by promoting diversity in the policy's output distribution, and has also been shown to enhance reasoning capabilities in various settings [Chao et al., 2024, Eysenbach and Levine, 2021]. However, for long CoT reasoning, the extended trajectory length (e.g., 8k-16k tokens) makes it difficult for the regularization signal to effectively influence reward-maximizing learning. Recent work [Cheng et al., 2025, Cui et al., 2025, Dong et al., 2025, Wang et al., 2025] has discovered that training with more diverse or high-entropy training data can further enhance training effectiveness. Compared to traditional entropy regularization, the above methods explicitly increase the proportion of low-probability (i.e., high-entropy) tokens in the training data. In our work, we address the mode-collapse problem by fundamentally shifting from reward maximization to reward distribution matching in our RL formulation.\"},\n",
       "  {'title': '4.2. GFlowNets',\n",
       "   'content': 'GFlowNets [Bengio et al., 2023a] represent a class of diversity-driven algorithms designed to balance probability flows across states. They have rich connections to probabilistic modeling methods [Ma et al., Malkin et al., 2023, Zhang et al., 2022a,b, 2024a, Zimmermann et al., 2022], and control methods [Pan et al., 2023b,c,d, Tiapkin et al., 2024, Zhang et al., 2024b]. This advantage has enabled GFlowNets to achieve successful applications in multiple downstream tasks, such as molecular drug discovery [Jain et al., 2022, 2023a,b, Kim et al., 2023, 2024, Liu et al., 2022, Pan et al., 2023a, Shen et al., 2023], phylogenetic inference [Zhou et al., 2024], and combinatorial optimization [Zhang et al., 2023a,b]. For generative AI, GFlowNets provide a powerful approach to align pretrained models in scenarios such as image generation [Yun et al., 2025, Zhang et al., 2025a] and language model fine-tuning [Hu et al., 2024, Lee et al., 2024, Yu et al., 2025a]. Another line of work primarily focuses on the theoretical aspects of GFlowNets. Recent theoretical studies have interpreted GFlowNets as solving a maximum entropy reinforcement learning problem within a modified Markov Decision Process (MDP) [Deleu et al., 2024, Mohammadpour et al., 2024, Tiapkin et al., 2024]. These theoretical contributions have\\n6\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\ninspired us to enhance reinforcement learning from a more foundational standpoint using GFlowNets principles. A comprehensive overview of GFlowNets theory can be found in Appendix C.'},\n",
       "  {'title': '4.3. Flow-Matching Policies',\n",
       "   'content': 'Flow matching simplifies diffusion-based approaches by learning vector fields that transport samples from prior to target distributions [Lipman et al., 2023]. Recent work has explored flow matching for policy optimization. McAllister et al. [2025] reformulates policy optimization using advantageweighted ratios from conditional flow matching loss, enabling flow-based policy training without expensive likelihood computations. Pfrommer et al. [2025] explored reward-weighted flow matching for improving policies beyond demonstration performance. Park et al. [2025] uses a separate one-step policy to avoid unstable backpropagation through time when training flow policies with RL. Zhang et al. [2025a] proposed a combined loss function integrating PPO and GFlowNets to optimize diffusion model alignment. However, these approaches focus on continuous control, image generation, or vision-action models, rather than addressing mode-collapse limitations in reward-maximizing RL. Inspired by flow matching principles, our work improves upon RL training to enhance training stability while promoting diverse solution exploration.'},\n",
       "  {'title': '5. Experiment Settings',\n",
       "   'content': 'Backbone Models. There are two learnable modules in Eq. 6: the policy model 𝜋𝜃 and the partition function 𝑍𝜙 . For the policy model 𝜋𝜃 , we use Qwen-2.5-7B/32B [Team, 2024] for math tasks and DeepSeek-R1-Distill-Qwen-7B [DeepSeek-AI, 2025] for code tasks, respectively. For partition function 𝑍𝜙 , following Lee et al. [2024], we use a randomly initialized 3-layer MLP with hidden dimensions matching those of the base model. The reference model 𝜋 ref is the corresponding fixed pretrained model. All training scripts are based on the veRL [Sheng et al., 2024]. For the reward function, following Lee et al. [2024], we set the hyperparameter 𝛽 = 15.\\nBaselines. We compare our method against three representative reward-maximization RL baselines: REINFORCE++ (R++; Hu et al., 2025, Sutton et al., 1999b), PPO [Schulman et al., 2017], and GRPO [Shao et al., 2024]. All baselines follow the official veRL recipes, with consistent training configurations. For fair comparison, all methods use the same learning rate, batch size, and training steps, and are evaluated at convergence using identical step counts.\\nTraining Configuration. We experiment on both math and code domains. For the math domain, we use the training set collected from DAPO [Yu et al., 2025b]. For the code domain, we follow the setup of DeepCoder [Luo et al., 2025], using their training set. For 7B model training, we use a single node equipped with 8 NVIDIA H800 GPUs (80GB memory each). For 32B model training, we scale to 4 nodes with 32 GPUs to accommodate the larger memory requirements. All experiments use max_prompt_length = 2048 and max_response_length = 8192 across both model sizes. We use a batch size of 512 for math reasoning tasks and 64 for code reasoning tasks. We set the learning rate to 1e-6 and enable dynamic batch sizing in veRL for efficient training. For GRPO and FlowRL, we configure rollout_n = 8, meaning each prompt generates 8 response rollouts as the group size.\\nEvaluation Configuration. For the math domain, we evaluate on six challenging benchmarks: AIME 2024/2025 [MAA, 2025], AMC 2023 [MAA, 2023], MATH-500 [Lightman et al., 2023a], Minerva [Lewkowycz et al., 2022], and Olympiad [He et al., 2024]. For the code domain, we evaluate on LiveCodeBench [Jain et al., 2024], CodeForces [Penedo et al., 2025], and HumanEval+ [Chen et al., 2021]. For all evaluation datasets, we perform 16 rollouts and report the average accuracy, denoted as Avg@16. We further report rating and percentile for Codeforces. During generation, we\\n7\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 1 | Results on math benchmarks. We report Avg@16 accuracy with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL outperforms all baselines across both 7B and 32B model scales.\\nTable 2 | Results on code benchmarks. We report metrics with relative improvements shown as subscripts. Positive gains are shown in green and negative changes in red. FlowRL achieves the strongest performance across all three benchmarks, demonstrating its effectiveness in code reasoning tasks.\\nuse sampling parameters of temperature =0.6 and top_p =0.95 for all evaluations. The response length for evaluation is set to 8,192, consistent with the training configuration.'},\n",
       "  {'title': '6.1. Main Results',\n",
       "   'content': \"Our experimental results, summarized in Table 1 and Table 2, demonstrate that FlowRL consistently outperforms all reward-maximization baselines across both math and code reasoning domains. Table 1 reports results on math reasoning benchmarks using both 7B and 32B base models, while Table 2 presents the corresponding results on code reasoning tasks. On math reasoning tasks, FlowRL achieves the highest average accuracy of 35.6% with the 7B model and 48.4% with the 32B model, surpassing PPO by 5.1% and GRPO by 10.1% on the 32B model. FlowRL shows strong improvements on challenging benchmarks like MATH-500 and Olympiad problems, demonstrating consistent gains\\n8\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 3 | Ablation study on FlowRL with Qwen2.5-7B as the base model. Avg@16 accuracy is reported across six math reasoning benchmarks. IS denotes importance sampling.\\nacross diverse mathematical domains. On code generation tasks, FlowRL achieves compelling improvements with the highest Avg@16 score of 37.43% on LiveCodeBench, a Codeforces rating of 1549.47 with 83.3% percentile ranking, and 83.28% accuracy on HumanEval+, outperforming all baselines across the board. These consistent performance gains across both domains and model scales provide strong empirical evidence that FlowRL's flow-balanced optimization successfully enhances generalization. This improvement comes from promoting diverse solution exploration compared to previous reward-maximizing RL approaches.\"},\n",
       "  {'title': '6.2. Ablation Studies',\n",
       "   'content': 'We conduct ablation studies on importance sampling and the 𝛽 hyperparameter. For importance sampling, we compared the performance with and without it, and implemented a combined loss approach proposed by Zhang et al. [2025a] that simultaneously optimizes both GFlowNets and PPO objectives. This combined loss focuses on optimizing diffusion models, and we adapt it to long CoT reasoning tasks for comparison. Table 3 demonstrates that importance sampling substantially improves FlowRL performance across all math reasoning benchmarks. Compared to Zhang et al. [2025a], using importance sampling as a trajectory-level ratio is more suitable than the combined loss of GFlowNets and PPO. The performance drop without importance sampling (from 35.63% to 26.71%) highlights the critical role of correcting for distribution mismatch between rollout generation and policy training. For the hyperparam-\\nFigure 3 | Ablation study on the 𝛽 in FlowRL. 𝛽 = 15 (highlighted in blue) achieves the best performance.\\n=5\\n=10\\n=15\\n=30\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\nAverage Score (%)\\n31.34\\n34.41\\n35.63\\n35.09\\neter 𝛽 , we conduct a series of parameter ablation studies, and Figure 3 shows that 𝛽 = 15 achieves optimal performance, with detailed results shown in Table 7.'},\n",
       "  {'title': '7.1. Diversity Analysis',\n",
       "   'content': 'To assess solution diversity , we follow the approach of Yu et al. [2025a] and employ GPT-4o-mini [OpenAI, 2024] to evaluate all responses generated by each method on AIME 24/25. The evaluation prompt is shown in Appendix C. As shown in Figure 4, FlowRL achieves higher diversity scores compared to baseline methods. This demonstrates that FlowRL improves sample diversity compared to baselines, which tend to exhibit repetitive solution patterns. This diversity evaluation reveals\\n9\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 4 | Case study comparing GRPO and FlowRL rollouts on an AIME problem. GRPO exhibits repetitive patterns (AM-GM × 3, identity loops × 2), while FlowRL follows a more diverse solution path.'},\n",
       "  {'title': 'Question',\n",
       "   'content': \"Let B be the set of rectangular boxes with surface area 54 and volume 23. Let 𝑟 be the radius of the smallest sphere that can contain each box in B . If 𝑟 2 = 𝑝 𝑞 with gcd ( 𝑝, 𝑞 ) = 1, find 𝑝 + 𝑞 .\\nGRPO\\n'. . .\\ndenote\\n𝑎, 𝑏, 𝑐\\n. . .\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n=\\n54,\\n𝑎𝑏𝑐\\n=\\n23 ...\\n𝑑\\n=\\n√\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2 ,\\n𝑟\\n=\\n𝑑\\n/\\n2\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n=\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n+\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n. . . AM-GM\\n×\\n3 :\\nAM-GM (1)\\n. . .\\nAM-GM (2)\\n. . .\\nAM-GM (3)\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n3\\nidentity loop\\n×\\n2 :\\nloop (1)\\n. . .\\nloop (2)\\n. . .\\n𝑎\\n=\\n𝑏\\n=\\n𝑐\\n(contradiction) .. .\\nback to\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n. . .\\nno factorization . . . '\\nFlowRL\\n'. . .\\nlet\\n𝑎, 𝑏, 𝑐\\nwith\\n2\\n(\\n𝑎𝑏\\n+\\n𝑏𝑐\\n+\\n𝑐𝑎\\n)\\n=\\n54,\\n𝑎𝑏𝑐\\n=\\n23\\n. . .\\n𝑑\\n=\\n√\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2 ,\\n𝑟\\n=\\n𝑑\\n/\\n2\\n. . .\\n(\\n𝑎\\n+\\n𝑏\\n+\\n𝑐\\n)\\n2\\n⇒\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n=\\n𝑠\\n2\\n-\\n54\\n. . .\\n𝑎\\n=\\n𝑏\\n. . .\\n𝑎\\n3\\n-\\n27\\n𝑎\\n+\\n46\\n=\\n0\\n. . .\\nrational root\\n𝑎\\n=\\n2 ...\\nfactor\\n(\\n𝑎\\n-\\n2\\n)(\\n𝑎\\n2\\n+\\n2\\n𝑎\\n-\\n23\\n)\\n. . .\\nbranch\\n𝑎\\n=\\n-\\n1\\n+\\n2\\n√\\n6 ...\\nback-sub\\n𝑐\\n=\\n23\\n/\\n𝑎\\n2\\n. . .\\n𝑎\\n2\\n+\\n𝑏\\n2\\n+\\n𝑐\\n2\\n=\\n657\\n16\\n. . .\\n𝑟\\n2\\n=\\n657\\n64\\n. . .\\nAnswer 721 ...'\\nsignificant differences in exploration patterns across methods. This nearly doubling of diversity score compared to the strongest baseline (PPO) indicates that FlowRL generates qualitatively different solution approaches rather than minor variations of the same strategy. The diversity analysis provides empirical validation of our core hypothesis that flow-balanced optimization promotes mode coverage in complex reasoning tasks.\"},\n",
       "  {'title': '7.2. Case Study',\n",
       "   'content': \"Table 4 illustrates the behavioral differences between GRPO and FlowRL on a representative AIME problem. GRPO exhibits repetitive patterns, applying AMGM three times and getting stuck in identity loops, failing to solve the problem. FlowRL explores more diverse actions: it sets 𝑎 = 𝑏 , derives a cubic equation, finds the rational root, and reaches the correct answer. This shows that FlowRL successfully avoids the repetitive exploration patterns. The contrast reveals fundamental differences in exploration strategies: GRPO's reward-maximizing approach leads to exploitation of familiar techniques (AM-GM inequality) without exploring alternatives, eventually reaching contradictory conclusions like 𝑎 = 𝑏 = 𝑐 . In contrast, FlowRL's distribution-matching enables strategic decisions such as the symmetry assumption 𝑎 = 𝑏 , which\\nFigure 4 | GPT-judged diversity scores on rollouts of AIME 24/25 problems. FlowRL generates more diverse solutions than R++, GRPO, and PPO.\\nR++\\nGRPO\\nPPO\\nFlowRL\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\nDiversity Score\\n1.11\\n1.23\\n1.31\\n2.28\\ntransforms the problem into a tractable cubic equation 𝑎 3 -27 𝑎 + 46 = 0, allowing systematic solution through rational root testing and polynomial factorization.\\n10\\nFlowRL: Matching Reward Distributions for LLM Reasoning\"},\n",
       "  {'title': '8. Conclusion',\n",
       "   'content': 'In this work, we introduce FlowRL, which transforms scalar rewards into normalized target distributions using a learnable partition function and minimizes the reverse KL divergence between the policy and target distribution. We demonstrate that this approach is theoretically equivalent to trajectory balance objectives from GFlowNets and implicitly maximizes both reward and entropy, thereby promoting diverse reasoning trajectories. To further address gradient explosion and sampling mismatch issues in long CoT reasoning, we incorporate importance sampling and length normalization. Through experiments on math and code reasoning benchmarks, FlowRL achieves consistent improvements across all tasks compared to GRPO and PPO. Our diversity analysis and case studies confirm that FlowRL generates more varied solution approaches while avoiding repetitive patterns.'},\n",
       "  {'title': 'Acknowledgments',\n",
       "   'content': 'We are grateful to Mingqian Feng and Yuetai Li for their valuable discussions and feedback, which helped improve the quality of this work.'},\n",
       "  {'title': 'References',\n",
       "   'content': \"Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of entropy on policy optimization. In International conference on machine learning , pages 151-160. PMLR, 2019.\\nBrian R Bartoldson, Siddarth Venkatraman, James Diffenderfer, Moksh Jain, Tal Ben-Nun, Seanie Lee, Minsu Kim, Johan Obando-Ceron, Yoshua Bengio, and Bhavya Kailkhura. Trajectory balance with asynchrony: Decoupling exploration and learning for fast, scalable llm post-training. arXiv preprint arXiv:2503.18929 , 2025.\\nEmmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Neural Information Processing Systems (NeurIPS) , 2021.\\nYoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J. Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. Journal of Machine Learning Research , 24(210):1-55, 2023a. URL http: //jmlr.org/papers/v24/22-0364.html .\\nYoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. Gflownet foundations. The Journal of Machine Learning Research , 24(1):10006-10060, 2023b.\\nChen-Hao Chao, Chien Feng, Wei-Fang Sun, Cheng-Kuang Lee, Simon See, and Chun-Yi Lee. Maximum entropy reinforcement learning via energy-based normalizing flow. arXiv preprint arXiv:2405.13629 , 2024.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\\n11\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.\\nDaixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758 , 2025.\\nMiruna Cretu, Charles Harris, Ilia Igashov, Arne Schneuing, Marwin Segler, Bruno Correia, Julien Roy, Emmanuel Bengio, and Pietro Liò. Synflownet: Design of diverse and novel molecules with synthesis constraints. arXiv preprint arXiv:2405.01155 , 2024.\\nGanqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617 , 2025.\\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 .\\nTristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, and Yoshua Bengio. Discrete probabilistic inference as control in multi-path environments. arXiv preprint arXiv:2402.10309 , 2024.\\nGuanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849 , 2025.\\nYilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in neural information processing systems , 32, 2019.\\nBenjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257 , 2021.\\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning , pages 10835-10866. PMLR, 2023.\\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning , pages 1861-1870. Pmlr, 2018.\\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 , 2024.\\nHaoran He, Can Chang, Huazhe Xu, and Ling Pan. Looking backward: Retrospective backward synthesis for goal-conditioned GFlownets. In The Thirteenth International Conference on Learning Representations , 2025. URL https://openreview.net/forum?id=fNMKqyvuZT .\\nGeoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and R M Neal. The 'wake-sleep' algorithm for unsupervised neural networks. Science , 268 5214:1158-61, 1995.\\nEdward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. arXiv preprint arXiv:2310.04363 , 2023.\\n12\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nEdward J Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, and Nikolay Malkin. Amortizing intractable inference in large language models. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/f orum?id=Ouj6p4ca60 .\\nJian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv. org/abs/2501 , 3262:32-33, 2025.\\nMoksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure F.P. Dossou, Chanakya Ekbote, Jie Fu, Tianyu Zhang, Micheal Kilgour, Dinghuai Zhang, Lena Simine, Payel Das, and Yoshua Bengio. Biological sequence design with GFlowNets. International Conference on Machine Learning (ICML) , 2022.\\nMoksh Jain, Tristan Deleu, Jason S. Hartford, Cheng-Hao Liu, Alex Hernández-García, and Yoshua Bengio. Gflownets for ai-driven scientific discovery. ArXiv , abs/2302.00615, 2023a. URL https: //api.semanticscholar.org/CorpusID:256459319 .\\nMoksh Jain, Sharath Chandra Raparthy, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Yoshua Bengio, Santiago Miret, and Emmanuel Bengio. Multi-objective GFlowNets. International Conference on Machine Learning (ICML) , 2023b.\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974 , 2024.\\nKoray Kavukcuoglu. Gemini 2.5: Our most intelligent AI model, 2025. URL https://blog.goo gle/technology/google-deepmind/gemini-model-thinking-updates-march-2025/ . Google Blog (The Keyword), Published Mar. 25, 2025.\\nMinsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, and Jinkyoo Park. Local search gflownets. ArXiv , abs/2310.02710, 2023.\\nMinsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan, Woochang Kim, Jinkyoo Park, Emmanuel Bengio, and Yoshua Bengio. Learning to scale logits for temperature-conditional gflownets, 2024.\\nSeanie Lee, Minsu Kim, Lynn Cherif, David Dobre, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, et al. Learning diverse attacks on large language models for robust red-teaming and safety tuning. arXiv preprint arXiv:2405.18540 , 2024.\\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems , volume 35, pages 3843-3857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/18abb eef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf .\\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050 , 2023a.\\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. In The Twelfth International Conference on Learning Representations , 2023b.\\n13\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nYaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t .\\nDianbo Liu, Moksh Jain, Bonaventure F. P. Dossou, Qianli Shen, Salem Lahlou, Anirudh Goyal, Nikolay Malkin, Chris C. Emezue, Dinghuai Zhang, Nadhir Hassen, Xu Ji, Kenji Kawaguchi, and Yoshua Bengio. Gflowout: Dropout with generative flow networks. In International Conference on Machine Learning , 2022.\\nMingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, et al. Scaling up rl: Unlocking diverse reasoning in llms via prolonged training. arXiv preprint arXiv:2507.12507 , 2025a.\\nZhen Liu, Tim Z Xiao, , Weiyang Liu, Yoshua Bengio, and Dinghuai Zhang. Efficient diversity-preserving diffusion alignment via gradient-informed gflownets. In ICLR , 2025b.\\nMichael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, Ion Stoica, and Tianjun Zhang. Deepcoder: A fully open-source 14b coder at o3-mini level, 2025. Notion Blog.\\nJiangyan Ma, Emmanuel Bengio, Yoshua Bengio, and Dinghuai Zhang. Baking symmetry into gflownets.\\nMAA. American mathematics competitions - amc. https://maa.org/ , 2023.\\nMAA. American invitational mathematics examination - aime. https://maa.org/ , 2025.\\nKanika Madan, Jarrid Rector-Brooks, Maksym Korablyov, Emmanuel Bengio, Moksh Jain, Andrei Cristian Nica, Tom Bosc, Yoshua Bengio, and Nikolay Malkin. Learning gflownets from partial episodes for improved convergence and stability. In International Conference on Machine Learning , pages 23467-23483. PMLR, 2023.\\nNikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in gflownets. Advances in Neural Information Processing Systems , 35: 5955-5967, 2022.\\nNikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and Yoshua Bengio. GFlowNets and variational inference. International Conference on Learning Representations (ICLR) , 2023.\\nDavid McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053 , 2025.\\nSobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropy gflownets with soft q-learning. In International Conference on Artificial Intelligence and Statistics , pages 2593-2601. PMLR, 2024.\\nOpenAI. Gpt-4o mini. https://openai.com/index/gpt-4o-mini-advancing-cost-effic ient-intelligence/ , 2024. Accessed: 2024.\\nAlexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and mitigating misaligned models. arXiv preprint arXiv:2201.03544 , 2022.\\nLing Pan, Moksh Jain, Kanika Madan, and Yoshua Bengio. Pre-training and fine-tuning generative flow networks, 2023a.\\n14\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nLing Pan, Nikolay Malkin, Dinghuai Zhang, and Yoshua Bengio. Better training of GFlowNets with local credit and incomplete trajectories. International Conference on Machine Learning (ICML) , 2023b.\\nLing Pan, Dinghuai Zhang, Aaron Courville, Longbo Huang, and Yoshua Bengio. Generative augmented flow networks. International Conference on Learning Representations (ICLR) , 2023c.\\nLing Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic generative flow networks. Uncertainty in Artificial Intelligence (UAI) , 2023d.\\nSeohong Park, Qiyang Li, and Sergey Levine. Flow q-learning. In Forty-second International Conference on Machine Learning , 2025. URL https://openreview.net/forum?id=KVf2SFL1pi .\\nGuilherme Penedo, Anton Lozhkov, Hynek Kydlíček, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https://huggingface.co/datasets/open-r1/codeforces , 2025.\\nSamuel Pfrommer, Yixiao Huang, and Somayeh Sojoudi. Reinforcement learning for flow-matching policies. arXiv preprint arXiv:2507.15073 , 2025.\\nAbhinav Rastogi, Albert Q Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910 , 2025.\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\\nMax W. Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and Tommaso Biancalani. Towards understanding and improving gflownet training. ArXiv , abs/2305.07170, 2023. URL https://api.semanticscholar.org/CorpusID:258676487 .\\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256 , 2024.\\nJoar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460-9471, 2022.\\nRichard S Sutton, Andrew G Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience , 11(1):126-134, 1999a.\\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. Müller, editors, Advances in Neural Information Processing Systems , volume 12. MIT Press, 1999b. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0b ed98e80ade0a5c43b0f-Paper.pdf .\\nQwen Team. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.g ithub.io/blog/qwen2.5/ .\\n15\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nDaniil Tiapkin, Nikita Morozov, Alexey Naumov, and Dmitry P Vetrov. Generative flow networks as entropy-regularized rl. In International Conference on Artificial Intelligence and Statistics , pages 4213-4221. PMLR, 2024.\\nShenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939 , 2025.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824-24837, 2022.\\nFangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, and Lianhui Qin. Flow of reasoning: Training llms for divergent reasoning with minimal examples. In Forty-second International Conference on Machine Learning , 2025a.\\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476 , 2025b.\\nTaeyoung Yun, Dinghuai Zhang, Jinkyoo Park, and Ling Pan. Learning to sample effective and diverse prompts for text-to-image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference , pages 23625-23635, 2025.\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems , 35:15476-15488, 2022.\\nDavid W. Zhang, Corrado Rainone, Markus F. Peschl, and Roberto Bondesan. Robust scheduling with gflownets. ArXiv , abs/2302.05446, 2023a. URL https://api.semanticscholar.org/Corp usID:256827133 .\\nDinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models with GFlowNets and beyond. arXiv preprint arXiv:2209.02606v2 , 2022a.\\nDinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio. Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning (ICML) , 2022b.\\nDinghuai Zhang, Hanjun Dai, Nikolay Malkin, Aaron C. Courville, Yoshua Bengio, and Ling Pan. Let the flows tell: Solving graph combinatorial optimization problems with gflownets. ArXiv , abs/2305.17010, 2023b.\\nDinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville, and Yoshua Bengio. Diffusion generative flow samplers: Improving learning signals through partial trajectory optimization, 2024a.\\nDinghuai Zhang, Ling Pan, Ricky T. Q. Chen, Aaron Courville, and Yoshua Bengio. Distributional gflownets with quantile flows, 2024b.\\nDinghuai Zhang, Yizhe Zhang, Jiatao Gu, Ruixiang ZHANG, Joshua M. Susskind, Navdeep Jaitly, and Shuangfei Zhai. Improving GFlownets for text-to-image diffusion alignment. Transactions on Machine Learning Research , 2025a. ISSN 2835-8856. URL https://openreview.net/forum ?id=XDbY3qhM42 .\\n16\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nKaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. A survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827 , 2025b.\\nMingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu Blanchette, and Yoshua Bengio. Phylogfn: Phylogenetic inference with generative flow networks, 2024.\\nHeiko Zimmermann, Fredrik Lindsten, J.-W. van de Meent, and Christian Andersson Naesseth. A variational perspective on generative flow networks. ArXiv , abs/2210.07992, 2022. URL https: //api.semanticscholar.org/CorpusID:252907672 .\\n17\\nFlowRL: Matching Reward Distributions for LLM Reasoning\"},\n",
       "  {'title': 'A. Proof of Proposition 1',\n",
       "   'content': 'We begin by analyzing the gradient of the Kullback-Leibler (KL) divergence between the policy 𝜋𝜃 ( y | x ) and the target reward distribution exp ( 𝛽𝑟 ( x , y ) ) 𝑍𝜙 ( x ) :\\nNext, consider the trajectory balance objective used in GFlowNets learning [Bartoldson et al., 2025, Bengio et al., 2023b, Lee et al., 2024], defined as:\\nTaking the gradient of this objective with respect to 𝜃 yields:\\nThus, minimizing the KL divergence is equivalent (up to a constant) to minimizing the trajectory balance loss, confirming Proposition 1.'},\n",
       "  {'title': 'B. Theoretical Analysis',\n",
       "   'content': 'We conduct an interpretation of FlowRL that clarifies the role of each component in the objective.\\nProposition 5. Minimizing the KL divergence in Eq. 5 is equivalent (in terms of gradients) to jointly maximizing reward and policy entropy:\\nRemark 6 ( FlowRL beyond reward maximization ) . Proposition 5 reveals that FlowRL can be interpreted as jointly maximizing expected reward and policy entropy. This shift encourages the policy to explore a broader set of high-quality solutions, enabling more diverse and generalizable behaviors on reasoning tasks. Our interpretation also aligns with prior work that views GFlowNets training as a form of maximum entropy RL [Deleu et al., 2024, Mohammadpour et al., 2024].\\n18\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nThe proof of Proposition 5 is provided as below.\\nRecall from Eq. 3 and Eq. 5 that the FlowRL objective is sourced from the minimization of a KL divergence:\\nRearranging the terms, we obtain:\\nFinally, we express the FlowRL objective in its compact form:\\n\\uf8f0\\n\\uf8fb\\nTherefore, minimizing the FlowRL objective can be interpreted as jointly maximizing reward and entropy, while also aligning the policy with a structured prior. The reward term drives task performance, while the normalization term 𝑍𝜙 ( x ) ensures consistency with a properly normalized target distribution. This encourages the policy 𝜋𝜃 to cover the entire reward-weighted distribution rather than collapsing to a few high-reward modes. The reference policy 𝜋 ref provides inductive bias that regularizes the policy toward desirable structures, and the entropy term H( 𝜋𝜃 ) encourages diversity in sampled solutions. Together, these components promote better generalization of FlowRL.'},\n",
       "  {'title': 'C. GFlowNets',\n",
       "   'content': 'We follow the notation of [He et al., 2025, Madan et al., 2023] to introduce the fundamentals of GFlowNets. Let X denote the compositional objects and 𝑅 be a reward function that assigns nonnegative values to each object 𝑥 ∈ X . GFlowNets aim to learn a sequential, constructive sampling policy 𝜋 that generates objects 𝑥 with probabilities proportional to their rewards, i.e., 𝜋 ( 𝑥 ) ∝ 𝑅 ( 𝑥 ) . This process can be represented as a directed acyclic graph (DAG) G = (S , A) , where the vertices 𝑠 ∈ S are referred to as states , and the directed edges ( 𝑢 → 𝑣 ) ∈ A are called actions . The generation of an object 𝑥 ∈ X corresponds to a complete trajectory 𝜏 = ( 𝑠 0 → · · · → 𝑠 𝑛 ) ∈ T within the DAG, beginning at the initial state 𝑠 0 and ending at a terminal state 𝑠 𝑛 ∈ X . The state flow 𝐹 ( 𝑠 ) is defined as a non-negative weight assigned to each state 𝑠 ∈ S . The forward policy 𝑃𝐹 ( 𝑠 ′ | 𝑠 ) specifies the transition probability to a child state 𝑠 ′ , while the backward policy 𝑃𝐵 ( 𝑠 | 𝑠 ′ ) specifies the transition probability to a parent state 𝑠 . To this end, detailed balance objective enforces local flow consistency across every edge ( 𝑠 → 𝑠 ′ ) ∈ A :\\n19\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTo achieve this flow consistency, GFlowNets employ training objectives at different levels of granularity, including detailed balance [Bengio et al., 2023b], trajectory balance [Malkin et al., 2022], and subtrajectory balance [Madan et al., 2023]. Leveraging their diversity-seeking behavior, GFlowNets have been successfully applied across a range of domains, including molecule generation [Cretu et al., 2024], diffusion fine-tuning [Liu et al., 2025b, Zhang et al., 2025a], and amortized reasoning [Hu et al., 2024, Yu et al., 2025a]. Among various training objective in GFlowNets, trajectory balance maintains flow consistency at the trajectory level, defined as:\\nFurthermore, sub-trajectory balance achieves local balance on arbitrary subpaths 𝜏𝑖 : 𝑗 = { 𝑠 𝑖 → · · ·→ 𝑠 𝑗 } , offering a more stable and less biased learning signal. We build on trajectory balance to extend our KL-based objective through a gradient-equivalence formulation (Prop. 1), and further improve it to better support long CoT reasoning in RL.\\nTable 5 | Math reasoning performance (Avg@64) at temperature = 0 . 6. Relative improvements are shown as subscripts, with positive gains in green and negative changes in red. FlowRL consistently outperforms all baselines and achieves the best average score under this low-temperature setting.\\nTable 6 | Math reasoning performance (Avg@64) at temperature = 1 . 0. Relative improvements are shown as subscripts, with positive gains in green. FlowRL maintains robust performance under higher generation randomness and continues to outperform all baselines on average.\\n20\\nFlowRL: Matching Reward Distributions for LLM Reasoning\\nTable 7 | Ablation study on the effect of the 𝛽 parameter in FlowRL. We report Avg@16 accuracy across six math reasoning benchmarks for different values of 𝛽 .'},\n",
       "  {'title': 'Diversity Evaluation Prompt',\n",
       "   'content': 'System: You are evaluating the DIVERSITY of solution approaches for a mathematics competition problem. Focus on detecting even SUBTLE differences in methodology that indicate different problemsolving strategies.'},\n",
       "  {'title': 'PROBLEM:', 'content': '{problem}'},\n",
       "  {'title': '16 SOLUTION ATTEMPTS:', 'content': '{formatted_responses}'},\n",
       "  {'title': 'Score 1 - Minimal Diversity:',\n",
       "   'content': 'Same mathematical setup, same variable choices, same solution path\\n14+ responses use essentially identical approaches\\nOnly trivial differences (arithmetic, notation, wording)\\nIndicates very low exploration/diversity in the generation process'},\n",
       "  {'title': 'Score 2 - Low Diversity:',\n",
       "   'content': '1-2 alternative approaches appear but are rare\\n11-13 responses use the same main approach\\nMinor variations within the dominant method (different substitutions, orderings)\\nSome exploration but heavily biased toward one strategy'},\n",
       "  {'title': 'Score 3 - Moderate Diversity:',\n",
       "   'content': '2-3 distinct alternative approaches present\\n7-10 responses use the most common approach\\nNoticeable variation in problem setup or mathematical techniques\\nBalanced mix showing reasonable exploration'},\n",
       "  {'title': 'Score 4 - High Diversity:',\n",
       "   'content': '3-4 distinct solution strategies well-represented\\n4-6 responses use the most common approach\\nMultiple mathematical techniques and problem framings\\nStrong evidence of diverse exploration strategies'},\n",
       "  {'title': 'Score 5 - Maximum Diversity:',\n",
       "   'content': '4+ distinctly different solution strategies\\nNo single approach dominates ( ≤ 3 responses use same method)\\nWide variety of mathematical techniques and creative approaches\\nIMPORTANT: Focusing on the DIVERSITY of the attempted approaches. Return ONLY a number from 1 to 5.\\nExcellent exploration and generation diversity\\n21'}],\n",
       " 'is_processed': True,\n",
       " 'metrics': {'accuracy': 35.6},\n",
       " 'research_area': 'Artificial Intelligence',\n",
       " 'research_areas_all': ['Artificial Intelligence',\n",
       "  'Computer Vision and Pattern Recognition',\n",
       "  'Machine Learning'],\n",
       " 'word_count': 10103,\n",
       " 'author_count': 8,\n",
       " 'institutions': ['Tsinghua University',\n",
       "  'Shanghai Jiao Tong University',\n",
       "  'Peking University',\n",
       "  'Toyota Technological Institute',\n",
       "  'University of China']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test : testing the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: d:\\Projects\\arxiv-ai-explorer\\backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amine\\anaconda3\\envs\\arxiv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\amine\\anaconda3\\envs\\arxiv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"./..\")\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "from src.services.retrieval import get_retriever\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"reward function\"\n",
    "top_k = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amine\\anaconda3\\envs\\arxiv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "retriever = get_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Called with Vector search: reward function | Limit: 10 | Include Sections: [] | Exclude Sections: ['References', 'Content']\n"
     ]
    }
   ],
   "source": [
    "include_sections = []\n",
    "exclude_sections = [\"References\", \"Content\"]\n",
    "\n",
    "chunks_no_references = retriever.vector_search(q, limit=top_k, include_sections=include_sections, exclude_sections=exclude_sections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E Additional Ablation Studies',\n",
       " '3.1. From Reward Maximization to Distribution Matching',\n",
       " '1. Introduction',\n",
       " '5. Experiment Settings',\n",
       " '4.1 Experimental Setup',\n",
       " 'FlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       " 'B. Theoretical Analysis',\n",
       " '2. Preliminaries',\n",
       " '4.1. Reinforcement Learning for Reasoning',\n",
       " 'C. GFlowNets']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_no_ref_sections = [section[\"section_title\"] for section in chunks_no_references if section[\"section_title\"]]\n",
    "chunks_no_ref_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Called with Vector search: reward function | Limit: 10 | Include Sections: [] | Exclude Sections: []\n"
     ]
    }
   ],
   "source": [
    "chunks = retriever.vector_search(q, limit=top_k, include_sections=[], exclude_sections=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['References',\n",
       " 'Content',\n",
       " 'E Additional Ablation Studies',\n",
       " '3.1. From Reward Maximization to Distribution Matching',\n",
       " '1. Introduction',\n",
       " '5. Experiment Settings',\n",
       " '4.1 Experimental Setup',\n",
       " 'FlowRL: Matching Reward Distributions for LLM Reasoning',\n",
       " 'B. Theoretical Analysis',\n",
       " '2. Preliminaries']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_sections = [section[\"section_title\"] for section in chunks if section[\"section_title\"]]\n",
    "chunks_sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test : Testing Openai agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amine\\anaconda3\\envs\\arxiv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, ModelSettings\n",
    "from src.agents.tools import search_papers\n",
    "\n",
    "retrieval_agent = Agent(\n",
    "    name=\"Paper Retrieval Agent\",\n",
    "    instructions=(\n",
    "        \"You are a research assistant. \"\n",
    "        \"Use the search_papers tool to retrieve relevant papers or chunks. \"\n",
    "        \"Apply filters when the user specifies sections to include or exclude.\"\n",
    "        \"Answer the user's question based on the retrieved papers or chunks.\"\n",
    "    ),\n",
    "    model=\"gpt-5-mini\",\n",
    "    tools=[search_papers],\n",
    "    model_settings=ModelSettings(tool_choice=\"auto\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner\n",
    "\n",
    "query = \"How Large language models think and reason ? Exculed sections with `References`.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Called with Search papers: reward function for COT reward function \"COT\" \"Chain of Thought\" reward function\n",
      "Tool Called with Vector search: reward function for COT reward function \"COT\" \"Chain of Thought\" reward function | Limit: 10 | Include Sections: None | Exclude Sections: ['References']\n",
      "Tool Called with Search papers: \"reward function\" \"CoT\" \"chain-of-thought\" r(x,y) reward function CoT paper\n",
      "Tool Called with Vector search: \"reward function\" \"CoT\" \"chain-of-thought\" r(x,y) reward function CoT paper | Limit: 10 | Include Sections: None | Exclude Sections: ['References']\n"
     ]
    }
   ],
   "source": [
    "res = await Runner.run(retrieval_agent, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Short answer (from FlowRL, excluding References):\\n\\n- Base formulation: FlowRL treats the reward r(x,y) as the scalar outcome reward and converts it into a target distribution\\n  p~(y|x) ∝ exp(β · r(x,y)) / Zφ(x).\\n\\n- Modified reward used in FlowRL (incorporates a reference model prior):\\n  replace β·r(x,y) with β·r(x,y) + log π_ref(y|x),\\n  so the target becomes proportional to π_ref(y|x) · exp(β·r(x,y)).\\n\\n- Practical normalizations and transforms:\\n  - Outcome reward r(x,y) is group-normalized within each sampled group:\\n    r̂_i = (r_i − mean(r)) / std(r), where r = {r1,...,rG}.\\n  - Length (reward) shaping: the log-probability term is normalized by sequence length (1/|y| · log πθ(y|x)) to avoid exploding gradients for long CoT sequences.\\n  - Importance-sampling / off-policy correction: use weight w = detach[πθ(y|x)] / π_old(y|x) with PPO-style clipping to stabilize updates.\\n\\n- Hyperparameter: they follow prior work and set β = 15 in experiments.\\n\\nIn short: the CoT reward in FlowRL is an outcome-based scalar r(x,y) (typically correctness-based), group-normalized, scaled by β and combined additively with log π_ref(y|x) to form the energy exp(β r + log π_ref), with length normalization and importance sampling used during optimization.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I searched the available paper excerpts and can answer based on those documents (I excluded any \\\"References\\\" sections as you requested). Two papers in the database are especially informative about how modern LLMs “think” and perform reasoning (how they are trained/steered and what mechanisms matter):\n",
    "\n",
    "Key high-level points\n",
    " LLM “reasoning” is implemented as generating extended token sequences (chains‑of‑thought, CoT) whose intermediate steps function as the model’s internal reasoning trajectory.  \n",
    " Supervised pretraining + in‑context examples produce a base ability to produce those trajectories; targeted fine‑tuning and RL are used to make those trajectories reliably correct and useful for downstream tasks. (See GeoReasoning-10K work and FlowRL excerpts.)\n",
    " Reward design and training objective strongly shape what the model considers a good reasoning trace: naive reward maximization can collapse to a single dominant solution mode, while objectives that encourage distributional coverage of high‑reward trajectories produce more diverse and robust reasoning. (FlowRL.)\n",
    " \n",
    " How this looks in the papers (details and mechanisms)\n",
    " Reasoning as trajectories / long CoT sequences:\n",
    "   - Reasoning tasks are cast as producing long token trajectories (CoT). Long trajectories (e.g., thousands of tokens) create optimization challenges for RL and standard reward signals (FlowRL excerpts).\n",
    " Reinforcement learning and reward signals:\n",
    "   - GeoReasoning-10K: uses an RL loop to refine captions for geometric images. The reward is composite: a reasoning reward (does the caption enable solving the downstream math question — evaluated by a frozen LLM) and a caption reward (semantic similarity to ground truth, measured by ROUGE/BLEU). The reasoning reward checks both answer format and correctness. This shapes model outputs toward captions that contain the key reasoning facts.\n",
    "   - FlowRL: notes that reward‑maximizing RL (PPO, etc.) tends to optimize toward dominant modes and can ignore other valid reasoning paths (mode collapse). Instead, it proposes matching the model’s output distribution to a target reward distribution so the model samples diverse, high‑reward trajectories in proportion to their reward.\n",
    " Distribution matching / energy-based normalization:\n",
    "   - FlowRL introduces a learned partition function Z_phi(x) that turns scalar rewards into a normalized target distribution (˜π(y|x) ∝ exp(β r(x,y))/Zφ(x)). Minimizing reverse KL between policy and that target (or using an equivalent trajectory‑balance loss inspired by GFlowNets) encourages sampling a variety of good reasoning trajectories rather than a single high‑probability one.\n",
    " Practical techniques to avoid sparse rewards and mode collapse:\n",
    "   - Use auxiliary similarity rewards (to avoid early sparsity) — e.g., caption ROUGE/BLEU in GeoReasoning.\n",
    "   - Use entropy regularization or explicit training with higher-entropy/diverse data; or change objective from reward maximization to distribution matching (FlowRL).\n",
    "   - Use frozen LLMs as verifiers to compute reasoning rewards (GeoReasoning).\n",
    " Empirical effects reported in snippets:\n",
    "   - FlowRL reports consistent improvements on math and code reasoning benchmarks by promoting diverse solution exploration rather than collapsing to a single mode.\n",
    "   - GeoReasoning-10K reports accuracy gains on several mathematical reasoning benchmarks when captions are refined with RL using verifiable rewards.\n",
    " \n",
    " Short intuitive summary\n",
    " LLMs “think” by producing sequences of tokens that can be read as stepwise reasoning. Training and fine‑tuning shape which sequences the model prefers. Carefully designed reward signals and training objectives (especially ones that reward a distribution of good reasoning traces rather than a single mode) improve accuracy, generalization, and robustness of the model’s internal reasoning paths.\n",
    " \n",
    " Limitations / caveat\n",
    " The above summary is drawn only from the provided excerpts (FlowRL: \\\"FlowRL: Matching Reward Distributions for LLM Reasoning\\\" and GeoReasoning: \\\"Generalizable Geometric Image Caption Synthesis\\\"). There are many other perspectives and experiments in the broader literature not included in these snippets. If you want, I can pull more excerpts from other papers in the database (e.g., on chain‑of‑thought, mechanistic interpretability, or verification) to expand or compare views."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
