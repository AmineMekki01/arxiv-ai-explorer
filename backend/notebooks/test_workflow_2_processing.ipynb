{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow 2: Paper Processing Pipeline - Local Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment configured for host machine:\n",
      "   - Database: postgresql://researchmind:password@localhost:5433/researchmind\n",
      "   - Redis: redis://localhost:6379/0\n",
      "   - Qdrant: localhost:6333\n",
      "   - Neo4j: bolt://localhost:7687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/arxiv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import hashlib\n",
    "\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root.parent))\n",
    "\n",
    "from notebook_setup import *\n",
    "\n",
    "\n",
    "from src.services.pdf_parser.docling_utils import deserialize_docling_document\n",
    "from src.services.chunking.chunker import ChunkingConfig, PaperChunker\n",
    "from src.services.embeddings import MultiVectorEmbedder\n",
    "from src.database import get_sync_session\n",
    "from src.models.paper import Paper\n",
    "from src.config import get_settings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qmodels\n",
    "\n",
    "settings = get_settings()\n",
    "workflow_data = {}\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Papers for Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 papers needing embeddings\n",
      "\n",
      "[1] 2510.25771v1: Gaperon: A Peppered English-French Generative Language Model...\n",
      "[2] 2510.25770v1: E-Scores for (In)Correctness Assessment of Generative Model ...\n",
      "[3] 2510.25758v1: TheraMind: A Strategic and Adaptive Agent for Longitudinal P...\n",
      "[4] 2510.25766v1: Decomposition-Enhanced Training for Post-Hoc Attributions In...\n",
      "[5] 2510.25761v1: DiagramEval: Evaluating LLM-Generated Diagrams via Graphs...\n",
      "\n",
      "✓ Loaded 5 papers\n"
     ]
    }
   ],
   "source": [
    "MAX_PAPERS = 6\n",
    "\n",
    "results: List[Dict[str, Any]] = []\n",
    "with get_sync_session() as session:\n",
    "    papers = (\n",
    "        session.query(Paper)\n",
    "        .filter(Paper.is_embedded == False)\n",
    "        .filter(Paper.docling_document.isnot(None))\n",
    "        .limit(MAX_PAPERS)\n",
    "        .all()\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(papers)} papers needing embeddings\\n\")\n",
    "    \n",
    "    for i, p in enumerate(papers, 1):\n",
    "        print(f\"[{i}] {p.arxiv_id}: {p.title[:60]}...\")\n",
    "        results.append({\n",
    "            \"arxiv_id\": p.arxiv_id,\n",
    "            \"title\": p.title,\n",
    "            \"authors\": p.authors or [],\n",
    "            \"categories\": p.categories or [],\n",
    "            \"primary_category\": p.primary_category,\n",
    "            \"published_date\": p.published_date,\n",
    "            \"docling_document\": p.docling_document,\n",
    "            \"affiliations\": p.affiliations,\n",
    "        })\n",
    "\n",
    "workflow_data['load_papers'] = results\n",
    "print(f\"\\n✓ Loaded {len(results)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Ensure Qdrant Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Collection 'arxiv_chunks' already exists\n"
     ]
    }
   ],
   "source": [
    "collection_name = settings.qdrant_collection\n",
    "USE_MULTI_VECTOR = True\n",
    "\n",
    "client = QdrantClient(\n",
    "    host=settings.qdrant_host,\n",
    "    port=settings.qdrant_port,\n",
    "    prefer_grpc=False,\n",
    "    timeout=30,\n",
    ")\n",
    "\n",
    "collections = client.get_collections().collections\n",
    "existing = any(c.name == collection_name for c in collections)\n",
    "\n",
    "if not existing:\n",
    "    print(f\"Creating collection '{collection_name}'...\")\n",
    "    \n",
    "    if USE_MULTI_VECTOR:\n",
    "        embedder = MultiVectorEmbedder()\n",
    "        dims = embedder.get_embedding_dimensions()\n",
    "        \n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config={\n",
    "                \"all-MiniLM-L6-v2\": qmodels.VectorParams(\n",
    "                    size=dims[\"dense_dim\"],\n",
    "                    distance=qmodels.Distance.COSINE,\n",
    "                ),\n",
    "            },\n",
    "            sparse_vectors_config={\n",
    "                \"bm25\": qmodels.SparseVectorParams(modifier=qmodels.Modifier.IDF)\n",
    "            }\n",
    "        )\n",
    "        print(f\"✓ Created with hybrid search (dense dim={dims['dense_dim']} + sparse BM25)\")\n",
    "    else:\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=qmodels.VectorParams(\n",
    "                size=settings.embedding_dim,\n",
    "                distance=qmodels.Distance.COSINE,\n",
    "            ),\n",
    "        )\n",
    "        print(f\"✓ Created with single vector (dim={settings.embedding_dim})\")\n",
    "else:\n",
    "    print(f\"✓ Collection '{collection_name}' already exists\")\n",
    "\n",
    "workflow_data['qdrant_collection'] = collection_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Chunk Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1654 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 2510.25771v1: 127 chunks\n",
      "[2] 2510.25770v1: 35 chunks\n",
      "[3] 2510.25758v1: 90 chunks\n",
      "[4] 2510.25766v1: 58 chunks\n",
      "[5] 2510.25761v1: 24 chunks\n",
      "\n",
      "✓ Total: 334 chunks from 5 papers\n"
     ]
    }
   ],
   "source": [
    "papers = workflow_data.get('load_papers', [])\n",
    "MAX_TOKENS = 1000\n",
    "\n",
    "cfg = ChunkingConfig(max_tokens=MAX_TOKENS)\n",
    "chunker = PaperChunker(cfg)\n",
    "all_chunks: List[Dict[str, Any]] = []\n",
    "\n",
    "for i, p in enumerate(papers, 1):\n",
    "    try:\n",
    "        arxiv_id = p.get('arxiv_id', 'unknown')\n",
    "        \n",
    "        if not p.get(\"docling_document\"):\n",
    "            print(f\"[{i}] {arxiv_id}: skipped (no document)\")\n",
    "            continue\n",
    "        \n",
    "        doc = deserialize_docling_document(p[\"docling_document\"])\n",
    "        chunks = chunker.chunk_paper(doc)\n",
    "        \n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            \n",
    "            all_chunks.append({\n",
    "                \"arxiv_id\": arxiv_id,\n",
    "                \"title\": p.get(\"title\", \"\"),\n",
    "                \"primary_category\": p.get(\"primary_category\", \"\"),\n",
    "                \"categories\": p.get(\"categories\", []),\n",
    "                \"published_date\": p.get(\"published_date\"),\n",
    "                \"authors\": p.get(\"authors\", []),\n",
    "                \"chunk_index\": idx,\n",
    "                \"chunk_text\": chunk.text if hasattr(chunk, 'text') else str(chunk),\n",
    "            })\n",
    "        \n",
    "        print(f\"[{i}] {arxiv_id}: {len(chunks)} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[{i}] {p.get('arxiv_id', 'unknown')}: ✗ {e}\")\n",
    "\n",
    "result = {\"papers\": papers, \"chunks\": all_chunks}\n",
    "workflow_data['chunk_documents'] = result\n",
    "print(f\"\\n✓ Total: {len(all_chunks)} chunks from {len(papers)} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid chunks: 334/334\n",
      "\n",
      "Generating hybrid embeddings (dense + sparse BM25)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:04<00:00,  1.02it/s]\n",
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 30.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated 334 hybrid embeddings\n",
      "  Dense dimension: 384\n",
      "\n",
      "✓ Embedding generation complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "payload = workflow_data.get('chunk_documents', {})\n",
    "papers = payload.get(\"papers\", [])\n",
    "chunks = payload.get(\"chunks\", [])\n",
    "\n",
    "valid_chunks = []\n",
    "valid_texts = []\n",
    "for chunk in chunks:\n",
    "    text = chunk.get(\"chunk_text\", \"\")\n",
    "    if text and isinstance(text, str) and len(text.strip()) > 0:\n",
    "        valid_chunks.append(chunk)\n",
    "        valid_texts.append(text.strip())\n",
    "\n",
    "print(f\"Valid chunks: {len(valid_chunks)}/{len(chunks)}\\n\")\n",
    "\n",
    "if valid_chunks and USE_MULTI_VECTOR:\n",
    "    print(\"Generating hybrid embeddings (dense + sparse BM25)...\")\n",
    "    embedder = MultiVectorEmbedder()\n",
    "    dense_embs, sparse_embs = embedder.embed_documents(valid_texts)\n",
    "    \n",
    "    for i, (dense, sparse) in enumerate(zip(dense_embs, sparse_embs)):\n",
    "        valid_chunks[i][\"vectors\"] = {\n",
    "            \"dense\": dense,\n",
    "            \"sparse\": sparse.as_object(),\n",
    "        }\n",
    "        valid_chunks[i][\"embedding_model\"] = \"hybrid (dense + BM25)\"\n",
    "    \n",
    "    print(f\"✓ Generated {len(dense_embs)} hybrid embeddings\")\n",
    "    print(f\"  Dense dimension: {len(dense_embs[0]) if dense_embs else 0}\")\n",
    "else:\n",
    "    print(\"No chunks to embed or multi-vector disabled\")\n",
    "\n",
    "result = {\"papers\": papers, \"chunks\": valid_chunks}\n",
    "workflow_data['generate_embeddings'] = result\n",
    "print(f\"\\n✓ Embedding generation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Upsert to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserting 334 chunks to Qdrant...\n",
      "\n",
      "  Upserted batch: 256 points\n",
      "  Upserted batch: 78 points\n",
      "\n",
      "✓ Upserted 334 vectors to Qdrant\n"
     ]
    }
   ],
   "source": [
    "payload = workflow_data.get('generate_embeddings', {})\n",
    "chunks = payload.get(\"chunks\", [])\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "upserted = 0\n",
    "batch_points: list[qmodels.PointStruct] = []\n",
    "\n",
    "def flush_batch():\n",
    "    global upserted, batch_points\n",
    "    if not batch_points:\n",
    "        return\n",
    "    client.upsert(collection_name=collection_name, points=batch_points)\n",
    "    upserted += len(batch_points)\n",
    "    print(f\"  Upserted batch: {len(batch_points)} points\")\n",
    "    batch_points = []\n",
    "\n",
    "print(f\"Upserting {len(chunks)} chunks to Qdrant...\\n\")\n",
    "\n",
    "for idx, ch in enumerate(chunks):\n",
    "    vectors = ch.get(\"vectors\")\n",
    "    if not vectors:\n",
    "        continue\n",
    "    \n",
    "    content_id = f\"{ch.get('arxiv_id','unknown')}_{ch.get('chunk_index',idx)}\"\n",
    "    pid = int(hashlib.sha256(content_id.encode('utf-8')).hexdigest(), 16) % (2**63 - 1)\n",
    "    \n",
    "    payload_data = {k: v for k, v in ch.items() if k not in {\"vectors\", \"vector\"}}\n",
    "    \n",
    "    point = qmodels.PointStruct(\n",
    "        id=pid,\n",
    "        vector={\n",
    "            \"all-MiniLM-L6-v2\": vectors[\"dense\"],\n",
    "            \"bm25\": vectors[\"sparse\"],\n",
    "        },\n",
    "        payload=payload_data\n",
    "    )\n",
    "    batch_points.append(point)\n",
    "    \n",
    "    if len(batch_points) >= BATCH_SIZE:\n",
    "        flush_batch()\n",
    "\n",
    "flush_batch()\n",
    "\n",
    "workflow_data['upsert_qdrant'] = {\"upserted\": upserted}\n",
    "print(f\"\\n✓ Upserted {upserted} vectors to Qdrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Mark Papers as Embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking 5 papers as embedded...\n",
      "\n",
      "  2510.25771v1\n",
      "  2510.25770v1\n",
      "  2510.25758v1\n",
      "  2510.25766v1\n",
      "  2510.25761v1\n",
      "\n",
      "✓ Marked 5 papers as embedded\n"
     ]
    }
   ],
   "source": [
    "payload = workflow_data.get('generate_embeddings', {})\n",
    "papers = payload.get(\"papers\", [])\n",
    "arxiv_ids = {p.get(\"arxiv_id\") for p in papers if p.get(\"arxiv_id\")}\n",
    "\n",
    "print(f\"Marking {len(arxiv_ids)} papers as embedded...\\n\")\n",
    "\n",
    "updated = 0\n",
    "with get_sync_session() as session:\n",
    "    try:\n",
    "        db_papers = session.query(Paper).filter(Paper.arxiv_id.in_(list(arxiv_ids))).all()\n",
    "        for dp in db_papers:\n",
    "            print(f\"  {dp.arxiv_id}\")\n",
    "            dp.is_embedded = True\n",
    "            dp.is_processed = True\n",
    "        session.commit()\n",
    "        updated = len(db_papers)\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        session.rollback()\n",
    "\n",
    "workflow_data['mark_embedded'] = {\"updated\": updated}\n",
    "print(f\"\\n✓ Marked {updated} papers as embedded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORKFLOW 2 SUMMARY\n",
      "============================================================\n",
      "Papers loaded:     5\n",
      "Chunks generated:  334\n",
      "Chunks embedded:   334\n",
      "Vectors upserted:  334\n",
      "Papers marked:     5\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"WORKFLOW 2 SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Papers loaded:     {len(workflow_data.get('load_papers', []))}\")\n",
    "print(f\"Chunks generated:  {len(workflow_data.get('chunk_documents', {}).get('chunks', []))}\")\n",
    "print(f\"Chunks embedded:   {len(workflow_data.get('generate_embeddings', {}).get('chunks', []))}\")\n",
    "print(f\"Vectors upserted:  {workflow_data.get('upsert_qdrant', {}).get('upserted', 0)}\")\n",
    "print(f\"Papers marked:     {workflow_data.get('mark_embedded', {}).get('updated', 0)}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
